{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "class DatasetFood(Dataset):\n",
    "    \n",
    "    def __init__(self, file_path, transform=None):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        self.data['img_name'] = \"food-recognition-challenge/train_set/train_set/\"+ self.data['img_name']\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.data['img_name'][index]\n",
    "        image = Image.open(img_name)\n",
    "        label = self.data['label'][index] - 1\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "train_dataset = DatasetFood('food-recognition-challenge/train_labels.csv', transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1,\n",
    "                                          shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "num_classes = 80\n",
    "model = models.resnet101(pretrained=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=80, bias=True)\n",
       "    (4): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "    \n",
    "model.fc = nn.Sequential(nn.Linear(2048, 512),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.Linear(512, num_classes),\n",
    "                                 nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.003)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH 0\n",
      "OUTPUTS tensor([[-3.9943, -4.5387, -4.4552, -4.2421, -4.4956, -4.4521, -4.4008, -4.2444,\n",
      "         -4.3997, -4.4613, -4.4881, -4.4458, -4.1470, -4.5598, -4.3038, -4.4489,\n",
      "         -4.4525, -4.2792, -4.1141, -4.3420, -4.1410, -4.6512, -4.4753, -4.7473,\n",
      "         -4.2285, -4.5155, -4.4185, -4.2905, -4.4559, -4.4484, -4.4102, -4.3197,\n",
      "         -4.2994, -4.2813, -4.2996, -4.3994, -4.2163, -4.3152, -4.4387, -4.4414,\n",
      "         -4.4639, -4.0861, -4.3522, -4.3408, -4.6585, -4.7030, -4.3950, -4.4550,\n",
      "         -4.7754, -4.3310, -4.4782, -4.5658, -4.1792, -4.3530, -4.1966, -4.2610,\n",
      "         -4.2967, -4.6226, -4.4519, -4.6224, -4.4055, -4.3839, -4.3425, -4.2010,\n",
      "         -4.2440, -4.2970, -4.4093, -4.3528, -4.5288, -4.4446, -4.3683, -4.4215,\n",
      "         -4.4880, -4.2419, -4.5014, -4.5820, -4.6503, -4.2370, -4.2727, -4.4490]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 1\n",
      "OUTPUTS tensor([[ -8.6468,  -9.3935,  -8.5022,  -7.3077,  -9.7129,  -8.1131,  -9.0182,\n",
      "          -9.3360,  -8.5817,  -0.0164,  -8.3330, -10.3489,  -7.5783,  -9.2041,\n",
      "          -7.9095,  -9.6447,  -8.8126,  -8.1079,  -8.2550, -10.0003,  -7.7352,\n",
      "          -8.9132,  -9.0582,  -9.3583,  -9.1101,  -9.1648,  -9.4908,  -9.4586,\n",
      "          -8.5129,  -8.8811, -10.2881,  -8.9581,  -8.2846,  -8.9417,  -7.3765,\n",
      "          -8.9220,  -8.2451,  -8.5351,  -9.4098,  -9.0656,  -9.4587,  -8.1083,\n",
      "          -8.1837,  -9.0744,  -9.1281, -11.6347,  -8.8213,  -9.6234, -10.1363,\n",
      "          -8.0254,  -8.2931,  -8.9586,  -9.4921,  -8.0946,  -7.9898,  -8.0001,\n",
      "          -7.6120,  -9.1618,  -8.4577, -10.2895,  -8.8597, -10.6046,  -9.7866,\n",
      "          -8.6664,  -8.0483,  -7.7278,  -8.5300,  -9.6569,  -8.6788, -10.2216,\n",
      "          -9.3225,  -9.4909,  -9.1796,  -6.7891,  -9.8092, -10.1560,  -9.6167,\n",
      "          -6.5454,  -7.7124,  -9.5480]], grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 2\n",
      "OUTPUTS tensor([[ -9.9223,  -9.1817,  -9.2711,  -8.2225,  -8.8892,  -9.3130,  -8.8906,\n",
      "          -8.7930,  -9.4801,  -0.0314,  -8.6863, -12.1017,  -7.6224,  -9.5198,\n",
      "          -8.1271,  -9.6837,  -9.7155,  -6.8402,  -8.4250, -10.5433,  -9.4887,\n",
      "         -10.4587,  -9.7682, -11.3365, -10.5622,  -9.4455,  -9.9672,  -9.9593,\n",
      "          -9.7678,  -8.9485, -10.2876, -10.0497,  -8.4785,  -9.6168,  -9.3302,\n",
      "         -10.7838,  -8.9272,  -8.9952, -10.2392,  -8.5522,  -9.2886,  -9.0141,\n",
      "          -8.6987,  -8.6512, -10.0957, -10.4126,  -9.2119, -10.9772, -10.6510,\n",
      "          -9.1887,  -8.4118,  -8.5043,  -9.9313,  -8.4545,  -9.3362,  -9.2490,\n",
      "          -8.5414,  -9.8402,  -8.5602,  -9.6242,  -9.9772, -12.3374,  -3.9069,\n",
      "          -8.7337,  -8.3135,  -9.2908,  -9.1489, -11.3826,  -8.0849, -11.3654,\n",
      "          -9.5953, -10.4882,  -9.6553,  -7.8396, -10.9391, -10.6071, -10.3447,\n",
      "          -6.5893,  -7.6311, -10.0784]], grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 3\n",
      "OUTPUTS tensor([[ -8.9541,  -8.0031,  -9.2271,  -8.3874,  -7.9006,  -7.6070,  -7.4588,\n",
      "          -8.3868,  -8.5861,  -2.4908,  -7.2865, -10.1650,  -6.4913,  -9.7160,\n",
      "          -7.1841,  -8.0898,  -7.4756,  -7.3155,  -8.5181,  -9.2550,  -9.0440,\n",
      "          -7.9393,  -6.7886, -10.2906,  -8.5913,  -7.7050,  -7.5503,  -9.3568,\n",
      "          -8.2283,  -7.4208,  -9.3611,  -8.5780,  -7.6749, -10.2330,  -6.5033,\n",
      "          -8.6834,  -8.2638,  -8.5427,  -8.6060,  -8.2329,  -8.1938,  -7.6618,\n",
      "          -6.5325,  -8.9278,  -9.7349,  -7.4737,  -6.9301,  -7.5945, -10.2813,\n",
      "          -5.6096,  -7.8753,  -7.8608,  -9.1999,  -6.1130,  -8.2419,  -9.5047,\n",
      "          -8.4165,  -9.0720,  -9.1961,  -9.5861,  -8.4831, -10.5864,  -0.1285,\n",
      "          -8.1664,  -6.4262,  -7.9112,  -7.5815, -10.5457,  -7.9719,  -9.1257,\n",
      "          -9.6453,  -9.3470,  -9.6405,  -6.4844, -10.3972,  -8.6059,  -9.4348,\n",
      "          -6.5349,  -5.5166,  -9.5812]], grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 4\n",
      "OUTPUTS tensor([[ -8.8942,  -8.9511,  -7.2060,  -7.9420,  -7.5967,  -6.7453,  -8.1868,\n",
      "          -8.2396,  -6.4463,  -4.1553,  -9.0305,  -8.5139,  -7.0850,  -8.2544,\n",
      "          -6.1125,  -8.6309,  -8.2410,  -6.1155,  -8.8524,  -7.7098,  -7.7838,\n",
      "          -8.0122,  -9.3373, -12.2789,  -7.6748,  -7.5685,  -7.3486, -10.1526,\n",
      "          -8.6306,  -7.4153,  -9.5066,  -8.1568,  -8.9425,  -9.0592,  -7.9920,\n",
      "          -8.5086,  -7.6663,  -8.7473,  -9.8233,  -9.3334,  -9.8004,  -9.2370,\n",
      "          -7.3402,  -8.5815,  -9.0827,  -7.7181,  -6.1915,  -7.5807,  -8.9095,\n",
      "          -3.3253,  -6.7339,  -7.6261,  -8.9744,  -6.3144,  -9.1521,  -8.2389,\n",
      "          -7.3712,  -9.3319,  -8.6349,  -8.8051,  -8.6272,  -9.7738,  -0.0970,\n",
      "          -7.3112,  -8.1783,  -9.2620,  -9.1517,  -9.9965,  -6.2520,  -6.9999,\n",
      "         -10.3552,  -9.0032,  -6.7046,  -6.4579, -11.5106,  -7.7949, -10.7157,\n",
      "          -6.9520,  -5.2037,  -8.8303]], grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 5\n",
      "OUTPUTS tensor([[-6.3629, -6.9458, -4.5130, -6.8393, -6.2763, -4.4347, -5.7497, -6.9935,\n",
      "         -5.4483, -3.5260, -5.6081, -7.2163, -5.4107, -7.0499, -6.4474, -4.7877,\n",
      "         -4.7744, -4.5421, -4.9624, -5.7827, -7.6170, -4.8237, -6.1603, -8.3377,\n",
      "         -6.6957, -6.2525, -5.0298, -8.4452, -6.4369, -4.9358, -8.3833, -7.6797,\n",
      "         -5.4546, -7.8559, -4.2502, -6.7211, -6.2723, -3.6652, -7.4381, -7.9613,\n",
      "         -7.5821, -7.2261, -5.2874, -7.8979, -9.2507, -4.5236, -5.4463, -5.1050,\n",
      "         -7.3733, -1.5950, -4.8087, -6.4678, -8.7732, -5.3353, -7.1074, -7.3327,\n",
      "         -6.4171, -6.3991, -7.8837, -8.0828, -6.1088, -8.0709, -1.1516, -6.5273,\n",
      "         -5.0381, -6.2931, -4.9555, -6.1776, -6.2397, -2.1899, -8.3924, -6.4571,\n",
      "         -6.0705, -4.5466, -9.2352, -5.2872, -7.2197, -5.5460, -2.5508, -7.7825]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 6\n",
      "OUTPUTS tensor([[ -7.9850,  -6.7680,  -6.0013,  -7.4584,  -6.9283,  -5.6639,  -7.0993,\n",
      "          -9.3119,  -6.1539,  -5.8047,  -7.7349,  -7.4232,  -5.8195,  -8.3463,\n",
      "          -7.1252,  -4.5238,  -7.3218,  -5.6374,  -6.9926,  -7.0137,  -8.1100,\n",
      "          -7.4034,  -6.0240, -11.6176,  -7.2699,  -8.2928,  -6.8353,  -9.5785,\n",
      "          -6.8858,  -6.9607,  -9.2896,  -9.6232,  -5.9876,  -8.5222,  -5.4360,\n",
      "          -8.0293,  -7.1924,  -5.0507,  -8.0863,  -8.8996,  -8.0660,  -7.3453,\n",
      "          -6.0890,  -9.6777, -10.3462,  -4.9747,  -6.1732,  -5.9490,  -6.6397,\n",
      "          -0.8256,  -5.7376,  -6.9260,  -9.7304,  -7.2311,  -6.9003,  -8.1258,\n",
      "          -6.5242,  -7.6934,  -8.4860,  -8.9861,  -6.9877,  -9.0388,  -2.9565,\n",
      "          -7.2153,  -6.9092,  -8.2495,  -7.3184,  -7.4867,  -8.1502,  -0.9209,\n",
      "          -9.6759,  -7.7180,  -6.4527,  -5.9066, -10.1745,  -6.9764,  -9.1240,\n",
      "          -7.1550,  -4.1234,  -8.8484]], grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 7\n",
      "OUTPUTS tensor([[ -8.6381,  -6.1674,  -5.4613,  -8.0593,  -5.8154,  -5.9902,  -6.6640,\n",
      "          -9.3544,  -6.8676,  -4.1049,  -6.4512,  -6.9389,  -5.9350,  -8.3751,\n",
      "          -7.8088,  -4.8331,  -7.8307,  -6.0051,  -7.0575,  -6.6415,  -8.4379,\n",
      "          -7.5150,  -6.1460, -10.2386,  -7.4810,  -9.1739,  -7.3504,  -9.3007,\n",
      "          -7.8087,  -6.0951, -10.0354,  -9.5726,  -4.6963,  -8.9468,  -6.0469,\n",
      "          -7.8818,  -7.0051,  -3.3903,  -7.7456,  -9.0444,  -8.4623,  -8.1607,\n",
      "          -6.6281,  -9.6685, -10.6300,  -4.8859,  -7.5495,  -6.4678,  -5.9236,\n",
      "          -1.3303,  -5.5929,  -7.1371, -10.6206,  -7.2007,  -6.1329,  -8.3437,\n",
      "          -6.9891,  -7.3409,  -7.5535,  -9.8559,  -7.6736,  -8.7585,  -4.5792,\n",
      "          -7.6825,  -6.8631,  -8.3589,  -7.2112,  -7.0497,  -8.1413,  -0.5479,\n",
      "          -9.4083,  -8.1381,  -8.4719,  -5.3109,  -9.5900,  -7.6193,  -7.5091,\n",
      "          -6.7506,  -5.0110,  -9.1794]], grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 8\n",
      "OUTPUTS tensor([[-6.7951, -6.3053, -4.1103, -7.2661, -5.7207, -4.9604, -6.6768, -7.8009,\n",
      "         -5.8189, -2.8477, -6.8792, -6.7062, -4.6861, -6.3537, -7.8289, -5.6997,\n",
      "         -6.9089, -5.1029, -5.5653, -6.2844, -7.0364, -7.2049, -5.2762, -9.9689,\n",
      "         -6.8989, -8.0637, -6.8675, -7.6708, -6.8820, -6.3925, -9.0721, -9.1586,\n",
      "         -5.2896, -8.5592, -6.4576, -7.6003, -6.3665, -2.4393, -7.3010, -7.7507,\n",
      "         -6.6759, -6.6325, -7.1229, -8.8046, -8.6210, -4.1969, -6.9224, -6.3202,\n",
      "         -4.5126, -2.1893, -4.6570, -7.7581, -9.3812, -7.4783, -6.4530, -6.5400,\n",
      "         -5.5373, -7.9066, -6.3467, -9.0908, -6.6418, -6.8718, -5.9096, -7.4205,\n",
      "         -7.2501, -7.2292, -7.5857, -7.3174, -7.4048, -0.5925, -7.4552, -7.5653,\n",
      "         -6.5164, -4.9571, -8.2272, -6.8317, -6.5588, -5.4706, -3.7030, -9.1295]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS tensor([[-4.4531, -4.2082, -3.1582, -6.5843, -4.5153, -4.2261, -5.4638, -7.3409,\n",
      "         -5.7262, -4.1060, -5.6698, -5.5900, -5.0469, -5.1357, -6.2429, -4.0005,\n",
      "         -6.2608, -5.3344, -4.4870, -6.5415, -5.7596, -4.1221, -4.2757, -6.9836,\n",
      "         -6.2830, -7.1494, -6.9255, -5.9619, -6.2941, -4.8301, -8.4145, -7.3096,\n",
      "         -3.8583, -6.5297, -5.1470, -7.1659, -5.1710, -1.9205, -6.0152, -6.2965,\n",
      "         -5.5580, -5.8593, -6.5290, -6.8402, -8.3039, -3.8542, -5.6521, -4.3418,\n",
      "         -4.2400, -2.3563, -4.5113, -5.5421, -7.9821, -6.5560, -4.7501, -5.6993,\n",
      "         -4.9726, -6.1346, -5.4876, -6.7378, -5.4186, -6.2619, -5.4730, -5.9000,\n",
      "         -5.6496, -7.2006, -6.1855, -5.3108, -6.0481, -1.1250, -7.3161, -5.7353,\n",
      "         -6.3660, -5.7218, -6.8019, -4.8051, -5.3237, -6.4664, -5.1262, -6.2529]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "[1,    10] loss: 0.038\n",
      "BATCH 10\n",
      "OUTPUTS tensor([[-2.9961, -5.4902, -4.0761, -6.5548, -5.3999, -4.7092, -5.2876, -6.3697,\n",
      "         -5.2637, -1.9148, -4.9009, -6.6628, -5.1911, -6.8051, -6.4255, -5.7141,\n",
      "         -4.5855, -4.4616, -2.4373, -4.3305, -6.6061, -3.5382, -4.9562, -8.5032,\n",
      "         -6.5346, -6.3742, -6.2870, -6.8082, -5.9236, -5.5281, -7.5841, -7.7106,\n",
      "         -6.2774, -7.8367, -5.3574, -6.4805, -6.2725, -1.4664, -6.8352, -8.4376,\n",
      "         -5.1731, -5.9180, -7.4915, -7.8507, -9.0068, -3.6217, -6.0437, -5.6415,\n",
      "         -3.9367, -4.2514, -4.1843, -6.8048, -8.4619, -6.1445, -5.8493, -6.2281,\n",
      "         -6.1990, -6.9911, -5.7610, -8.1925, -5.9538, -5.9129, -5.9504, -7.3983,\n",
      "         -4.8254, -6.5990, -4.3876, -6.1767, -5.7649, -2.0020, -7.8471, -6.7122,\n",
      "         -5.5408, -4.8966, -7.2516, -4.9723, -5.9819, -5.6300, -4.2379, -7.3614]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 11\n",
      "OUTPUTS tensor([[-1.9425, -4.0416, -4.6950, -6.4839, -4.6789, -5.7186, -5.7972, -6.3162,\n",
      "         -4.6144, -1.1507, -5.7657, -6.6011, -4.9518, -5.6157, -6.7326, -5.4526,\n",
      "         -5.3034, -4.3136, -2.2212, -5.0029, -5.9159, -3.7866, -4.4023, -8.2729,\n",
      "         -7.0357, -5.8424, -6.9930, -6.3728, -5.7233, -6.3818, -7.9761, -7.9467,\n",
      "         -6.3403, -7.5844, -5.8720, -7.8625, -5.1362, -2.4750, -6.0371, -8.4688,\n",
      "         -5.2765, -5.2666, -8.1277, -8.3110, -8.7709, -4.0214, -6.3579, -5.7600,\n",
      "         -3.9008, -5.1034, -3.9257, -6.4927, -8.6641, -6.9443, -5.4222, -6.3659,\n",
      "         -6.9708, -6.7279, -5.4632, -8.6951, -5.2815, -5.3141, -5.9540, -7.3871,\n",
      "         -5.3882, -7.0019, -5.5545, -7.6672, -4.9667, -3.9998, -6.2605, -5.8790,\n",
      "         -6.1735, -5.9703, -6.0259, -5.9988, -5.1981, -5.6107, -4.1905, -7.9414]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 12\n",
      "OUTPUTS tensor([[-1.6631, -3.8360, -5.1113, -7.6914, -4.1495, -5.4435, -5.7084, -6.2262,\n",
      "         -4.6016, -1.0956, -5.8434, -7.0286, -4.7714, -7.1331, -6.9450, -6.1008,\n",
      "         -6.2061, -5.2090, -3.5789, -5.0760, -6.4018, -3.1084, -5.9937, -9.3617,\n",
      "         -6.5914, -5.9843, -7.5502, -8.3087, -5.7768, -6.2366, -8.7260, -8.1061,\n",
      "         -6.3187, -7.8713, -6.0246, -7.9445, -5.3686, -2.6281, -6.5182, -8.0482,\n",
      "         -6.8279, -5.8261, -8.3156, -8.6210, -8.6268, -3.6545, -6.0186, -5.5980,\n",
      "         -2.6644, -4.3375, -4.1437, -6.2861, -9.8228, -6.7491, -5.2639, -6.3523,\n",
      "         -7.4809, -6.2294, -6.1060, -9.1893, -5.2368, -6.1794, -5.6512, -6.5651,\n",
      "         -6.9705, -7.9216, -5.3110, -6.5910, -3.9474, -5.0472, -7.1599, -7.0568,\n",
      "         -7.3978, -5.9024, -7.9192, -7.7423, -6.2803, -6.8255, -4.9753, -7.4527]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 13\n",
      "OUTPUTS tensor([[-2.3862, -4.0717, -4.7648, -6.3376, -3.4338, -6.6323, -5.8831, -4.3203,\n",
      "         -4.2777, -1.8169, -5.1724, -5.8804, -2.8619, -5.9269, -5.9336, -5.4965,\n",
      "         -5.5064, -3.5986, -4.2053, -4.1042, -5.3123, -4.7639, -5.8336, -8.4629,\n",
      "         -4.6583, -5.8642, -5.4323, -7.2369, -5.4355, -5.7465, -6.3859, -8.0058,\n",
      "         -4.8473, -7.3189, -6.9654, -7.0529, -5.7032, -2.9962, -4.7797, -7.5556,\n",
      "         -6.5985, -4.8373, -6.6672, -8.0206, -7.7585, -3.2247, -7.1844, -7.0631,\n",
      "         -2.2508, -4.6098, -3.8752, -5.9474, -8.0351, -5.7913, -5.0167, -5.6005,\n",
      "         -5.4393, -6.2830, -4.1253, -8.3694, -5.8492, -5.4413, -5.4286, -6.3718,\n",
      "         -6.5307, -6.5252, -5.0806, -5.8495, -3.5964, -4.6915, -5.5882, -6.0213,\n",
      "         -6.2678, -3.7424, -5.6564, -6.1950, -5.2747, -4.7738, -2.8161, -8.0275]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 14\n",
      "OUTPUTS tensor([[-1.7428, -3.1303, -5.8340, -7.4544, -2.3543, -6.0443, -5.6808, -5.8312,\n",
      "         -3.5255, -1.3707, -6.4582, -6.9569, -3.5262, -7.1714, -7.0081, -5.8613,\n",
      "         -5.9750, -5.6178, -4.1594, -5.5022, -6.7892, -2.4002, -5.4157, -9.0549,\n",
      "         -6.3111, -5.1959, -6.5962, -7.2118, -5.1682, -6.9761, -8.1883, -7.3038,\n",
      "         -7.0538, -7.3842, -5.2445, -8.2156, -4.9169, -4.5420, -5.0511, -6.9480,\n",
      "         -7.7576, -5.7051, -7.9391, -8.2850, -7.8660, -4.3081, -5.4289, -4.6327,\n",
      "         -2.8239, -5.3561, -4.7886, -5.8074, -9.2295, -7.5396, -5.3503, -6.0963,\n",
      "         -8.3355, -5.7251, -6.9830, -8.0199, -4.6923, -7.1535, -5.3936, -5.9648,\n",
      "         -6.9363, -6.7453, -5.3499, -6.1454, -4.1034, -5.8671, -6.9473, -6.8031,\n",
      "         -7.0616, -6.7990, -7.6264, -8.1945, -6.3462, -6.4555, -4.7870, -7.5227]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 15\n",
      "OUTPUTS tensor([[-2.3907, -3.5055, -5.2898, -7.4725, -2.3293, -5.8541, -4.5739, -4.9052,\n",
      "         -3.7647, -2.0348, -5.1871, -7.0713, -1.9300, -6.1709, -5.9253, -6.6793,\n",
      "         -4.6113, -4.8242, -3.5010, -4.7797, -5.3993, -2.8219, -5.1541, -6.6487,\n",
      "         -5.8994, -5.5635, -6.6082, -7.0089, -4.4034, -7.3464, -7.7432, -6.9911,\n",
      "         -5.4453, -7.3839, -5.9972, -7.5611, -4.5846, -3.3224, -5.9570, -6.6589,\n",
      "         -7.1289, -4.5943, -6.9915, -9.2184, -7.6006, -4.1945, -6.1307, -5.1485,\n",
      "         -3.8961, -5.3200, -5.2820, -6.8022, -7.7499, -7.5226, -5.0980, -6.9292,\n",
      "         -7.0590, -5.3771, -5.5081, -9.0082, -5.0642, -5.7521, -5.1643, -6.5967,\n",
      "         -5.3326, -7.2462, -5.6897, -5.9426, -2.6762, -4.7032, -5.6495, -4.3267,\n",
      "         -6.3148, -6.2162, -6.5201, -7.3892, -4.8894, -5.7133, -4.6377, -7.1208]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 16\n",
      "OUTPUTS tensor([[-1.8373, -4.6473, -7.2072, -7.6460, -2.0396, -7.4843, -6.7195, -5.6067,\n",
      "         -3.4558, -1.7350, -6.4670, -8.2136, -3.4313, -6.5176, -7.6748, -6.2261,\n",
      "         -5.8931, -5.5440, -4.1584, -6.0862, -6.6810, -3.1962, -6.9679, -9.7043,\n",
      "         -6.7592, -6.2375, -7.1131, -8.1093, -6.3602, -6.5256, -8.2185, -8.2689,\n",
      "         -6.6089, -6.4689, -6.8586, -7.4631, -4.8434, -3.7521, -6.1374, -9.6227,\n",
      "         -7.2003, -5.8522, -8.9832, -8.0889, -8.6454, -4.4793, -6.8083, -6.2727,\n",
      "         -1.4451, -5.5618, -4.3733, -6.9050, -8.6176, -7.6782, -6.1217, -6.5653,\n",
      "         -8.1527, -4.3285, -6.8483, -8.0992, -5.1396, -8.5675, -5.9728, -6.2062,\n",
      "         -7.5432, -7.7531, -6.0134, -8.0779, -5.1628, -5.7173, -6.6980, -6.0266,\n",
      "         -6.7337, -7.6415, -7.2073, -7.1414, -6.6245, -6.1052, -4.6185, -9.0172]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 17\n",
      "OUTPUTS tensor([[-2.1761, -3.8476, -6.4485, -7.2875, -1.1253, -6.9674, -6.4629, -5.0878,\n",
      "         -3.9212, -2.7007, -5.4568, -7.4532, -3.1431, -6.4673, -7.5899, -5.0380,\n",
      "         -6.3498, -5.2414, -5.6807, -7.2013, -6.2877, -3.4889, -5.1007, -8.8937,\n",
      "         -6.4363, -5.7343, -6.5476, -7.3092, -6.2454, -6.8009, -7.0834, -8.4221,\n",
      "         -5.3344, -7.2097, -6.7608, -8.1815, -4.5950, -4.6208, -4.3430, -8.1806,\n",
      "         -6.4877, -4.6307, -8.6500, -7.0873, -8.5523, -4.9835, -6.2690, -5.9245,\n",
      "         -2.2281, -5.1862, -3.3456, -5.3383, -8.6923, -7.9008, -4.9990, -6.2966,\n",
      "         -7.5919, -3.5077, -5.7759, -7.4940, -5.2040, -8.5827, -5.0040, -5.8522,\n",
      "         -8.6765, -6.4229, -6.7902, -7.3708, -5.6785, -5.4516, -5.9437, -5.2193,\n",
      "         -6.9118, -6.2290, -5.6161, -7.0648, -6.4573, -7.1197, -3.9105, -8.8606]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 18\n",
      "OUTPUTS tensor([[-1.9294, -3.8122, -6.4068, -6.9833, -1.2793, -6.6691, -4.7113, -5.6374,\n",
      "         -3.4475, -2.2156, -6.5585, -7.8707, -2.6677, -7.7333, -8.3718, -6.9308,\n",
      "         -5.8221, -7.1259, -4.1559, -5.7362, -6.7460, -2.2451, -5.2830, -7.7608,\n",
      "         -7.2963, -6.2532, -8.8333, -6.3637, -5.1895, -7.4236, -7.6946, -6.8563,\n",
      "         -7.1158, -6.6005, -5.9412, -8.0361, -3.9052, -4.1111, -5.1876, -7.1099,\n",
      "         -7.9778, -6.4943, -8.7165, -7.9861, -7.8054, -5.1612, -4.8455, -4.9305,\n",
      "         -4.0092, -6.6850, -5.2029, -5.6536, -8.8705, -7.9606, -6.6958, -6.5462,\n",
      "         -9.3415, -5.6646, -7.4681, -7.4512, -3.9924, -8.0969, -6.5217, -5.8136,\n",
      "         -6.8605, -6.5442, -6.2220, -6.8849, -4.8630, -6.6209, -6.5724, -4.0114,\n",
      "         -6.8949, -7.4271, -7.1892, -7.5422, -7.5242, -6.0799, -5.2683, -7.1375]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS tensor([[-2.7642, -4.6796, -4.7636, -8.0943, -1.7850, -5.4409, -5.5584, -5.6931,\n",
      "         -2.1012, -4.2865, -5.7392, -7.0727, -2.9180, -6.6107, -6.7397, -5.3148,\n",
      "         -6.2367, -5.4088, -4.7402, -6.6092, -6.2364, -1.9634, -4.7494, -5.9955,\n",
      "         -8.0152, -6.1440, -7.9459, -5.3682, -5.7486, -7.1745, -8.9270, -6.4407,\n",
      "         -6.4781, -7.2208, -5.5776, -7.1058, -4.1365, -3.4621, -6.5293, -5.0735,\n",
      "         -7.0336, -5.3895, -8.1474, -7.7333, -8.9771, -5.4169, -4.5832, -3.9070,\n",
      "         -5.2773, -4.5134, -5.9648, -5.3417, -8.9402, -7.4274, -5.3739, -7.3350,\n",
      "         -7.6080, -4.2722, -6.6628, -6.3477, -4.7821, -7.9476, -5.0697, -6.4562,\n",
      "         -6.0126, -6.4855, -6.5086, -4.9976, -2.9006, -5.2000, -7.1603, -2.3081,\n",
      "         -7.7441, -6.7262, -6.6277, -6.0221, -6.2186, -7.0617, -5.3403, -6.1044]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "[1,    20] loss: 0.028\n",
      "BATCH 20\n",
      "OUTPUTS tensor([[-3.3528, -4.6008, -5.1678, -7.9546, -1.7269, -6.4595, -5.3445, -5.4620,\n",
      "         -3.1291, -3.4100, -6.2872, -7.0162, -1.7091, -6.1561, -7.8585, -5.8608,\n",
      "         -6.4445, -5.8440, -5.2587, -6.4809, -6.0512, -2.4413, -4.3859, -6.0167,\n",
      "         -7.3069, -6.4603, -7.8184, -5.5932, -5.2144, -8.0163, -7.9399, -7.1295,\n",
      "         -6.2039, -7.3544, -6.6918, -8.6287, -3.1332, -4.4620, -5.8304, -5.8040,\n",
      "         -7.2005, -4.6263, -7.6086, -8.6542, -8.0226, -6.0504, -5.8630, -4.8613,\n",
      "         -5.2267, -4.5859, -6.1306, -6.6282, -8.4962, -8.8828, -6.2727, -7.4664,\n",
      "         -7.7807, -4.6243, -6.0681, -7.4758, -5.0168, -6.7586, -5.3666, -6.4660,\n",
      "         -6.3440, -6.1964, -7.2050, -6.7537, -3.0384, -5.7647, -6.1923, -1.9008,\n",
      "         -6.8091, -6.4145, -6.5906, -7.1053, -6.2725, -6.1557, -4.6867, -7.2199]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 21\n",
      "OUTPUTS tensor([[-2.7556, -4.7775, -5.1615, -7.5962, -1.7000, -6.4309, -5.3044, -5.0001,\n",
      "         -3.3809, -3.7620, -6.0092, -7.5074, -1.5124, -6.1888, -7.5422, -6.3276,\n",
      "         -6.3315, -6.0294, -5.7935, -6.5494, -6.1958, -2.6563, -4.5182, -6.6377,\n",
      "         -7.3411, -6.6425, -8.2267, -5.7788, -4.9672, -8.1654, -7.6613, -7.1692,\n",
      "         -6.0925, -7.7941, -6.9383, -8.3248, -3.6951, -4.1097, -5.6971, -6.1952,\n",
      "         -7.0640, -5.0841, -7.8385, -8.5386, -8.3861, -6.2896, -6.2118, -5.5920,\n",
      "         -5.2821, -4.4619, -5.4917, -6.4306, -8.4487, -8.7494, -5.8879, -7.1941,\n",
      "         -7.7237, -4.9195, -5.8193, -8.0498, -5.2741, -7.1834, -5.2627, -6.7192,\n",
      "         -6.7338, -6.5416, -7.6631, -6.6186, -3.5378, -5.1934, -6.3012, -1.8659,\n",
      "         -7.5239, -5.5443, -6.1323, -6.8571, -6.3995, -6.5474, -4.4833, -7.5207]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 22\n",
      "OUTPUTS tensor([[-1.9290, -4.5783, -5.3684, -8.3555, -1.7378, -6.5129, -6.6409, -6.6003,\n",
      "         -3.1992, -3.5222, -7.2626, -7.1735, -1.9251, -6.3544, -8.3336, -6.1294,\n",
      "         -7.0133, -6.0934, -4.8747, -6.6282, -7.1765, -2.2344, -5.3142, -7.5085,\n",
      "         -7.5939, -6.4965, -7.7107, -6.7321, -6.2940, -7.7318, -8.4404, -8.0417,\n",
      "         -6.6124, -7.0021, -6.3317, -8.7350, -2.9006, -4.2311, -5.8591, -6.6666,\n",
      "         -7.5726, -6.2294, -8.5665, -8.7570, -8.6288, -5.7105, -6.4433, -5.2698,\n",
      "         -4.0247, -4.3030, -5.6510, -6.3383, -9.5403, -8.5639, -6.2453, -7.5957,\n",
      "         -7.8681, -4.3132, -6.1929, -7.7261, -5.4730, -7.1927, -5.4562, -7.4658,\n",
      "         -7.4012, -6.9362, -6.9466, -6.7928, -3.8140, -5.3809, -7.2740, -2.2490,\n",
      "         -8.7535, -5.8564, -7.5900, -7.7825, -6.6147, -6.5212, -5.1585, -7.8148]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 23\n",
      "OUTPUTS tensor([[-2.0805, -4.1644, -5.1419, -7.6291, -2.1534, -5.7297, -6.5500, -6.5401,\n",
      "         -3.3105, -3.2895, -7.2218, -6.4970, -2.0706, -5.7222, -8.0220, -5.2296,\n",
      "         -7.0020, -5.9325, -4.6708, -6.4685, -6.8572, -2.3437, -4.8904, -6.8686,\n",
      "         -7.0377, -6.0677, -7.0763, -5.8107, -5.9150, -7.1030, -7.3095, -7.3553,\n",
      "         -6.1381, -6.1033, -5.7319, -8.2460, -2.5706, -4.4836, -5.4154, -5.9222,\n",
      "         -6.7698, -5.6950, -7.7125, -7.7588, -7.6145, -5.7433, -6.0055, -4.5747,\n",
      "         -3.3993, -3.5652, -5.4307, -5.9327, -8.6562, -7.8851, -5.8286, -7.0732,\n",
      "         -7.2429, -4.1766, -5.9121, -6.8866, -5.0349, -6.3773, -5.0183, -6.9334,\n",
      "         -6.9521, -6.1735, -6.2341, -6.3240, -3.8261, -4.8651, -6.8843, -2.2910,\n",
      "         -7.9542, -5.6675, -7.4618, -7.4339, -6.3324, -6.1333, -5.0778, -6.8252]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 24\n",
      "OUTPUTS tensor([[-1.9504, -4.9681, -5.5227, -6.6944, -2.3109, -5.5333, -4.9261, -5.1115,\n",
      "         -3.6973, -3.8935, -5.9540, -6.8516, -3.0861, -5.2821, -7.1839, -4.3925,\n",
      "         -6.7898, -5.8223, -5.7288, -6.6540, -6.6076, -2.9827, -5.2644, -5.5207,\n",
      "         -7.1138, -5.7449, -6.7659, -5.0668, -4.5315, -5.8029, -5.0001, -5.3481,\n",
      "         -5.4572, -6.0886, -5.6384, -8.0317, -3.1198, -4.2637, -5.6892, -5.1138,\n",
      "         -6.6517, -4.7847, -6.0692, -7.1547, -7.5777, -5.9782, -5.1898, -4.8593,\n",
      "         -4.0497, -2.1586, -4.9084, -5.2555, -7.5597, -7.4453, -4.9824, -5.7597,\n",
      "         -7.7047, -3.5704, -5.8742, -6.3056, -4.3613, -6.6892, -4.7339, -5.1649,\n",
      "         -6.0221, -5.8966, -6.6674, -5.4173, -3.9024, -4.3953, -6.7391, -2.2534,\n",
      "         -6.6496, -4.5493, -5.9949, -5.9382, -6.1234, -6.2772, -4.6004, -5.6191]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 25\n",
      "OUTPUTS tensor([[-2.0947, -4.7502, -5.2806, -6.2992, -2.3422, -4.9655, -4.3408, -5.0195,\n",
      "         -3.5033, -3.4021, -5.9203, -6.2247, -3.4546, -4.9847, -7.3415, -3.9039,\n",
      "         -6.2172, -5.3888, -4.6952, -5.6996, -6.6131, -3.6580, -5.0699, -5.1927,\n",
      "         -6.9382, -5.6598, -6.7041, -4.6252, -4.8879, -5.1537, -4.4282, -4.8897,\n",
      "         -5.5397, -5.1003, -5.4041, -7.3921, -3.1685, -3.7238, -5.5291, -5.3077,\n",
      "         -5.7566, -4.6714, -5.9962, -6.2860, -7.0615, -5.6315, -4.3693, -4.3413,\n",
      "         -3.2506, -2.4525, -4.3241, -4.5481, -7.4926, -6.5529, -5.2057, -5.2599,\n",
      "         -7.1800, -3.4512, -5.6429, -5.4633, -4.4119, -6.1108, -5.6079, -4.9260,\n",
      "         -5.8316, -5.4308, -5.6111, -5.4915, -4.7026, -4.2332, -6.7265, -2.8296,\n",
      "         -5.9008, -4.3160, -5.6833, -5.4645, -6.4244, -5.6103, -4.7763, -5.3938]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 26\n",
      "OUTPUTS tensor([[-2.4985, -3.4254, -4.7035, -6.5141, -2.8024, -4.6392, -5.1695, -5.8472,\n",
      "         -4.1470, -3.4694, -6.3020, -5.3610, -2.9197, -4.7391, -6.4036, -4.4251,\n",
      "         -5.9741, -4.9840, -3.8842, -5.1586, -5.7117, -3.1767, -4.5902, -5.3615,\n",
      "         -6.1997, -5.0485, -5.6259, -5.1178, -5.5472, -5.6412, -5.2717, -5.3492,\n",
      "         -5.0918, -4.6800, -4.8796, -6.6006, -2.4850, -4.0927, -4.7171, -5.2456,\n",
      "         -5.3867, -5.0166, -6.3482, -6.2001, -6.3263, -4.9869, -4.8381, -4.0197,\n",
      "         -2.7184, -3.5484, -4.6213, -4.7442, -7.4366, -6.2016, -4.8956, -5.7862,\n",
      "         -5.8407, -3.5299, -4.9942, -5.5918, -4.7459, -5.2472, -4.8332, -5.9795,\n",
      "         -5.9971, -5.4680, -4.7820, -5.2158, -4.0538, -4.0849, -6.3491, -2.7744,\n",
      "         -6.6644, -4.7198, -6.1256, -6.0591, -5.5785, -5.3336, -4.8931, -5.6068]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 27\n",
      "OUTPUTS tensor([[-2.0810, -4.0408, -5.4613, -5.9028, -2.7227, -4.8124, -4.3813, -4.7893,\n",
      "         -4.1808, -3.7810, -5.5778, -5.6226, -4.1463, -4.4810, -6.5410, -3.9613,\n",
      "         -6.0666, -5.2337, -4.9824, -5.8210, -6.5383, -4.0299, -4.8822, -4.8749,\n",
      "         -6.3164, -5.5503, -6.2758, -4.4281, -4.7719, -4.8573, -3.9545, -4.3399,\n",
      "         -5.0086, -4.6234, -5.5694, -7.0586, -3.2202, -4.5905, -5.4369, -5.0668,\n",
      "         -5.4723, -4.4750, -5.2225, -5.7668, -6.6171, -5.4594, -4.3452, -4.7397,\n",
      "         -3.7942, -2.2361, -3.8615, -4.3752, -6.9961, -6.2046, -4.4918, -4.9411,\n",
      "         -6.6607, -3.0944, -5.0977, -5.2444, -4.1106, -5.9288, -5.2481, -4.5803,\n",
      "         -5.7414, -5.1202, -5.4898, -5.2952, -4.3340, -4.1750, -6.0297, -3.3615,\n",
      "         -5.5450, -4.3025, -5.4294, -5.5806, -5.9120, -5.7674, -4.5935, -4.9098]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 28\n",
      "OUTPUTS tensor([[-2.1609, -3.1734, -5.2772, -5.8946, -3.8960, -4.6478, -5.2595, -5.5357,\n",
      "         -4.3366, -3.2342, -6.5833, -5.5785, -3.3493, -4.4653, -5.2615, -4.5285,\n",
      "         -6.2371, -5.3118, -4.2881, -5.4338, -4.8844, -3.2427, -4.9489, -5.6633,\n",
      "         -6.1138, -5.0782, -4.9818, -4.4128, -5.3298, -5.5520, -4.3473, -4.1241,\n",
      "         -5.1906, -3.9660, -5.0981, -5.4740, -3.7725, -4.3675, -5.0381, -5.6042,\n",
      "         -4.8967, -4.8504, -6.3868, -4.9599, -5.5127, -5.4180, -5.1103, -4.4364,\n",
      "         -2.2959, -3.0920, -4.0209, -5.1555, -6.1207, -5.9513, -4.3597, -5.0150,\n",
      "         -5.8847, -4.2644, -5.1836, -5.6434, -4.2242, -5.7245, -4.8141, -5.7085,\n",
      "         -5.7462, -5.1874, -4.4095, -5.1447, -4.6843, -3.3917, -6.1901, -3.6319,\n",
      "         -6.1293, -4.4248, -5.9083, -5.2903, -5.3058, -5.1493, -4.4873, -5.1326]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS tensor([[-2.8380, -3.2465, -5.4003, -5.2601, -3.9064, -4.3724, -3.7572, -4.6058,\n",
      "         -5.0866, -3.6469, -5.5594, -5.6818, -4.6050, -3.6815, -5.0253, -3.7423,\n",
      "         -5.7596, -5.0314, -4.2981, -4.8774, -5.3879, -4.3532, -5.0544, -3.9928,\n",
      "         -6.2150, -5.2106, -5.5452, -3.9671, -4.9251, -4.6322, -3.3930, -3.7576,\n",
      "         -4.8938, -4.4317, -5.2757, -5.3707, -3.9729, -4.4744, -5.3754, -4.9250,\n",
      "         -4.2040, -4.2293, -4.6868, -4.6153, -5.7424, -5.5548, -3.7297, -4.4553,\n",
      "         -4.0512, -2.8533, -3.7205, -4.1095, -6.0599, -5.5149, -4.1853, -4.4915,\n",
      "         -6.2553, -3.5240, -5.1761, -5.0412, -3.6779, -5.9509, -5.6019, -4.3309,\n",
      "         -5.6074, -4.9772, -4.7815, -4.9419, -4.0844, -3.6775, -5.8667, -4.5785,\n",
      "         -4.7866, -4.8087, -5.0136, -4.6088, -5.5255, -5.5090, -4.5720, -4.3962]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "[1,    30] loss: 0.028\n",
      "BATCH 30\n",
      "OUTPUTS tensor([[-1.8877, -3.8286, -5.5243, -5.2975, -4.5424, -5.2828, -5.1997, -4.9332,\n",
      "         -4.1855, -3.5577, -6.5169, -5.0761, -4.1904, -3.9212, -3.9489, -4.8981,\n",
      "         -5.7894, -5.0954, -4.3456, -5.5203, -4.5172, -3.5340, -5.1770, -5.5098,\n",
      "         -5.8621, -4.8535, -4.4494, -3.9798, -5.2496, -5.4790, -4.4308, -3.5988,\n",
      "         -5.4130, -4.1282, -5.5255, -4.6997, -4.4947, -4.6231, -5.7090, -5.4233,\n",
      "         -4.2477, -4.5648, -5.6989, -4.6283, -4.9992, -5.3042, -4.8071, -4.5346,\n",
      "         -3.0230, -3.3171, -3.4426, -5.1007, -5.2237, -5.7019, -3.7948, -4.5131,\n",
      "         -5.8344, -4.1551, -5.0629, -5.1659, -3.9863, -5.5809, -4.6317, -4.8805,\n",
      "         -5.3789, -4.7304, -4.5372, -5.4719, -4.4496, -3.8208, -5.8851, -4.1987,\n",
      "         -5.3641, -4.5622, -5.6627, -5.0429, -5.1498, -5.0128, -3.8887, -5.0199]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 31\n",
      "OUTPUTS tensor([[-3.0500, -3.8604, -5.1935, -5.3006, -4.7665, -4.5747, -5.7662, -5.2095,\n",
      "         -4.3241, -4.0285, -6.1901, -6.9108, -4.0665, -3.4972, -5.1164, -3.4858,\n",
      "         -7.5847, -5.5895, -5.4930, -6.2334, -4.7558, -4.8639, -5.8273, -5.4276,\n",
      "         -6.6855, -6.1671, -5.3026, -3.0653, -4.8958, -5.1716, -3.0517, -2.8376,\n",
      "         -5.6298, -3.9172, -5.5594, -5.2632, -5.5731, -4.5810, -5.9699, -4.6087,\n",
      "         -3.4345, -3.8345, -5.5036, -4.2987, -5.7843, -6.9466, -5.0785, -4.6514,\n",
      "         -2.8519, -2.1921, -3.9671, -4.8984, -5.5196, -5.1158, -4.2660, -4.4415,\n",
      "         -6.4667, -4.4703, -6.3778, -5.9213, -4.9665, -6.1638, -5.1667, -4.8564,\n",
      "         -6.0909, -5.5006, -5.6376, -4.9792, -5.6769, -2.8242, -6.5834, -4.9371,\n",
      "         -5.9571, -4.3584, -5.2949, -4.2278, -5.8220, -6.0829, -5.0118, -4.5511]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 32\n",
      "OUTPUTS tensor([[-2.1663, -3.6052, -6.3003, -5.6269, -6.0012, -5.3862, -5.1882, -5.1283,\n",
      "         -5.1282, -3.8678, -7.1407, -6.6381, -5.0102, -3.1299, -4.0380, -3.9339,\n",
      "         -7.4160, -5.3224, -4.9884, -5.8806, -4.9354, -5.2125, -5.9252, -4.7289,\n",
      "         -7.3119, -5.9139, -4.6802, -2.4979, -6.0450, -5.8434, -3.1843, -3.0286,\n",
      "         -6.1523, -4.9465, -6.4588, -4.6016, -5.9287, -5.4753, -6.7084, -5.6999,\n",
      "         -2.6704, -4.0922, -5.4047, -4.1227, -5.8491, -7.2441, -4.5624, -5.0604,\n",
      "         -3.7352, -2.8182, -3.3843, -4.9970, -5.9903, -6.0171, -3.9247, -4.4619,\n",
      "         -7.0659, -3.6960, -5.9565, -6.0127, -4.3575, -7.0244, -5.9941, -5.1786,\n",
      "         -6.5843, -5.0607, -5.1786, -6.0099, -5.4314, -3.0723, -7.1132, -5.8886,\n",
      "         -5.4217, -5.1532, -5.7906, -4.5780, -6.0564, -6.1331, -4.3722, -5.2074]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 33\n",
      "OUTPUTS tensor([[-1.5319, -4.6523, -6.4854, -4.8321, -5.8391, -5.9541, -4.9459, -4.7217,\n",
      "         -4.6124, -3.9079, -6.4003, -5.7841, -5.3290, -3.3328, -3.6493, -4.6033,\n",
      "         -6.1728, -5.3254, -4.8473, -5.8542, -5.3928, -4.6200, -5.9385, -4.4379,\n",
      "         -6.6435, -5.6771, -4.7196, -2.8313, -5.5334, -5.3059, -3.7425, -3.3701,\n",
      "         -5.9431, -5.0145, -6.3932, -4.6973, -5.9446, -5.4199, -6.8729, -5.6793,\n",
      "         -3.5196, -4.4188, -4.6218, -4.4767, -5.5714, -6.2229, -4.4691, -5.4842,\n",
      "         -4.7839, -2.9608, -3.5218, -5.1075, -5.1182, -6.0170, -3.8059, -4.2057,\n",
      "         -6.6300, -4.0017, -5.6257, -5.4230, -3.7272, -6.7168, -5.6264, -4.2810,\n",
      "         -5.7155, -4.6430, -5.3794, -6.2199, -4.8070, -3.8716, -6.1162, -5.8861,\n",
      "         -4.5258, -5.0825, -5.4461, -4.6699, -5.7434, -5.6095, -3.8883, -4.9810]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 34\n",
      "OUTPUTS tensor([[-2.5553, -3.4274, -6.3345, -5.6468, -6.4481, -5.3929, -4.9183, -5.2291,\n",
      "         -5.4898, -4.0009, -7.0884, -6.4149, -5.2072, -3.2000, -3.6288, -4.1106,\n",
      "         -7.0549, -5.1378, -4.7274, -5.3707, -4.8305, -5.4260, -5.7941, -3.9627,\n",
      "         -7.1602, -5.7172, -4.3595, -2.1599, -6.3743, -5.9587, -3.3642, -3.2749,\n",
      "         -6.1441, -5.3065, -6.3914, -4.2010, -5.9352, -5.6956, -6.5226, -5.8932,\n",
      "         -2.1241, -4.3223, -5.2864, -4.0118, -5.6321, -7.1117, -4.4382, -5.1553,\n",
      "         -4.2241, -3.5333, -3.4339, -5.0067, -6.0001, -6.0006, -4.1157, -4.6041,\n",
      "         -6.0447, -3.7227, -5.7312, -6.0197, -4.4056, -6.9620, -6.1826, -5.5018,\n",
      "         -6.5628, -4.9477, -4.8796, -6.0346, -5.1146, -3.2364, -6.9959, -6.1814,\n",
      "         -5.2409, -5.4583, -5.8276, -4.6239, -5.9175, -5.9587, -4.4705, -5.3532]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 35\n",
      "OUTPUTS tensor([[-2.9455, -3.5367, -6.1926, -5.3402, -6.5827, -5.4246, -4.6133, -5.0772,\n",
      "         -5.5087, -4.0353, -6.7602, -6.1616, -5.0656, -3.3630, -3.5450, -4.2664,\n",
      "         -6.5712, -4.9613, -4.4831, -4.8941, -4.7002, -5.4358, -5.5961, -3.5539,\n",
      "         -6.8173, -5.5835, -4.2042, -2.0503, -6.2257, -5.9148, -3.3962, -3.4560,\n",
      "         -6.0555, -5.5947, -6.2715, -3.9799, -5.7759, -5.7571, -6.3020, -5.7886,\n",
      "         -2.0516, -4.3282, -4.9569, -4.0363, -5.3462, -6.8806, -4.3152, -5.2083,\n",
      "         -4.7331, -3.9833, -3.6236, -4.9760, -5.7381, -5.8844, -4.2441, -4.5600,\n",
      "         -5.5560, -3.8306, -5.4998, -5.9619, -4.3469, -6.6162, -6.2717, -5.0854,\n",
      "         -6.2640, -4.7354, -4.7850, -6.0117, -4.7732, -3.4429, -6.5902, -6.2360,\n",
      "         -4.8932, -5.5946, -5.6194, -4.5788, -5.7059, -5.6059, -4.4213, -5.3248]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 36\n",
      "OUTPUTS tensor([[-3.4656, -3.2841, -5.8333, -5.4121, -6.3715, -4.8834, -4.2926, -5.2017,\n",
      "         -5.4566, -3.9041, -6.3647, -5.1818, -4.9686, -3.8540, -3.2988, -4.5003,\n",
      "         -5.5481, -4.5700, -4.0276, -4.2610, -4.2916, -4.6906, -4.9360, -3.6842,\n",
      "         -6.0299, -4.6568, -3.5765, -2.8190, -5.9975, -5.6672, -4.0879, -3.8913,\n",
      "         -5.4176, -5.3813, -5.4069, -3.5284, -5.1772, -5.4578, -5.3302, -5.9498,\n",
      "         -2.2284, -4.7618, -5.2514, -3.7223, -4.7376, -5.7874, -4.2750, -4.9342,\n",
      "         -4.4182, -4.7260, -3.7269, -4.9466, -5.3486, -5.7649, -4.3291, -4.6776,\n",
      "         -4.4890, -4.0697, -4.7078, -5.3653, -4.0923, -6.1253, -5.6924, -5.4109,\n",
      "         -5.6278, -4.4399, -3.7672, -5.3745, -4.3996, -3.5662, -5.9226, -5.6023,\n",
      "         -4.7206, -5.3553, -5.4186, -4.4690, -4.8681, -4.9333, -4.3189, -5.1812]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 37\n",
      "OUTPUTS tensor([[-3.3865, -3.3567, -5.7976, -5.3309, -5.7877, -5.1496, -4.1793, -4.8889,\n",
      "         -5.7600, -4.4303, -6.2338, -5.6251, -5.4461, -3.5535, -3.3696, -4.2175,\n",
      "         -5.8062, -4.5065, -4.2202, -4.5310, -4.7216, -5.6991, -5.2060, -3.1513,\n",
      "         -6.4079, -5.1172, -4.3475, -2.1350, -5.5325, -5.5576, -3.9338, -4.0313,\n",
      "         -5.5619, -5.5963, -6.0080, -4.0085, -5.0067, -5.4755, -5.9166, -5.4030,\n",
      "         -2.3363, -4.4176, -4.5400, -4.1910, -5.2265, -6.2831, -3.9274, -4.7319,\n",
      "         -5.0318, -4.4932, -3.3937, -4.3484, -5.6754, -5.3786, -4.3232, -4.6614,\n",
      "         -4.7749, -3.4532, -4.9747, -5.4015, -4.5093, -5.9569, -6.0319, -4.3192,\n",
      "         -6.1107, -4.6774, -4.5070, -5.6082, -4.1314, -4.0054, -6.3335, -5.8558,\n",
      "         -4.7670, -5.7619, -5.4556, -4.7745, -5.7057, -5.4806, -4.5546, -5.2615]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 38\n",
      "OUTPUTS tensor([[-3.2785, -3.5046, -6.0089, -5.2169, -5.6786, -4.9783, -4.2442, -5.1267,\n",
      "         -5.2459, -4.0256, -5.9524, -5.5560, -5.0706, -3.7211, -3.5443, -4.1689,\n",
      "         -5.5204, -4.5727, -4.4648, -4.8593, -4.6551, -4.8985, -5.1243, -3.3066,\n",
      "         -6.2698, -4.9182, -4.1650, -2.3352, -4.9713, -5.2411, -3.5945, -3.7798,\n",
      "         -5.2111, -4.7945, -5.6551, -4.1376, -5.0119, -5.1988, -5.6671, -5.5831,\n",
      "         -2.6659, -4.6503, -4.6458, -3.8800, -5.0102, -5.9885, -4.4186, -5.0898,\n",
      "         -4.6625, -4.0166, -3.8739, -4.6387, -4.6383, -5.5685, -4.6226, -4.4584,\n",
      "         -4.5659, -4.1206, -4.7194, -5.2909, -4.3090, -5.7889, -5.5729, -4.3942,\n",
      "         -5.6460, -4.6579, -4.2112, -5.2416, -4.3768, -3.2127, -5.9355, -5.3034,\n",
      "         -4.6338, -5.1885, -5.1157, -4.3259, -5.2739, -5.1117, -4.4935, -5.0607]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS tensor([[-2.3539, -4.6303, -5.7670, -4.5643, -5.0660, -5.6281, -4.6649, -4.7722,\n",
      "         -4.6764, -4.0655, -5.7817, -5.2076, -4.9528, -3.8334, -3.4233, -4.6316,\n",
      "         -4.9840, -4.3798, -4.1763, -5.1600, -4.4526, -4.7842, -5.2652, -3.8701,\n",
      "         -5.8805, -4.8318, -4.2695, -2.6629, -4.4200, -5.1267, -4.3016, -3.9293,\n",
      "         -5.2131, -4.6542, -5.9254, -4.2277, -4.7190, -4.7711, -6.1199, -5.0016,\n",
      "         -3.5394, -4.5202, -4.0070, -4.4640, -4.6588, -5.5884, -4.5725, -4.6018,\n",
      "         -4.6941, -4.0947, -3.9097, -4.4878, -4.1465, -4.9896, -4.3015, -4.2076,\n",
      "         -4.8951, -4.0529, -4.6272, -5.0402, -4.4314, -4.7160, -4.8429, -3.4998,\n",
      "         -5.3253, -4.5184, -4.6220, -5.4590, -3.8699, -3.9840, -5.6538, -4.9915,\n",
      "         -4.5697, -5.1476, -5.1676, -4.7261, -5.4894, -4.7841, -4.0831, -5.1171]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "[1,    40] loss: 0.026\n",
      "BATCH 40\n",
      "OUTPUTS tensor([[-4.5221, -3.8041, -4.9431, -4.7107, -4.3771, -4.4059, -4.7335, -5.2469,\n",
      "         -4.7940, -4.0768, -5.0493, -5.5791, -4.2788, -4.3592, -4.8114, -3.8824,\n",
      "         -5.0562, -4.2303, -4.3812, -4.9656, -3.9484, -4.7545, -4.9282, -3.8421,\n",
      "         -5.4509, -4.5673, -4.3596, -3.2601, -4.1339, -4.5424, -4.0715, -3.8829,\n",
      "         -4.3779, -3.7271, -4.8646, -4.4176, -4.1734, -4.0707, -4.9584, -4.4213,\n",
      "         -3.8078, -4.5303, -4.2281, -4.0535, -4.4066, -5.4620, -5.0148, -4.1571,\n",
      "         -3.8470, -4.0691, -4.6496, -4.2141, -3.8893, -4.2087, -5.0364, -4.3106,\n",
      "         -4.5627, -4.6860, -4.5151, -5.0597, -5.1037, -4.1548, -4.3878, -4.3915,\n",
      "         -5.1755, -5.0094, -4.2075, -4.1696, -5.1362, -3.2205, -5.3699, -4.3089,\n",
      "         -5.1283, -4.5938, -4.6572, -4.1303, -5.0532, -4.6073, -4.9601, -4.6834]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 41\n",
      "OUTPUTS tensor([[-3.8237, -3.9354, -5.5690, -4.9073, -4.6468, -4.8592, -4.3843, -5.3046,\n",
      "         -5.2636, -4.1412, -5.5385, -5.2194, -5.0478, -4.4199, -3.9786, -4.2180,\n",
      "         -4.5973, -3.8642, -3.9667, -4.8452, -3.9602, -4.9341, -4.8902, -3.7950,\n",
      "         -5.7968, -3.9542, -3.9944, -2.8969, -4.4470, -4.8507, -4.6927, -4.2745,\n",
      "         -4.5220, -4.3303, -5.3412, -4.0272, -4.0601, -4.4459, -5.4284, -4.9178,\n",
      "         -3.5720, -4.8451, -4.0286, -3.9957, -4.3029, -5.4389, -4.7243, -3.9004,\n",
      "         -4.4099, -4.7083, -4.2796, -4.0726, -3.7383, -4.5494, -4.9222, -4.3220,\n",
      "         -4.4937, -4.3063, -4.0160, -4.8721, -4.8504, -3.9895, -4.6618, -4.2189,\n",
      "         -5.3634, -4.7375, -3.7118, -4.5797, -4.6002, -3.5931, -5.6694, -4.5923,\n",
      "         -4.8809, -5.2363, -4.9739, -4.4148, -5.2271, -4.5126, -4.6360, -5.0822]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 42\n",
      "OUTPUTS tensor([[-4.3201, -3.9276, -4.2572, -4.4887, -4.4118, -4.4626, -4.7076, -4.5778,\n",
      "         -4.5618, -4.3208, -4.8826, -4.8927, -4.0916, -4.2141, -4.3669, -4.3049,\n",
      "         -4.9718, -4.4313, -4.1668, -4.2554, -4.0526, -4.6839, -4.6885, -3.9366,\n",
      "         -4.7088, -4.6053, -4.3109, -3.7059, -4.2547, -4.7432, -4.3479, -4.0111,\n",
      "         -4.6833, -4.5206, -4.6451, -4.1844, -4.3043, -4.3094, -4.5774, -4.2638,\n",
      "         -3.5833, -4.1986, -4.5161, -4.4280, -4.3981, -4.9402, -4.4576, -4.0046,\n",
      "         -3.9552, -4.4190, -4.0099, -4.3350, -4.5044, -4.1968, -4.3308, -4.5104,\n",
      "         -4.0486, -4.2098, -4.7957, -4.8863, -4.7869, -3.9898, -4.4847, -4.1243,\n",
      "         -4.8762, -4.6278, -4.5486, -4.5054, -4.3091, -4.3252, -4.9122, -4.5313,\n",
      "         -4.8698, -4.5654, -4.7413, -4.5225, -4.7362, -4.6016, -4.5875, -4.5554]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 43\n",
      "OUTPUTS tensor([[-3.8627, -4.4843, -5.6441, -4.7299, -4.0862, -5.1742, -4.8998, -5.5499,\n",
      "         -5.2188, -4.1211, -5.6757, -5.4796, -4.9158, -4.7067, -4.4641, -4.3541,\n",
      "         -4.5111, -3.7009, -3.9725, -5.2645, -3.6648, -5.0764, -5.1084, -4.2087,\n",
      "         -6.0315, -3.5584, -4.1810, -2.9795, -4.1370, -4.9024, -5.3152, -4.4898,\n",
      "         -4.5901, -4.0650, -5.5370, -4.2134, -3.8321, -4.1454, -5.9049, -4.5238,\n",
      "         -4.3080, -4.9711, -3.6966, -4.3885, -4.0996, -5.6353, -5.2761, -3.4800,\n",
      "         -4.2424, -4.8237, -4.6591, -3.6088, -3.4947, -4.0104, -5.2632, -4.2825,\n",
      "         -5.0682, -4.6541, -3.9681, -5.0220, -5.4857, -3.1316, -4.2783, -4.0017,\n",
      "         -5.6035, -5.0287, -3.8709, -4.5724, -4.9139, -3.6688, -5.8136, -4.3458,\n",
      "         -5.2311, -5.3518, -5.2510, -4.7078, -5.6962, -4.4254, -4.7185, -5.2476]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 44\n",
      "OUTPUTS tensor([[-4.5244, -4.8289, -5.4891, -4.6786, -3.5372, -4.9896, -5.0659, -5.8062,\n",
      "         -5.1395, -4.2711, -5.2552, -5.6849, -4.9183, -5.1687, -5.2420, -4.2361,\n",
      "         -4.3702, -3.6909, -4.1565, -5.6686, -3.6474, -5.0998, -5.1936, -4.6370,\n",
      "         -5.8026, -3.6452, -4.3967, -3.7177, -4.2344, -4.5086, -5.3389, -4.5859,\n",
      "         -4.0980, -3.4917, -5.3425, -4.5874, -3.7031, -3.7546, -5.6595, -4.3482,\n",
      "         -5.0885, -5.1379, -3.5812, -4.3322, -4.0344, -5.5051, -5.6628, -3.5965,\n",
      "         -4.1882, -4.8197, -5.4362, -3.4744, -3.2958, -3.7794, -5.6357, -4.2271,\n",
      "         -5.4736, -5.0989, -3.8722, -4.9436, -5.7171, -2.9658, -3.8842, -4.5402,\n",
      "         -5.3306, -5.2869, -3.9099, -4.0935, -5.7162, -3.5350, -5.6803, -4.0833,\n",
      "         -5.4610, -5.1124, -4.8832, -4.4446, -5.5715, -4.3019, -5.1240, -5.1700]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 45\n",
      "OUTPUTS tensor([[-4.2925, -5.0382, -5.5307, -4.6640, -3.4031, -5.2510, -5.3386, -5.9057,\n",
      "         -5.1991, -4.3344, -5.4947, -5.9109, -4.9212, -5.2125, -5.3128, -4.3453,\n",
      "         -4.5421, -3.7175, -4.1099, -5.8452, -3.5673, -5.3044, -5.4219, -4.6747,\n",
      "         -5.9739, -3.5498, -4.4761, -3.6053, -4.1588, -4.6852, -5.5373, -4.5854,\n",
      "         -4.2858, -3.5599, -5.6098, -4.6148, -3.7316, -3.7376, -5.9741, -4.2632,\n",
      "         -5.1156, -5.1335, -3.5122, -4.5257, -4.0483, -5.7374, -5.8184, -3.4229,\n",
      "         -4.1497, -4.9151, -5.4706, -3.3500, -3.3454, -3.6655, -5.6486, -4.2571,\n",
      "         -5.4384, -5.1177, -3.9816, -5.1459, -5.9465, -2.5461, -3.8626, -4.3327,\n",
      "         -5.5422, -5.4282, -4.1141, -4.2774, -5.6605, -3.6998, -5.9314, -4.1907,\n",
      "         -5.6848, -5.2853, -5.0979, -4.6361, -5.8520, -4.3915, -5.1791, -5.3474]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 46\n",
      "OUTPUTS tensor([[-4.1848, -5.1709, -5.7263, -4.7296, -3.3362, -5.3742, -5.3291, -5.9794,\n",
      "         -5.2781, -4.3724, -5.5765, -5.9074, -5.1023, -5.2811, -5.2351, -4.4401,\n",
      "         -4.4791, -3.7131, -4.1405, -5.9462, -3.6508, -5.3331, -5.4708, -4.7716,\n",
      "         -6.0851, -3.3190, -4.4947, -3.6298, -4.2203, -4.7308, -5.6411, -4.7027,\n",
      "         -4.3140, -3.6116, -5.7273, -4.6588, -3.7686, -3.8151, -6.1063, -4.4032,\n",
      "         -5.2135, -5.2642, -3.5005, -4.5559, -4.0858, -5.7621, -5.8637, -3.3464,\n",
      "         -4.2994, -4.9991, -5.5338, -3.1959, -3.2911, -3.7854, -5.7193, -4.2822,\n",
      "         -5.3551, -5.1609, -3.9260, -5.1400, -5.9353, -2.4255, -3.9094, -4.3565,\n",
      "         -5.5814, -5.4425, -4.0839, -4.3621, -5.6337, -3.7380, -5.9956, -4.0570,\n",
      "         -5.6704, -5.3882, -5.1552, -4.6943, -5.9193, -4.4082, -5.1736, -5.4432]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 47\n",
      "OUTPUTS tensor([[-4.3575, -5.2267, -5.7171, -4.7336, -3.1970, -5.3122, -5.3014, -5.9797,\n",
      "         -5.2441, -4.4005, -5.4486, -5.8546, -5.1226, -5.3497, -5.3335, -4.4507,\n",
      "         -4.4067, -3.7424, -4.2293, -5.9706, -3.7241, -5.2783, -5.4256, -4.8918,\n",
      "         -6.0076, -3.2561, -4.5522, -3.8410, -4.3023, -4.6460, -5.6198, -4.7763,\n",
      "         -4.2289, -3.5445, -5.6340, -4.7537, -3.7814, -3.7970, -6.0065, -4.4225,\n",
      "         -5.3936, -5.3211, -3.5308, -4.5577, -4.1027, -5.6681, -5.8946, -3.3693,\n",
      "         -4.3580, -4.9897, -5.6593, -3.1259, -3.3178, -3.8137, -5.7874, -4.2989,\n",
      "         -5.2908, -5.2507, -3.9225, -5.0759, -5.9135, -2.4417, -3.8738, -4.5309,\n",
      "         -5.4871, -5.4484, -4.0818, -4.2832, -5.7644, -3.7279, -5.8828, -3.8444,\n",
      "         -5.4440, -5.3121, -5.0710, -4.6547, -5.8428, -4.3854, -5.2083, -5.3943]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 48\n",
      "OUTPUTS tensor([[-4.5133, -5.2339, -5.7452, -4.7793, -3.0841, -5.2708, -5.2790, -6.0122,\n",
      "         -5.2636, -4.4346, -5.3910, -5.8412, -5.1656, -5.4105, -5.4014, -4.4648,\n",
      "         -4.3805, -3.7759, -4.3063, -5.9903, -3.7935, -5.2605, -5.4045, -4.9689,\n",
      "         -5.9910, -3.2002, -4.5963, -3.9773, -4.3888, -4.6113, -5.6078, -4.8416,\n",
      "         -4.1845, -3.5221, -5.5834, -4.8240, -3.8020, -3.8141, -5.9450, -4.4778,\n",
      "         -5.4857, -5.3887, -3.5844, -4.5550, -3.9562, -5.6310, -5.9234, -3.3551,\n",
      "         -4.4177, -5.0084, -5.7420, -3.0378, -3.3366, -3.8698, -5.8619, -4.3361,\n",
      "         -5.1941, -5.3257, -3.9324, -5.0540, -5.9085, -2.4502, -3.8876, -4.6869,\n",
      "         -5.4533, -5.4713, -4.0697, -4.2407, -5.8843, -3.7157, -5.8378, -3.6803,\n",
      "         -5.2662, -5.2877, -5.0340, -4.6361, -5.8024, -4.3929, -5.2590, -5.3835]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS tensor([[-4.3304, -4.3700, -4.4356, -4.3915, -4.3406, -4.4477, -4.4351, -4.4232,\n",
      "         -4.3095, -4.3220, -4.4357, -4.4252, -4.3536, -4.3563, -4.2942, -4.4579,\n",
      "         -4.4023, -4.4226, -4.3614, -4.3753, -4.4010, -4.3083, -4.4713, -4.3521,\n",
      "         -4.4208, -4.3945, -4.3965, -4.3233, -4.3782, -4.4079, -4.3601, -4.3059,\n",
      "         -4.3777, -4.3320, -4.4500, -4.4380, -4.3216, -4.2997, -4.4018, -4.4601,\n",
      "         -4.3279, -4.4274, -4.4352, -4.4101, -4.3902, -4.4142, -4.4275, -4.3619,\n",
      "         -4.3402, -4.3253, -4.3238, -4.3699, -4.3402, -4.4515, -4.4040, -4.4340,\n",
      "         -4.3021, -4.3561, -4.4073, -4.4439, -4.4341, -4.3374, -4.3086, -4.3767,\n",
      "         -4.3933, -4.4237, -4.4330, -4.3812, -4.3244, -4.3441, -4.4107, -4.3070,\n",
      "         -4.4302, -4.3172, -4.4159, -4.4038, -4.4352, -4.3999, -4.3961, -4.3972]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "[1,    50] loss: 0.022\n",
      "BATCH 50\n",
      "OUTPUTS tensor([[-4.2432, -5.2474, -5.7602, -4.8397, -2.9737, -5.3069, -5.3144, -6.0171,\n",
      "         -5.2941, -4.4992, -5.4251, -5.8558, -5.1976, -5.4242, -5.3956, -4.5390,\n",
      "         -4.4554, -3.8757, -4.3786, -5.9854, -3.8909, -5.2911, -5.4385, -5.0003,\n",
      "         -5.9997, -3.1069, -4.6622, -4.0488, -4.4534, -4.6781, -5.6048, -4.8864,\n",
      "         -4.2653, -3.6187, -5.6077, -4.8794, -3.8881, -3.8979, -5.9486, -4.5495,\n",
      "         -5.4620, -5.4218, -3.6926, -4.6243, -3.7208, -5.6525, -5.9228, -3.2894,\n",
      "         -4.4802, -5.0469, -5.7272, -2.9181, -3.4415, -3.9637, -5.8664, -4.4171,\n",
      "         -4.9273, -5.3476, -4.0162, -5.1058, -5.9086, -2.3479, -3.9678, -4.7366,\n",
      "         -5.4856, -5.5009, -4.1585, -4.3192, -5.8634, -3.8108, -5.8539, -3.4741,\n",
      "         -4.9833, -5.3158, -5.0851, -4.7029, -5.8187, -4.4683, -5.2916, -5.4176]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 51\n",
      "OUTPUTS tensor([[-4.1153, -5.1852, -5.6770, -4.8231, -2.9833, -5.2579, -5.2632, -5.9146,\n",
      "         -5.2355, -4.4973, -5.3694, -5.7674, -5.1466, -5.2006, -5.3132, -4.5469,\n",
      "         -4.4669, -3.9288, -4.3888, -5.8776, -3.9407, -5.2324, -5.3826, -4.9577,\n",
      "         -5.9013, -3.1201, -4.6566, -4.0717, -4.4571, -4.6743, -5.5167, -4.8540,\n",
      "         -4.2880, -3.6795, -5.5382, -4.8594, -3.9289, -3.9364, -5.8495, -4.5569,\n",
      "         -5.3682, -5.3630, -3.7599, -4.6232, -3.6386, -5.5773, -5.8227, -3.3029,\n",
      "         -4.4795, -5.0042, -5.6242, -2.9277, -3.5152, -4.0115, -5.7701, -4.4337,\n",
      "         -4.7611, -5.2832, -4.0538, -5.0746, -5.8103, -2.3948, -4.0017, -4.7176,\n",
      "         -5.4232, -5.4368, -4.1923, -4.3383, -5.7513, -3.8600, -5.7661, -3.4128,\n",
      "         -4.8147, -5.2556, -5.0533, -4.6968, -5.7336, -4.4787, -5.2379, -5.3592]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 52\n",
      "OUTPUTS tensor([[-4.0069, -5.0866, -5.5455, -4.7803, -3.0367, -5.1725, -5.1749, -5.7567,\n",
      "         -5.1369, -4.4757, -5.2744, -5.6279, -5.0594, -4.9700, -5.1896, -4.5377,\n",
      "         -4.4629, -3.9813, -4.3835, -5.7145, -3.9888, -5.1338, -5.2873, -4.8835,\n",
      "         -5.7481, -3.1733, -4.6291, -4.0863, -4.4441, -4.6494, -5.3835, -4.7913,\n",
      "         -4.2996, -3.7445, -5.4248, -4.8130, -3.9664, -3.9709, -5.6969, -4.5476,\n",
      "         -5.2329, -5.2636, -3.8318, -4.6015, -3.5901, -5.4561, -5.6700, -3.3478,\n",
      "         -4.4603, -4.9282, -5.4718, -2.9842, -3.5974, -4.0576, -5.6218, -4.4357,\n",
      "         -4.5920, -5.1785, -4.0868, -5.0119, -5.6603, -2.5036, -4.0304, -4.6744,\n",
      "         -5.3183, -5.3303, -4.2184, -4.3449, -5.5865, -3.9075, -5.4764, -3.3872,\n",
      "         -4.6471, -5.1554, -4.9897, -4.6681, -5.5986, -4.4728, -5.1464, -5.2599]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 53\n",
      "OUTPUTS tensor([[-3.9301, -4.9734, -5.2627, -4.7254, -3.1254, -5.0710, -5.0701, -5.5743,\n",
      "         -5.0202, -4.4450, -5.1615, -5.4657, -4.9561, -4.7608, -5.0498, -4.5204,\n",
      "         -4.4517, -4.0332, -4.3711, -5.5277, -4.0360, -5.0171, -5.1743, -4.7963,\n",
      "         -5.5703, -3.2584, -4.5917, -4.0981, -4.4235, -4.6146, -5.2317, -4.7154,\n",
      "         -4.3059, -3.8111, -5.2919, -4.7549, -4.0021, -4.0035, -5.5208, -4.5308,\n",
      "         -5.0827, -5.1460, -3.9053, -4.5703, -3.5789, -5.3139, -5.4950, -3.4181,\n",
      "         -4.4329, -4.8372, -5.2994, -3.0783, -3.6832, -4.1029, -5.4511, -4.4308,\n",
      "         -4.4422, -5.0565, -4.1176, -4.9347, -5.4882, -2.6567, -4.0566, -4.6206,\n",
      "         -5.1942, -5.2049, -4.2408, -4.3457, -5.4003, -3.9539, -5.2035, -3.3984,\n",
      "         -4.5018, -5.0371, -4.9117, -4.6290, -5.4413, -4.4594, -5.0383, -5.1422]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 54\n",
      "OUTPUTS tensor([[-3.8916, -4.8559, -5.0129, -4.6660, -3.2484, -4.9628, -4.9588, -5.3808,\n",
      "         -4.8953, -4.4115, -5.0405, -5.2928, -4.8462, -4.5853, -4.9055, -4.5011,\n",
      "         -4.4384, -4.0870, -4.3573, -5.3309, -4.0853, -4.8924, -5.0537, -4.7053,\n",
      "         -5.3807, -3.3747, -4.5513, -4.1122, -4.4012, -4.4644, -5.0731, -4.6346,\n",
      "         -4.3112, -3.8808, -5.1501, -4.6932, -4.0392, -4.0373, -5.3339, -4.5122,\n",
      "         -4.9302, -5.0209, -3.9821, -4.5361, -3.6088, -5.1619, -5.3107, -3.5137,\n",
      "         -4.4033, -4.7403, -5.1202, -3.2090, -3.7733, -4.1503, -5.2709, -4.4242,\n",
      "         -4.3223, -4.9278, -4.1493, -4.8514, -5.3067, -2.8500, -4.0835, -4.5640,\n",
      "         -5.0611, -5.0715, -4.2634, -4.3453, -5.2063, -4.0019, -4.9625, -3.4495,\n",
      "         -4.3888, -4.9107, -4.8275, -4.5862, -5.2738, -4.4438, -4.9234, -5.0164]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 55\n",
      "OUTPUTS tensor([[-3.9040, -4.7236, -4.7874, -4.5958, -3.4305, -4.8363, -4.8292, -5.1551,\n",
      "         -4.7487, -4.3728, -4.8975, -5.0901, -4.7185, -4.4420, -4.7435, -4.4786,\n",
      "         -4.4218, -4.1502, -4.3419, -5.1032, -4.1437, -4.7461, -4.9124, -4.6031,\n",
      "         -5.1577, -3.5450, -4.5045, -4.1339, -4.3761, -4.3571, -4.8921, -4.5416,\n",
      "         -4.3166, -3.9627, -4.9838, -4.6224, -4.0833, -4.0773, -5.1152, -4.4903,\n",
      "         -4.7636, -4.8753, -4.0720, -4.4962, -3.6993, -4.9830, -5.0980, -3.6533,\n",
      "         -4.3698, -4.5458, -4.9174, -3.4029, -3.8795, -4.2062, -5.0624, -4.4159,\n",
      "         -4.2353, -4.7799, -4.1869, -4.7526, -5.0968, -3.1171, -4.1153, -4.5004,\n",
      "         -4.9039, -4.9157, -4.2899, -4.3442, -4.9854, -4.0587, -4.7450, -3.5616,\n",
      "         -4.3124, -4.7623, -4.7278, -4.5355, -5.0772, -4.4248, -4.7900, -4.8686]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 56\n",
      "OUTPUTS tensor([[-3.9633, -4.5484, -4.6166, -4.5332, -3.6450, -4.7181, -4.7084, -4.9385,\n",
      "         -4.6097, -4.3422, -4.7618, -4.8957, -4.5993, -4.3499, -4.5946, -4.4642,\n",
      "         -4.4122, -4.2211, -4.3349, -4.8859, -4.2104, -4.6075, -4.7791, -4.5119,\n",
      "         -4.9426, -3.7451, -4.4663, -4.1671, -4.3593, -4.3048, -4.7224, -4.4577,\n",
      "         -4.3291, -4.0518, -4.8254, -4.5610, -4.1349, -4.1247, -4.9050, -4.4764,\n",
      "         -4.6138, -4.7386, -4.1696, -4.4644, -3.8350, -4.8114, -4.8961, -3.8183,\n",
      "         -4.3447, -4.4087, -4.7277, -3.6312, -3.9933, -4.2702, -4.8640, -4.4153,\n",
      "         -4.1934, -4.6420, -4.2318, -4.6612, -4.8969, -3.4179, -4.1544, -4.4464,\n",
      "         -4.7540, -4.7687, -4.3242, -4.3504, -4.7772, -4.1232, -4.5808, -3.7165,\n",
      "         -4.2827, -4.6216, -4.6354, -4.4924, -4.8889, -4.4132, -4.6654, -4.7288]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 57\n",
      "OUTPUTS tensor([[-3.9893, -4.4683, -4.5389, -4.5150, -3.7423, -4.6751, -4.6639, -4.8523,\n",
      "         -4.5562, -4.3354, -4.7120, -4.8200, -4.5540, -4.3080, -4.5353, -4.4661,\n",
      "         -4.4174, -4.2608, -4.3393, -4.7991, -4.2482, -4.5541, -4.7294, -4.4780,\n",
      "         -4.8578, -3.8365, -4.4580, -4.1875, -4.3585, -4.2796, -4.6546, -4.4275,\n",
      "         -4.3437, -4.0977, -4.7652, -4.5422, -4.1644, -4.1524, -4.8221, -4.4794,\n",
      "         -4.5531, -4.6869, -4.2212, -4.4589, -3.8951, -4.7447, -4.8156, -3.8949,\n",
      "         -4.3403, -4.3457, -4.6506, -3.7343, -4.0499, -4.3071, -4.7852, -4.4238,\n",
      "         -4.1746, -4.5882, -4.2581, -4.6307, -4.8174, -3.5450, -4.1785, -4.4290,\n",
      "         -4.6979, -4.7131, -4.3479, -4.3624, -4.6923, -4.1581, -4.5063, -3.7856,\n",
      "         -4.2691, -4.5673, -4.6044, -4.4824, -4.8157, -4.4173, -4.6185, -4.6767]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 58\n",
      "OUTPUTS tensor([[-3.9878, -4.4236, -4.4963, -4.5151, -3.7722, -4.6660, -4.6538, -4.8281,\n",
      "         -4.5419, -4.3365, -4.7018, -4.8004, -4.5420, -4.2799, -4.5163, -4.4719,\n",
      "         -4.4259, -4.2804, -4.3451, -4.7741, -4.2670, -4.5398, -4.7181, -4.4683,\n",
      "         -4.8354, -3.8651, -4.4605, -4.1964, -4.3615, -4.2564, -4.6338, -4.4202,\n",
      "         -4.3553, -4.1177, -4.7507, -4.5406, -4.1783, -4.1658, -4.7997, -4.4865,\n",
      "         -4.5317, -4.6741, -4.2452, -4.4625, -3.9012, -4.7273, -4.7923, -3.9196,\n",
      "         -4.3421, -4.3080, -4.6256, -3.7656, -4.0733, -4.3259, -4.7624, -4.4328,\n",
      "         -4.1570, -4.5725, -4.2708, -4.6270, -4.7945, -3.5820, -4.1906, -4.4256,\n",
      "         -4.6854, -4.6996, -4.3619, -4.3731, -4.6649, -4.1737, -4.4653, -3.8014,\n",
      "         -4.2529, -4.5528, -4.6005, -4.4850, -4.7970, -4.4252, -4.6065, -4.6650]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS tensor([[-4.0161, -4.3812, -4.4544, -4.5036, -3.8461, -4.6372, -4.6240, -4.7699,\n",
      "         -4.5047, -4.3317, -4.6684, -4.7495, -4.5108, -4.2619, -4.4766, -4.4739,\n",
      "         -4.4303, -4.3087, -4.3486, -4.7157, -4.2942, -4.5027, -4.6847, -4.4455,\n",
      "         -4.7781, -3.9341, -4.4558, -4.2113, -4.3608, -4.2511, -4.5877, -4.3994,\n",
      "         -4.3661, -4.1491, -4.7101, -4.5287, -4.1985, -4.1847, -4.7437, -4.4895,\n",
      "         -4.4919, -4.6394, -4.2817, -4.4596, -3.9489, -4.6816, -4.7381, -3.9772,\n",
      "         -4.3390, -4.2769, -4.5394, -3.8441, -4.1122, -4.3524, -4.7092, -4.4395,\n",
      "         -4.1536, -4.5357, -4.2886, -4.6069, -4.7408, -3.6791, -4.2069, -4.4138,\n",
      "         -4.6477, -4.6624, -4.3792, -4.3822, -4.6074, -4.1976, -4.4252, -3.8585,\n",
      "         -4.2534, -4.5151, -4.5800, -4.4790, -4.7477, -4.4289, -4.5746, -4.6300]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "[1,    60] loss: 0.024\n",
      "BATCH 60\n",
      "OUTPUTS tensor([[-4.0623, -4.3504, -4.4229, -4.4879, -3.9395, -4.6016, -4.5876, -4.6999,\n",
      "         -4.4592, -4.3248, -4.6266, -4.6874, -4.4729, -4.2567, -4.4307, -4.4746,\n",
      "         -4.4034, -4.3400, -4.3514, -4.6461, -4.3244, -4.4575, -4.6435, -4.4190,\n",
      "         -4.7084, -4.0210, -4.4487, -4.2294, -4.3590, -4.2614, -4.5335, -4.3743,\n",
      "         -4.3765, -4.1845, -4.6603, -4.5138, -4.2208, -4.2054, -4.6758, -4.4912,\n",
      "         -4.4480, -4.5973, -4.3225, -4.4545, -4.0191, -4.6258, -4.6738, -4.0492,\n",
      "         -4.3343, -4.2586, -4.4611, -3.9436, -4.1565, -4.3816, -4.6458, -4.4454,\n",
      "         -4.1630, -4.4921, -4.3081, -4.5809, -4.6766, -3.8029, -4.2244, -4.3997,\n",
      "         -4.6008, -4.6170, -4.3976, -4.3906, -4.5402, -4.2245, -4.3954, -3.9363,\n",
      "         -4.2679, -4.4690, -4.5536, -4.4698, -4.6876, -4.4311, -4.5360, -4.5869]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 61\n",
      "OUTPUTS tensor([[-4.1329, -4.3330, -4.4036, -4.4672, -4.0611, -4.5566, -4.5421, -4.6127,\n",
      "         -4.4023, -4.3162, -4.5730, -4.6094, -4.4259, -4.2673, -4.3761, -4.4747,\n",
      "         -4.3926, -4.3773, -4.3545, -4.5602, -4.3605, -4.4009, -4.5917, -4.3876,\n",
      "         -4.6206, -4.1339, -4.4394, -4.2532, -4.3567, -4.2919, -4.4675, -4.3437,\n",
      "         -4.3876, -4.2271, -4.5974, -4.4951, -4.2477, -4.2302, -4.5907, -4.4920,\n",
      "         -4.3831, -4.5449, -4.3713, -4.4473, -4.1201, -4.5554, -4.5945, -4.1426,\n",
      "         -4.3285, -4.2555, -4.3894, -4.0732, -4.2103, -4.4164, -4.5676, -4.4515,\n",
      "         -4.1892, -4.4389, -4.3316, -4.5471, -4.5973, -3.9646, -4.2452, -4.3830,\n",
      "         -4.5411, -4.5602, -4.4191, -4.3994, -4.4589, -4.2571, -4.3775, -4.0434,\n",
      "         -4.3008, -4.4113, -4.5193, -4.4573, -4.6121, -4.4323, -4.4881, -4.5325]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 62\n",
      "OUTPUTS tensor([[-4.1412, -4.3228, -4.3933, -4.4677, -4.0812, -4.5523, -4.5372, -4.6009,\n",
      "         -4.3934, -4.3153, -4.5681, -4.6001, -4.4184, -4.2635, -4.3664, -4.4776,\n",
      "         -4.3824, -4.3873, -4.3569, -4.5483, -4.3704, -4.3920, -4.5860, -4.3822,\n",
      "         -4.6098, -4.1527, -4.4411, -4.2575, -4.3569, -4.2912, -4.4563, -4.3389,\n",
      "         -4.3936, -4.2356, -4.5904, -4.4948, -4.2534, -4.2355, -4.5798, -4.4961,\n",
      "         -4.3623, -4.5386, -4.3836, -4.4493, -4.1329, -4.5458, -4.5833, -4.1583,\n",
      "         -4.3281, -4.2481, -4.3677, -4.0947, -4.2207, -4.4262, -4.5564, -4.4562,\n",
      "         -4.1891, -4.4300, -4.3364, -4.5456, -4.5859, -3.9905, -4.2498, -4.3803,\n",
      "         -4.5351, -4.5539, -4.4263, -4.4051, -4.4448, -4.2636, -4.3677, -4.0572,\n",
      "         -4.3017, -4.4022, -4.5177, -4.4588, -4.6031, -4.4366, -4.4815, -4.5269]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 63\n",
      "OUTPUTS tensor([[-4.1753, -4.3187, -4.3880, -4.4607, -4.1369, -4.5341, -4.5187, -4.5640,\n",
      "         -4.3680, -4.3115, -4.5465, -4.5677, -4.3976, -4.2713, -4.3425, -4.4792,\n",
      "         -4.3811, -4.4056, -4.3593, -4.5120, -4.3884, -4.3667, -4.5647, -4.3685,\n",
      "         -4.5729, -4.2043, -4.4388, -4.2685, -4.3558, -4.3079, -4.4272, -4.3254,\n",
      "         -4.4005, -4.2545, -4.5644, -4.4884, -4.2654, -4.2465, -4.5440, -4.4986,\n",
      "         -4.3377, -4.5170, -4.4071, -4.4479, -4.1801, -4.5155, -4.5497, -4.2009,\n",
      "         -4.3255, -4.2502, -4.3400, -4.1540, -4.2446, -4.4437, -4.5232, -4.4609,\n",
      "         -4.2035, -4.3954, -4.3467, -4.5329, -4.5520, -4.0641, -4.2589, -4.3730,\n",
      "         -4.5108, -4.5307, -4.4377, -4.4111, -4.4089, -4.2781, -4.3630, -4.1055,\n",
      "         -4.3191, -4.3764, -4.5047, -4.4552, -4.5717, -4.4393, -4.4612, -4.5048]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 64\n",
      "OUTPUTS tensor([[-4.3022, -4.3388, -4.4031, -4.4316, -4.3197, -4.4717, -4.4563, -4.4425,\n",
      "         -4.2888, -4.3014, -4.4700, -4.4576, -4.3337, -4.3231, -4.2727, -4.4802,\n",
      "         -4.4131, -4.4572, -4.3653, -4.3937, -4.4389, -4.2881, -4.4924, -4.3301,\n",
      "         -4.4483, -4.3734, -4.4272, -4.3069, -4.3530, -4.3878, -4.3391, -4.2857,\n",
      "         -4.4150, -4.3138, -4.4757, -4.4647, -4.3038, -4.2817, -4.4242, -4.4999,\n",
      "         -4.3021, -4.4455, -4.4742, -4.4388, -4.3500, -4.4160, -4.4415, -4.3411,\n",
      "         -4.3196, -4.2878, -4.2942, -4.3488, -4.3196, -4.4918, -4.4161, -4.4695,\n",
      "         -4.2716, -4.3325, -4.3803, -4.4848, -4.4430, -4.3067, -4.2884, -4.3535,\n",
      "         -4.4257, -4.4521, -4.4677, -4.4225, -4.3005, -4.3246, -4.3791, -4.2768,\n",
      "         -4.3970, -4.2959, -4.4557, -4.4377, -4.4651, -4.4404, -4.3961, -4.4283]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 65\n",
      "OUTPUTS tensor([[-4.2519, -4.3245, -4.3908, -4.4459, -4.2513, -4.4971, -4.4811, -4.4892,\n",
      "         -4.3157, -4.3031, -4.5018, -4.5015, -4.3549, -4.2986, -4.2955, -4.4817,\n",
      "         -4.3934, -4.4413, -4.3635, -4.4390, -4.4234, -4.3148, -4.5213, -4.3417,\n",
      "         -4.4974, -4.3083, -4.4340, -4.2912, -4.3456, -4.3528, -4.3688, -4.2981,\n",
      "         -4.4132, -4.2911, -4.5112, -4.4755, -4.2885, -4.2676, -4.4709, -4.5029,\n",
      "         -4.3062, -4.4732, -4.4529, -4.4445, -4.2833, -4.4534, -4.4823, -4.2884,\n",
      "         -4.3199, -4.2674, -4.3022, -4.2762, -4.2912, -4.4776, -4.4563, -4.4694,\n",
      "         -4.2420, -4.3455, -4.3663, -4.5063, -4.4836, -4.2157, -4.2762, -4.3585,\n",
      "         -4.4605, -4.4837, -4.4597, -4.4220, -4.3372, -4.3062, -4.3667, -4.2097,\n",
      "         -4.3637, -4.3232, -4.4775, -4.4472, -4.5075, -4.4436, -4.4198, -4.4595]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 66\n",
      "OUTPUTS tensor([[-4.2956, -4.3338, -4.3982, -4.4379, -4.3124, -4.4774, -4.4614, -4.4497,\n",
      "         -4.2880, -4.2975, -4.4779, -4.4664, -4.3325, -4.3179, -4.2715, -4.4829,\n",
      "         -4.4065, -4.4595, -4.3656, -4.4008, -4.4415, -4.2873, -4.4983, -4.3282,\n",
      "         -4.4573, -4.3632, -4.4314, -4.3035, -4.3467, -4.3814, -4.3384, -4.2840,\n",
      "         -4.4194, -4.3099, -4.4830, -4.4689, -4.3003, -4.2784, -4.4322, -4.5049,\n",
      "         -4.2975, -4.4503, -4.4763, -4.4426, -4.3409, -4.4203, -4.4472, -4.3350,\n",
      "         -4.3169, -4.2823, -4.2901, -4.3415, -4.3152, -4.4950, -4.4213, -4.4736,\n",
      "         -4.2663, -4.3279, -4.3764, -4.4920, -4.4478, -4.2967, -4.2849, -4.3511,\n",
      "         -4.4337, -4.4589, -4.4709, -4.4274, -4.3001, -4.3207, -4.3744, -4.2674,\n",
      "         -4.3914, -4.2951, -4.4629, -4.4427, -4.4734, -4.4456, -4.3982, -4.4355]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 67\n",
      "OUTPUTS tensor([[-4.2461, -4.3175, -4.3841, -4.4526, -4.2468, -4.5028, -4.4862, -4.4957,\n",
      "         -4.3147, -4.2969, -4.5096, -4.5097, -4.3536, -4.2924, -4.2938, -4.4851,\n",
      "         -4.3850, -4.4450, -4.3646, -4.4453, -4.4274, -4.3138, -4.5270, -4.3398,\n",
      "         -4.5058, -4.3011, -4.4387, -4.2888, -4.3371, -4.3460, -4.3675, -4.2965,\n",
      "         -4.4186, -4.2890, -4.5182, -4.4799, -4.2865, -4.2656, -4.4783, -4.5087,\n",
      "         -4.2980, -4.4779, -4.4567, -4.4490, -4.2758, -4.4574, -4.4872, -4.2847,\n",
      "         -4.3179, -4.2605, -4.2944, -4.2718, -4.2888, -4.4821, -4.4608, -4.4743,\n",
      "         -4.2364, -4.3367, -4.3639, -4.5137, -4.4878, -4.2094, -4.2740, -4.3564,\n",
      "         -4.4683, -4.4901, -4.4639, -4.4278, -4.3358, -4.3038, -4.3602, -4.2012,\n",
      "         -4.3580, -4.3223, -4.4848, -4.4527, -4.5153, -4.4496, -4.4218, -4.4665]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 68\n",
      "OUTPUTS tensor([[-4.1237, -4.2795, -4.3519, -4.4916, -4.0843, -4.5705, -4.5528, -4.6167,\n",
      "         -4.3915, -4.3018, -4.5926, -4.6226, -4.4158, -4.2311, -4.3585, -4.4942,\n",
      "         -4.3345, -4.4105, -4.3670, -4.5623, -4.3938, -4.3900, -4.6035, -4.3770,\n",
      "         -4.6322, -4.1501, -4.4599, -4.2581, -4.3176, -4.2594, -4.4501, -4.3362,\n",
      "         -4.4185, -4.2428, -4.6109, -4.5109, -4.2582, -4.2404, -4.5993, -4.5199,\n",
      "         -4.3028, -4.5519, -4.4089, -4.4679, -4.1145, -4.5575, -4.5937, -4.1616,\n",
      "         -4.3279, -4.2085, -4.3088, -4.0984, -4.2285, -4.4513, -4.5661, -4.4783,\n",
      "         -4.1640, -4.3625, -4.3396, -4.5705, -4.5948, -3.9915, -4.2537, -4.3775,\n",
      "         -4.5587, -4.5725, -4.4486, -4.4306, -4.4357, -4.2680, -4.3277, -4.0381,\n",
      "         -4.2767, -4.4003, -4.5424, -4.4804, -4.6246, -4.4613, -4.4878, -4.5479]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS tensor([[-4.0549, -4.2562, -4.3320, -4.5194, -3.9950, -4.6144, -4.5958, -4.6913,\n",
      "         -4.4393, -4.3048, -4.6460, -4.6930, -4.4551, -4.1956, -4.3986, -4.5043,\n",
      "         -4.3037, -4.3961, -4.3728, -4.6344, -4.3798, -4.4375, -4.6330, -4.4015,\n",
      "         -4.7106, -4.0670, -4.4770, -4.2440, -4.3045, -4.2096, -4.5007, -4.3625,\n",
      "         -4.4239, -4.2208, -4.6696, -4.5335, -4.2462, -4.2301, -4.6742, -4.5317,\n",
      "         -4.3022, -4.5991, -4.3870, -4.4837, -4.0237, -4.6199, -4.6594, -4.0949,\n",
      "         -4.3374, -4.1776, -4.3136, -4.0029, -4.1985, -4.4391, -4.6310, -4.4859,\n",
      "         -4.1229, -4.3732, -4.3298, -4.6087, -4.6608, -3.8703, -4.2461, -4.3930,\n",
      "         -4.6163, -4.6249, -4.4451, -4.4378, -4.4960, -4.2515, -4.3078, -3.9458,\n",
      "         -4.2306, -4.4489, -4.5810, -4.5015, -4.6929, -4.4736, -4.5299, -4.6000]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "[1,    70] loss: 0.022\n",
      "BATCH 70\n",
      "OUTPUTS tensor([[-3.9979, -4.2358, -4.3146, -4.5461, -3.9221, -4.6547, -4.6353, -4.7578,\n",
      "         -4.4825, -4.3079, -4.6945, -4.7560, -4.4909, -4.1656, -4.4346, -4.4967,\n",
      "         -4.2771, -4.3871, -4.3805, -4.6986, -4.3711, -4.4804, -4.6531, -4.4244,\n",
      "         -4.7805, -3.9993, -4.4944, -4.2344, -4.2927, -4.1678, -4.5460, -4.3871,\n",
      "         -4.4318, -4.2051, -4.7226, -4.5554, -4.2388, -4.2241, -4.7411, -4.5448,\n",
      "         -4.3000, -4.6419, -4.3718, -4.5001, -3.9483, -4.6760, -4.7179, -4.0412,\n",
      "         -4.3479, -4.1511, -4.3160, -3.9248, -4.1760, -4.4320, -4.6890, -4.4954,\n",
      "         -4.0887, -4.3801, -4.3242, -4.6443, -4.7196, -3.7708, -4.2425, -4.4085,\n",
      "         -4.6684, -4.6723, -4.4452, -4.4470, -4.5494, -4.2404, -4.2906, -3.8692,\n",
      "         -4.1921, -4.4928, -4.6169, -4.5224, -4.7540, -4.4870, -4.5682, -4.6473]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 71\n",
      "OUTPUTS tensor([[-3.9378, -4.2096, -4.2967, -4.5772, -3.8455, -4.7008, -4.6804, -4.8325,\n",
      "         -4.5322, -4.3131, -4.7496, -4.8268, -4.5324, -4.1343, -4.4762, -4.4884,\n",
      "         -4.2492, -4.3794, -4.3911, -4.7707, -4.3638, -4.5298, -4.6743, -4.4516,\n",
      "         -4.8591, -3.9285, -4.5154, -4.2263, -4.2810, -4.1237, -4.5977, -4.4164,\n",
      "         -4.4425, -4.1906, -4.7825, -4.5812, -4.2333, -4.2202, -4.8163, -4.5611,\n",
      "         -4.2981, -4.6908, -4.3575, -4.5200, -3.8687, -4.7396, -4.7839, -3.9853,\n",
      "         -4.3618, -4.1233, -4.3189, -3.8426, -4.1541, -4.4264, -4.7543, -4.5078,\n",
      "         -4.0529, -4.3880, -4.3208, -4.6851, -4.7861, -3.6655, -4.2411, -4.4277,\n",
      "         -4.7273, -4.7261, -4.4475, -4.4592, -4.6100, -4.2307, -4.2727, -3.7885,\n",
      "         -4.1519, -4.5433, -4.6581, -4.5473, -4.8229, -4.5038, -4.6124, -4.7010]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 72\n",
      "OUTPUTS tensor([[-3.8653, -4.1799, -4.2770, -4.6190, -3.7527, -4.7618, -4.7402, -4.9299,\n",
      "         -4.5990, -4.3234, -4.8219, -4.9192, -4.5886, -4.0978, -4.5325, -4.4812,\n",
      "         -4.2173, -4.3727, -4.4077, -4.8648, -4.3574, -4.5961, -4.7042, -4.4897,\n",
      "         -4.9613, -3.8432, -4.5447, -4.2196, -4.2699, -4.0714, -4.6670, -4.4570,\n",
      "         -4.4587, -4.1564, -4.8611, -4.6167, -4.2299, -4.2189, -4.9143, -4.5844,\n",
      "         -4.2987, -4.7554, -4.3422, -4.5479, -3.7720, -4.8234, -4.8704, -3.9185,\n",
      "         -4.3830, -4.0914, -4.3254, -3.7426, -4.1302, -4.4220, -4.8401, -4.5262,\n",
      "         -4.0110, -4.4010, -4.3200, -4.7392, -4.8733, -3.5367, -4.2430, -4.4556,\n",
      "         -4.8046, -4.7968, -4.4531, -4.4772, -4.6906, -4.2222, -4.2532, -3.6911,\n",
      "         -4.1043, -4.6111, -4.7128, -4.5815, -4.9128, -4.5276, -4.6714, -4.7716]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 73\n",
      "OUTPUTS tensor([[-3.7806, -4.1476, -4.2569, -4.6754, -3.6431, -4.8422, -4.8192, -5.0557,\n",
      "         -4.6876, -4.3414, -4.9163, -5.0385, -4.6641, -4.0570, -4.6077, -4.4766,\n",
      "         -4.1687, -4.3688, -4.4333, -4.9865, -4.3540, -4.6842, -4.7449, -4.5422,\n",
      "         -5.0929, -3.7434, -4.5855, -4.2163, -4.2609, -4.0113, -4.7587, -4.5128,\n",
      "         -4.4833, -4.1186, -4.9634, -4.6652, -4.2310, -4.2226, -5.0409, -4.6176,\n",
      "         -4.3037, -4.8405, -4.3279, -4.5872, -3.6575, -4.9327, -4.9828, -3.8411,\n",
      "         -4.4148, -4.0562, -4.3374, -3.6242, -4.1059, -4.4212, -4.9515, -4.5536,\n",
      "         -3.9637, -4.4213, -4.3245, -4.8109, -4.9867, -3.3829, -4.2508, -4.4956,\n",
      "         -4.9052, -4.8892, -4.4646, -4.5040, -4.7964, -4.2169, -4.2332, -3.5762,\n",
      "         -4.0499, -4.7010, -4.7851, -4.6284, -5.0292, -4.5616, -4.7498, -4.8638]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 74\n",
      "OUTPUTS tensor([[-3.7097, -4.1197, -4.2408, -4.7315, -3.5522, -4.9200, -4.8956, -5.1741,\n",
      "         -4.7727, -4.3616, -5.0068, -5.1512, -4.7372, -4.0236, -4.6801, -4.4717,\n",
      "         -4.1242, -4.3715, -4.4617, -5.1011, -4.3569, -4.7688, -4.7791, -4.5943,\n",
      "         -5.2169, -3.6610, -4.6276, -4.2187, -4.2557, -3.9612, -4.8461, -4.5681,\n",
      "         -4.5113, -4.0847, -5.0609, -4.7140, -4.2379, -4.2318, -5.1602, -4.6531,\n",
      "         -4.3091, -4.9221, -4.3213, -4.6279, -3.5610, -5.0366, -5.0888, -3.7786,\n",
      "         -4.4487, -4.0274, -4.3487, -3.5255, -4.0904, -4.3931, -5.0568, -4.5838,\n",
      "         -3.9249, -4.4400, -4.3345, -4.8808, -5.0938, -3.2535, -4.2637, -4.5366,\n",
      "         -5.0012, -4.9776, -4.4808, -4.5340, -4.8963, -4.2180, -4.2173, -3.4795,\n",
      "         -4.0050, -4.7873, -4.8557, -4.6760, -5.1392, -4.5978, -4.8255, -4.9521]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 75\n",
      "OUTPUTS tensor([[-3.6548, -4.0961, -4.2281, -4.7827, -3.4830, -4.9890, -4.9633, -5.2768,\n",
      "         -4.8477, -4.3813, -5.0867, -5.2494, -4.8020, -3.9975, -4.7438, -4.4657,\n",
      "         -4.0853, -4.3787, -4.4898, -5.2005, -4.3643, -4.8434, -4.8035, -4.6412,\n",
      "         -5.3245, -3.5988, -4.6670, -4.2247, -4.2531, -3.9222, -4.9226, -4.6179,\n",
      "         -4.5395, -4.0554, -5.1465, -4.7587, -4.2483, -4.2442, -5.2639, -4.6873,\n",
      "         -4.3132, -4.9941, -4.3210, -4.6662, -3.4860, -5.1275, -5.1810, -3.7325,\n",
      "         -4.4810, -4.0048, -4.3571, -3.4502, -4.0825, -4.3657, -5.1483, -4.6136,\n",
      "         -3.8950, -4.4548, -4.3474, -4.9436, -5.1870, -3.1537, -4.2791, -4.5746,\n",
      "         -5.0856, -5.0553, -4.4610, -4.5637, -4.9830, -4.2236, -4.2048, -3.4044,\n",
      "         -3.9702, -4.8634, -4.9190, -4.7202, -5.2351, -4.6326, -4.8926, -5.0300]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 76\n",
      "OUTPUTS tensor([[-3.6022, -4.0739, -4.2168, -4.8380, -3.4171, -5.0625, -5.0354, -5.3844,\n",
      "         -4.9277, -4.4047, -5.1713, -5.3524, -4.8716, -3.9731, -4.8120, -4.4609,\n",
      "         -4.0474, -4.3894, -4.5216, -5.3046, -4.3751, -4.9229, -4.8289, -4.6922,\n",
      "         -5.4371, -3.5399, -4.7102, -4.2340, -4.2533, -3.8851, -5.0039, -4.6721,\n",
      "         -4.5716, -4.0277, -5.2368, -4.8074, -4.2625, -4.2604, -5.3726, -4.7253,\n",
      "         -4.3189, -5.0705, -4.3242, -4.7084, -3.4137, -5.2235, -5.2779, -3.6897,\n",
      "         -4.5175, -3.9838, -4.3668, -3.3779, -4.0783, -4.3390, -5.2446, -4.6472,\n",
      "         -3.8669, -4.4713, -4.3641, -5.0106, -5.2851, -3.0572, -4.2984, -4.6166,\n",
      "         -5.1748, -5.1375, -4.4278, -4.5973, -5.0746, -4.2328, -4.1937, -3.3320,\n",
      "         -3.9374, -4.9444, -4.9866, -4.7684, -5.3359, -4.6714, -4.9642, -5.1124]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 77\n",
      "OUTPUTS tensor([[-4.2839, -4.3093, -4.3819, -4.4671, -4.3040, -4.5003, -4.4812, -4.4719,\n",
      "         -4.2729, -4.2760, -4.5083, -4.4964, -4.3177, -4.3036, -4.2566, -4.4633,\n",
      "         -4.3795, -4.4790, -4.3699, -4.4223, -4.4622, -4.2722, -4.4742, -4.3139,\n",
      "         -4.4864, -4.3444, -4.4529, -4.2925, -4.3240, -4.3668, -4.3229, -4.2701,\n",
      "         -4.4449, -4.2900, -4.5094, -4.4885, -4.2892, -4.2670, -4.4579, -4.5318,\n",
      "         -4.2769, -4.4674, -4.4966, -4.4626, -4.3262, -4.4312, -4.4630, -4.3266,\n",
      "         -4.3030, -4.2644, -4.2702, -4.3349, -4.3047, -4.4970, -4.4351, -4.4966,\n",
      "         -4.2534, -4.3050, -4.3634, -4.5228, -4.4593, -4.2862, -4.2719, -4.3372,\n",
      "         -4.4634, -4.4841, -4.4737, -4.4546, -4.2829, -4.3082, -4.3585, -4.2375,\n",
      "         -4.3789, -4.2798, -4.4933, -4.4672, -4.5019, -4.4731, -4.4021, -4.4628]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 78\n",
      "OUTPUTS tensor([[-3.5385, -4.0392, -4.1984, -4.9196, -3.3425, -5.1678, -5.1383, -5.5338,\n",
      "         -5.0402, -4.4396, -5.2919, -5.4964, -4.9700, -3.9404, -4.9071, -4.4456,\n",
      "         -3.9886, -4.4130, -4.5714, -5.4492, -4.3986, -5.0348, -4.8480, -4.7649,\n",
      "         -5.5939, -3.4739, -4.7752, -4.2525, -4.2543, -3.8381, -5.1171, -4.7497,\n",
      "         -4.6233, -3.9849, -5.3644, -4.8791, -4.2892, -4.2899, -5.5242, -4.7843,\n",
      "         -4.3209, -5.1787, -4.3384, -4.7722, -3.3272, -5.3584, -5.4126, -3.6444,\n",
      "         -4.5724, -3.9259, -4.3722, -3.2955, -4.0817, -4.2897, -5.3785, -4.7001,\n",
      "         -3.8311, -4.4868, -4.3943, -5.1079, -5.4166, -2.9455, -4.3321, -4.6781,\n",
      "         -5.3012, -5.2538, -4.3569, -4.6509, -5.2007, -4.2528, -4.1757, -3.2453,\n",
      "         -3.8957, -5.0585, -5.0846, -4.8405, -5.4771, -4.7315, -5.0658, -5.2294]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS tensor([[-3.4981, -4.0224, -4.1910, -4.9764, -3.2932, -5.2408, -5.2099, -5.6366,\n",
      "         -5.1192, -4.4678, -5.3748, -5.5953, -5.0395, -3.9224, -4.9749, -4.4438,\n",
      "         -3.9578, -4.4303, -4.6073, -5.5489, -4.4160, -5.1133, -4.8707, -4.8174,\n",
      "         -5.7013, -3.4309, -4.8210, -4.2678, -4.2605, -3.8101, -5.1968, -4.8053,\n",
      "         -4.6598, -3.9640, -5.4523, -4.9296, -4.3097, -4.3123, -5.6281, -4.8258,\n",
      "         -4.2994, -5.2540, -4.3491, -4.8173, -3.2708, -5.4515, -5.5059, -3.6154,\n",
      "         -4.6125, -3.8962, -4.3824, -3.2406, -4.0860, -4.2671, -5.4713, -4.7375,\n",
      "         -3.8105, -4.5043, -4.4173, -5.1752, -5.5064, -2.8703, -4.3574, -4.7229,\n",
      "         -5.3881, -5.3343, -4.3251, -4.6887, -5.2892, -4.2688, -4.1685, -3.1891,\n",
      "         -3.8712, -5.1386, -5.1525, -4.8910, -5.5740, -4.7737, -5.1370, -5.3102]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "[1,    80] loss: 0.022\n",
      "BATCH 80\n",
      "OUTPUTS tensor([[-3.5421, -4.0174, -4.1810, -4.9286, -3.3615, -5.1751, -5.1445, -5.5380,\n",
      "         -5.0431, -4.4378, -5.3006, -5.5027, -4.9724, -3.9297, -4.9066, -4.4153,\n",
      "         -3.9596, -4.4248, -4.5782, -5.4528, -4.4101, -5.0376, -4.8057, -4.7654,\n",
      "         -5.6000, -3.4911, -4.7830, -4.2559, -4.2460, -3.8331, -5.1175, -4.7517,\n",
      "         -4.6349, -3.9616, -5.3717, -4.8855, -4.2966, -4.2975, -5.5299, -4.7943,\n",
      "         -4.2470, -5.1837, -4.3515, -4.7803, -3.3384, -5.3644, -5.4158, -3.6602,\n",
      "         -4.5765, -3.8910, -4.3457, -3.3156, -4.0917, -4.2491, -5.3816, -4.7098,\n",
      "         -3.8241, -4.4626, -4.4017, -5.1169, -5.3566, -2.9722, -4.3395, -4.6804,\n",
      "         -5.3092, -5.2599, -4.2871, -4.6623, -5.1992, -4.2599, -4.1584, -3.2557,\n",
      "         -3.8902, -5.0617, -5.0937, -4.8495, -5.4834, -4.7419, -5.0700, -5.2366]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 81\n",
      "OUTPUTS tensor([[-4.2800, -4.3010, -4.3777, -4.4773, -4.3007, -4.5086, -4.4886, -4.4800,\n",
      "         -4.2691, -4.2705, -4.5188, -4.5066, -4.3138, -4.2995, -4.2528, -4.4557,\n",
      "         -4.3709, -4.4861, -4.3723, -4.4303, -4.4697, -4.2683, -4.4667, -4.3102,\n",
      "         -4.4964, -4.3394, -4.4608, -4.2892, -4.3186, -4.3629, -4.3189, -4.2664,\n",
      "         -4.4540, -4.2829, -4.5187, -4.4958, -4.2858, -4.2636, -4.4670, -4.5414,\n",
      "         -4.2678, -4.4740, -4.5039, -4.4700, -4.3220, -4.4361, -4.4693, -4.3237,\n",
      "         -4.2991, -4.2561, -4.2654, -4.3325, -4.3014, -4.4875, -4.4408, -4.5049,\n",
      "         -4.2495, -4.2996, -4.3597, -4.5334, -4.4437, -4.2825, -4.2680, -4.3334,\n",
      "         -4.4736, -4.4931, -4.4598, -4.4643, -4.2786, -4.3042, -4.3544, -4.2302,\n",
      "         -4.3753, -4.2759, -4.5037, -4.4760, -4.5117, -4.4828, -4.4045, -4.4723]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 82\n",
      "OUTPUTS tensor([[-3.6113, -3.9861, -4.1707, -4.8657, -3.4668, -5.0856, -5.0555, -5.4001,\n",
      "         -4.9371, -4.3997, -5.1989, -5.3737, -4.8795, -3.9449, -4.8120, -4.3794,\n",
      "         -3.9682, -4.4233, -4.5416, -5.3187, -4.4081, -4.9321, -4.7159, -4.6946,\n",
      "         -5.4582, -3.5846, -4.7340, -4.2438, -4.2308, -3.8717, -5.0067, -4.6787,\n",
      "         -4.6053, -3.9643, -5.2604, -4.8276, -4.2828, -4.2812, -5.3927, -4.7552,\n",
      "         -4.1774, -5.0873, -4.3615, -4.7329, -3.4425, -5.2431, -5.2902, -3.7312,\n",
      "         -4.5292, -3.8892, -4.2969, -3.4308, -4.1053, -4.2287, -5.2565, -4.6759,\n",
      "         -3.8470, -4.4069, -4.3842, -5.0386, -5.1436, -3.1268, -4.3187, -4.6236,\n",
      "         -5.2005, -5.1578, -4.2382, -4.6306, -5.0726, -4.2520, -4.1482, -3.3581,\n",
      "         -3.9228, -4.9545, -5.0147, -4.7955, -5.3574, -4.7025, -4.9780, -5.1357]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 83\n",
      "OUTPUTS tensor([[-3.7090, -4.0041, -4.1809, -4.7811, -3.5985, -4.9671, -4.9383, -5.2200,\n",
      "         -4.7997, -4.3563, -5.0628, -5.2038, -4.7601, -3.9830, -4.6929, -4.3640,\n",
      "         -4.0109, -4.4185, -4.4940, -5.1440, -4.4030, -4.7954, -4.6395, -4.6065,\n",
      "         -5.2714, -3.7018, -4.6690, -4.2320, -4.2238, -3.9358, -4.8283, -4.5861,\n",
      "         -4.5631, -3.9939, -5.1129, -4.7519, -4.2649, -4.2597, -5.2122, -4.7015,\n",
      "         -4.1532, -4.9615, -4.3711, -4.6698, -3.5798, -5.0832, -5.1271, -3.8197,\n",
      "         -4.4693, -3.9236, -4.2627, -3.5748, -4.1220, -4.2428, -5.0942, -4.6295,\n",
      "         -3.8928, -4.3606, -4.3614, -4.9334, -4.9666, -3.3175, -4.2912, -4.5526,\n",
      "         -5.0559, -5.0235, -4.2381, -4.5857, -4.9117, -4.2425, -4.1582, -3.4936,\n",
      "         -3.9797, -4.8153, -4.9085, -4.7229, -5.1913, -4.6479, -4.8585, -5.0020]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 84\n",
      "OUTPUTS tensor([[-4.2772, -4.2933, -4.3750, -4.4846, -4.2985, -4.5147, -4.4940, -4.4858,\n",
      "         -4.2663, -4.2669, -4.5263, -4.5139, -4.3110, -4.2968, -4.2501, -4.4510,\n",
      "         -4.3657, -4.4914, -4.3741, -4.4360, -4.4753, -4.2656, -4.4621, -4.3075,\n",
      "         -4.5035, -4.3363, -4.4666, -4.2868, -4.3150, -4.3604, -4.3123, -4.2637,\n",
      "         -4.4607, -4.2785, -4.5254, -4.5012, -4.2835, -4.2612, -4.4735, -4.5483,\n",
      "         -4.2613, -4.4789, -4.5093, -4.4754, -4.3194, -4.4397, -4.4739, -4.3218,\n",
      "         -4.2962, -4.2509, -4.2621, -4.3310, -4.2991, -4.4818, -4.4449, -4.5110,\n",
      "         -4.2424, -4.2961, -4.3571, -4.5410, -4.4330, -4.2802, -4.2651, -4.3307,\n",
      "         -4.4809, -4.4995, -4.4518, -4.4713, -4.2755, -4.3014, -4.3517, -4.2258,\n",
      "         -4.3729, -4.2711, -4.5112, -4.4824, -4.5187, -4.4898, -4.4064, -4.4792]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 85\n",
      "OUTPUTS tensor([[-4.0440, -4.1524, -4.2768, -4.5776, -4.0200, -4.6650, -4.6412, -4.7400,\n",
      "         -4.4431, -4.2781, -4.7084, -4.7528, -4.4567, -4.1536, -4.3929, -4.3913,\n",
      "         -4.2066, -4.4469, -4.3992, -4.6804, -4.4311, -4.4413, -4.4977, -4.3967,\n",
      "         -4.7714, -4.0816, -4.5239, -4.2447, -4.2577, -4.1790, -4.4572, -4.3620,\n",
      "         -4.4817, -4.1465, -4.7264, -4.5764, -4.2560, -4.2403, -4.7304, -4.5882,\n",
      "         -4.1861, -4.6396, -4.4404, -4.5299, -4.0243, -4.6604, -4.6975, -4.1162,\n",
      "         -4.3390, -4.1007, -4.2346, -4.0316, -4.2130, -4.3636, -4.6671, -4.5376,\n",
      "         -4.0867, -4.2936, -4.3378, -4.6699, -4.5867, -3.9055, -4.2541, -4.3915,\n",
      "         -4.6772, -4.6766, -4.3379, -4.4971, -4.4900, -4.2584, -4.2537, -3.9328,\n",
      "         -4.2032, -4.4249, -4.6419, -4.5546, -4.7511, -4.5317, -4.5551, -4.6558]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 86\n",
      "OUTPUTS tensor([[-4.2356, -4.2638, -4.3553, -4.5029, -4.2499, -4.5418, -4.5201, -4.5299,\n",
      "         -4.2923, -4.2650, -4.5595, -4.5563, -4.3318, -4.2669, -4.2704, -4.4367,\n",
      "         -4.3347, -4.4857, -4.3779, -4.4784, -4.4698, -4.2913, -4.4633, -4.3190,\n",
      "         -4.5506, -4.2911, -4.4781, -4.2769, -4.3019, -4.3274, -4.3286, -4.2768,\n",
      "         -4.4667, -4.2525, -4.5612, -4.5154, -4.2760, -4.2548, -4.5184, -4.5576,\n",
      "         -4.2431, -4.5069, -4.4997, -4.4862, -4.2677, -4.4769, -4.5122, -4.2853,\n",
      "         -4.3001, -4.2214, -4.2537, -4.2794, -4.2821, -4.4573, -4.4826, -4.5175,\n",
      "         -4.2113, -4.2917, -4.3509, -4.5654, -4.4499, -4.2158, -4.2601, -4.3376,\n",
      "         -4.5164, -4.5312, -4.4264, -4.4782, -4.3075, -4.2911, -4.3321, -4.1735,\n",
      "         -4.3418, -4.2884, -4.5359, -4.4966, -4.5599, -4.4920, -4.4307, -4.5111]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 87\n",
      "OUTPUTS tensor([[-4.2746, -4.2870, -4.3723, -4.4921, -4.2963, -4.5211, -4.4998, -4.4921,\n",
      "         -4.2637, -4.2634, -4.5340, -4.5216, -4.3083, -4.2896, -4.2475, -4.4470,\n",
      "         -4.3612, -4.4967, -4.3761, -4.4422, -4.4808, -4.2629, -4.4580, -4.3049,\n",
      "         -4.5111, -4.3334, -4.4725, -4.2845, -4.3117, -4.3580, -4.3051, -4.2612,\n",
      "         -4.4674, -4.2746, -4.5325, -4.5068, -4.2811, -4.2589, -4.4805, -4.5553,\n",
      "         -4.2559, -4.4841, -4.5146, -4.4810, -4.3168, -4.4437, -4.4789, -4.3199,\n",
      "         -4.2934, -4.2464, -4.2591, -4.3294, -4.2969, -4.4771, -4.4494, -4.5171,\n",
      "         -4.2365, -4.2928, -4.3546, -4.5487, -4.4230, -4.2779, -4.2624, -4.3282,\n",
      "         -4.4886, -4.5063, -4.4451, -4.4783, -4.2726, -4.2986, -4.3491, -4.2218,\n",
      "         -4.3706, -4.2635, -4.5189, -4.4889, -4.5262, -4.4852, -4.4087, -4.4864]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 88\n",
      "OUTPUTS tensor([[-4.2736, -4.2850, -4.3715, -4.4946, -4.2955, -4.5233, -4.5018, -4.4944,\n",
      "         -4.2628, -4.2623, -4.5367, -4.5243, -4.3075, -4.2875, -4.2466, -4.4457,\n",
      "         -4.3598, -4.4984, -4.3768, -4.4445, -4.4825, -4.2620, -4.4567, -4.3040,\n",
      "         -4.5138, -4.3324, -4.4745, -4.2837, -4.3107, -4.3572, -4.3030, -4.2604,\n",
      "         -4.4696, -4.2734, -4.5350, -4.5087, -4.2804, -4.2560, -4.4830, -4.5576,\n",
      "         -4.2542, -4.4859, -4.5163, -4.4828, -4.3160, -4.4453, -4.4808, -4.3192,\n",
      "         -4.2925, -4.2450, -4.2580, -4.3289, -4.2961, -4.4756, -4.4511, -4.5192,\n",
      "         -4.2347, -4.2917, -4.3537, -4.5514, -4.4193, -4.2771, -4.2614, -4.3273,\n",
      "         -4.4913, -4.5087, -4.4431, -4.4806, -4.2716, -4.2977, -4.3483, -4.2205,\n",
      "         -4.3699, -4.2612, -4.5216, -4.4911, -4.5288, -4.4820, -4.4095, -4.4889]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS tensor([[-4.2727, -4.2832, -4.3706, -4.4972, -4.2948, -4.5256, -4.5039, -4.4968,\n",
      "         -4.2620, -4.2612, -4.5396, -4.5206, -4.3066, -4.2855, -4.2458, -4.4445,\n",
      "         -4.3585, -4.5001, -4.3775, -4.4469, -4.4843, -4.2612, -4.4555, -4.3032,\n",
      "         -4.5166, -4.3315, -4.4766, -4.2830, -4.3097, -4.3564, -4.3010, -4.2596,\n",
      "         -4.4719, -4.2722, -4.5376, -4.5107, -4.2796, -4.2534, -4.4857, -4.5600,\n",
      "         -4.2526, -4.4878, -4.5180, -4.4848, -4.3151, -4.4469, -4.4827, -4.3185,\n",
      "         -4.2916, -4.2437, -4.2571, -4.3283, -4.2954, -4.4742, -4.4529, -4.5213,\n",
      "         -4.2331, -4.2907, -4.3529, -4.5542, -4.4159, -4.2763, -4.2605, -4.3265,\n",
      "         -4.4940, -4.5112, -4.4413, -4.4830, -4.2707, -4.2968, -4.3475, -4.2193,\n",
      "         -4.3691, -4.2591, -4.5243, -4.4934, -4.5316, -4.4792, -4.4104, -4.4915]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "[1,    90] loss: 0.022\n",
      "BATCH 90\n",
      "OUTPUTS tensor([[-4.2719, -4.2816, -4.3698, -4.4999, -4.2940, -4.5280, -4.5061, -4.4994,\n",
      "         -4.2612, -4.2602, -4.5425, -4.5172, -4.3058, -4.2836, -4.2450, -4.4434,\n",
      "         -4.3572, -4.5018, -4.3783, -4.4494, -4.4861, -4.2604, -4.4544, -4.3024,\n",
      "         -4.5196, -4.3306, -4.4787, -4.2823, -4.3087, -4.3557, -4.2991, -4.2588,\n",
      "         -4.4742, -4.2711, -4.5403, -4.5127, -4.2789, -4.2510, -4.4884, -4.5624,\n",
      "         -4.2512, -4.4899, -4.5197, -4.4868, -4.3143, -4.4486, -4.4791, -4.3179,\n",
      "         -4.2908, -4.2424, -4.2562, -4.3278, -4.2947, -4.4729, -4.4549, -4.5234,\n",
      "         -4.2315, -4.2897, -4.3522, -4.5570, -4.4127, -4.2756, -4.2597, -4.3257,\n",
      "         -4.4969, -4.5138, -4.4395, -4.4854, -4.2698, -4.2959, -4.3467, -4.2182,\n",
      "         -4.3684, -4.2571, -4.5271, -4.4957, -4.5345, -4.4765, -4.4114, -4.4942]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 91\n",
      "OUTPUTS tensor([[-4.2711, -4.2800, -4.3691, -4.5026, -4.2933, -4.5305, -4.5084, -4.5021,\n",
      "         -4.2604, -4.2592, -4.5455, -4.5141, -4.3050, -4.2819, -4.2443, -4.4424,\n",
      "         -4.3561, -4.5035, -4.3791, -4.4520, -4.4879, -4.2596, -4.4533, -4.3017,\n",
      "         -4.5227, -4.3298, -4.4809, -4.2816, -4.3078, -4.3550, -4.2974, -4.2581,\n",
      "         -4.4765, -4.2700, -4.5431, -4.5148, -4.2782, -4.2487, -4.4913, -4.5649,\n",
      "         -4.2498, -4.4920, -4.5214, -4.4888, -4.3136, -4.4505, -4.4757, -4.3173,\n",
      "         -4.2899, -4.2412, -4.2553, -4.3272, -4.2940, -4.4717, -4.4569, -4.5256,\n",
      "         -4.2300, -4.2888, -4.3514, -4.5599, -4.4098, -4.2748, -4.2588, -4.3250,\n",
      "         -4.4999, -4.5165, -4.4379, -4.4878, -4.2689, -4.2950, -4.3459, -4.2171,\n",
      "         -4.3677, -4.2553, -4.5235, -4.4981, -4.5374, -4.4741, -4.4125, -4.4970]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 92\n",
      "OUTPUTS tensor([[-4.2703, -4.2785, -4.3684, -4.5054, -4.2926, -4.5331, -4.5107, -4.5048,\n",
      "         -4.2597, -4.2582, -4.5485, -4.5112, -4.3043, -4.2803, -4.2436, -4.4414,\n",
      "         -4.3550, -4.5053, -4.3800, -4.4547, -4.4898, -4.2589, -4.4523, -4.3010,\n",
      "         -4.5259, -4.3289, -4.4831, -4.2809, -4.3069, -4.3543, -4.2958, -4.2574,\n",
      "         -4.4730, -4.2691, -4.5460, -4.5170, -4.2776, -4.2467, -4.4943, -4.5674,\n",
      "         -4.2485, -4.4942, -4.5231, -4.4909, -4.3128, -4.4524, -4.4726, -4.3167,\n",
      "         -4.2891, -4.2400, -4.2545, -4.3267, -4.2934, -4.4706, -4.4590, -4.5278,\n",
      "         -4.2287, -4.2879, -4.3508, -4.5628, -4.4071, -4.2741, -4.2580, -4.3243,\n",
      "         -4.5030, -4.5192, -4.4364, -4.4903, -4.2681, -4.2942, -4.3452, -4.2160,\n",
      "         -4.3671, -4.2536, -4.5202, -4.5005, -4.5405, -4.4718, -4.4136, -4.4999]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 93\n",
      "OUTPUTS tensor([[-4.2695, -4.2771, -4.3677, -4.5082, -4.2920, -4.5357, -4.5131, -4.5077,\n",
      "         -4.2591, -4.2573, -4.5516, -4.5086, -4.3036, -4.2788, -4.2430, -4.4404,\n",
      "         -4.3539, -4.5071, -4.3809, -4.4575, -4.4916, -4.2583, -4.4514, -4.3004,\n",
      "         -4.5291, -4.3282, -4.4853, -4.2803, -4.3061, -4.3537, -4.2943, -4.2568,\n",
      "         -4.4697, -4.2682, -4.5490, -4.5192, -4.2770, -4.2448, -4.4907, -4.5699,\n",
      "         -4.2473, -4.4965, -4.5248, -4.4930, -4.3121, -4.4545, -4.4698, -4.3162,\n",
      "         -4.2884, -4.2390, -4.2537, -4.3262, -4.2928, -4.4696, -4.4612, -4.5300,\n",
      "         -4.2274, -4.2871, -4.3501, -4.5658, -4.4046, -4.2735, -4.2573, -4.3236,\n",
      "         -4.5061, -4.5221, -4.4350, -4.4928, -4.2674, -4.2935, -4.3446, -4.2151,\n",
      "         -4.3664, -4.2521, -4.5171, -4.5030, -4.5436, -4.4697, -4.4148, -4.5028]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 94\n",
      "OUTPUTS tensor([[-4.2688, -4.2759, -4.3671, -4.5047, -4.2914, -4.5384, -4.5156, -4.5106,\n",
      "         -4.2585, -4.2565, -4.5548, -4.5062, -4.3030, -4.2774, -4.2424, -4.4396,\n",
      "         -4.3530, -4.5089, -4.3819, -4.4604, -4.4935, -4.2577, -4.4505, -4.2998,\n",
      "         -4.5325, -4.3275, -4.4876, -4.2798, -4.3053, -4.3531, -4.2930, -4.2562,\n",
      "         -4.4667, -4.2673, -4.5520, -4.5214, -4.2765, -4.2430, -4.4874, -4.5725,\n",
      "         -4.2462, -4.4988, -4.5266, -4.4952, -4.3115, -4.4566, -4.4672, -4.3157,\n",
      "         -4.2877, -4.2380, -4.2530, -4.3258, -4.2923, -4.4686, -4.4635, -4.5323,\n",
      "         -4.2262, -4.2863, -4.3495, -4.5689, -4.4023, -4.2728, -4.2566, -4.3230,\n",
      "         -4.5093, -4.5250, -4.4337, -4.4954, -4.2666, -4.2928, -4.3440, -4.2141,\n",
      "         -4.3659, -4.2507, -4.5144, -4.5055, -4.5469, -4.4679, -4.4161, -4.5058]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 95\n",
      "OUTPUTS tensor([[-4.2681, -4.2746, -4.3665, -4.5014, -4.2907, -4.5411, -4.5181, -4.5136,\n",
      "         -4.2579, -4.2556, -4.5580, -4.5040, -4.3024, -4.2761, -4.2418, -4.4388,\n",
      "         -4.3521, -4.5107, -4.3829, -4.4633, -4.4954, -4.2571, -4.4497, -4.2992,\n",
      "         -4.5358, -4.3267, -4.4899, -4.2792, -4.3045, -4.3525, -4.2917, -4.2556,\n",
      "         -4.4640, -4.2665, -4.5551, -4.5237, -4.2759, -4.2414, -4.4844, -4.5751,\n",
      "         -4.2451, -4.5012, -4.5284, -4.4974, -4.3108, -4.4587, -4.4649, -4.3152,\n",
      "         -4.2870, -4.2370, -4.2522, -4.3253, -4.2917, -4.4677, -4.4658, -4.5346,\n",
      "         -4.2251, -4.2856, -4.3489, -4.5719, -4.4002, -4.2722, -4.2559, -4.3224,\n",
      "         -4.5125, -4.5279, -4.4325, -4.4979, -4.2659, -4.2920, -4.3434, -4.2132,\n",
      "         -4.3653, -4.2493, -4.5118, -4.5081, -4.5501, -4.4643, -4.4175, -4.5088]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 96\n",
      "OUTPUTS tensor([[-4.2674, -4.2735, -4.3659, -4.4984, -4.2901, -4.5439, -4.5207, -4.5166,\n",
      "         -4.2573, -4.2548, -4.5613, -4.5020, -4.3018, -4.2748, -4.2412, -4.4380,\n",
      "         -4.3512, -4.5125, -4.3839, -4.4663, -4.4973, -4.2565, -4.4489, -4.2986,\n",
      "         -4.5393, -4.3261, -4.4923, -4.2787, -4.3038, -4.3520, -4.2905, -4.2551,\n",
      "         -4.4615, -4.2657, -4.5582, -4.5260, -4.2754, -4.2399, -4.4817, -4.5777,\n",
      "         -4.2441, -4.5036, -4.5301, -4.4997, -4.3102, -4.4609, -4.4627, -4.3147,\n",
      "         -4.2864, -4.2360, -4.2515, -4.3249, -4.2912, -4.4668, -4.4682, -4.5369,\n",
      "         -4.2240, -4.2849, -4.3484, -4.5750, -4.3982, -4.2715, -4.2552, -4.3219,\n",
      "         -4.5157, -4.5308, -4.4313, -4.5004, -4.2653, -4.2913, -4.3428, -4.2124,\n",
      "         -4.3648, -4.2481, -4.5095, -4.5046, -4.5534, -4.4610, -4.4189, -4.5119]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 97\n",
      "OUTPUTS tensor([[-4.2667, -4.2724, -4.3653, -4.4957, -4.2896, -4.5467, -4.5233, -4.5197,\n",
      "         -4.2568, -4.2540, -4.5646, -4.5001, -4.3012, -4.2737, -4.2408, -4.4373,\n",
      "         -4.3504, -4.5144, -4.3850, -4.4693, -4.4993, -4.2560, -4.4482, -4.2981,\n",
      "         -4.5427, -4.3254, -4.4947, -4.2782, -4.3031, -4.3515, -4.2893, -4.2546,\n",
      "         -4.4592, -4.2649, -4.5547, -4.5283, -4.2749, -4.2385, -4.4792, -4.5803,\n",
      "         -4.2431, -4.5061, -4.5320, -4.5020, -4.3096, -4.4632, -4.4607, -4.3143,\n",
      "         -4.2858, -4.2352, -4.2509, -4.3245, -4.2908, -4.4660, -4.4707, -4.5393,\n",
      "         -4.2230, -4.2842, -4.3478, -4.5781, -4.3964, -4.2710, -4.2546, -4.3213,\n",
      "         -4.5190, -4.5339, -4.4302, -4.5030, -4.2646, -4.2907, -4.3423, -4.2116,\n",
      "         -4.3643, -4.2469, -4.5073, -4.5014, -4.5567, -4.4580, -4.4203, -4.5150]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 98\n",
      "OUTPUTS tensor([[-4.2661, -4.2714, -4.3648, -4.4932, -4.2890, -4.5495, -4.5259, -4.5228,\n",
      "         -4.2563, -4.2533, -4.5679, -4.4984, -4.3007, -4.2726, -4.2403, -4.4366,\n",
      "         -4.3496, -4.5163, -4.3861, -4.4724, -4.5012, -4.2555, -4.4475, -4.2976,\n",
      "         -4.5462, -4.3248, -4.4971, -4.2777, -4.3025, -4.3510, -4.2883, -4.2541,\n",
      "         -4.4571, -4.2642, -4.5514, -4.5307, -4.2744, -4.2372, -4.4768, -4.5829,\n",
      "         -4.2422, -4.5086, -4.5337, -4.5042, -4.3090, -4.4655, -4.4571, -4.3138,\n",
      "         -4.2851, -4.2343, -4.2503, -4.3241, -4.2903, -4.4652, -4.4731, -4.5416,\n",
      "         -4.2221, -4.2835, -4.3473, -4.5813, -4.3947, -4.2704, -4.2539, -4.3208,\n",
      "         -4.5223, -4.5369, -4.4292, -4.5056, -4.2640, -4.2901, -4.3417, -4.2108,\n",
      "         -4.3638, -4.2458, -4.5053, -4.4984, -4.5601, -4.4553, -4.4218, -4.5182]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS tensor([[-4.2654, -4.2704, -4.3643, -4.4909, -4.2884, -4.5523, -4.5286, -4.5259,\n",
      "         -4.2558, -4.2525, -4.5711, -4.4967, -4.3002, -4.2716, -4.2398, -4.4359,\n",
      "         -4.3488, -4.5181, -4.3872, -4.4754, -4.5031, -4.2549, -4.4468, -4.2971,\n",
      "         -4.5497, -4.3242, -4.4995, -4.2772, -4.3018, -4.3505, -4.2873, -4.2536,\n",
      "         -4.4551, -4.2635, -4.5484, -4.5330, -4.2740, -4.2359, -4.4730, -4.5855,\n",
      "         -4.2413, -4.5111, -4.5355, -4.5065, -4.3084, -4.4679, -4.4537, -4.3133,\n",
      "         -4.2845, -4.2334, -4.2496, -4.3237, -4.2898, -4.4645, -4.4756, -4.5440,\n",
      "         -4.2212, -4.2829, -4.3468, -4.5844, -4.3931, -4.2698, -4.2533, -4.3203,\n",
      "         -4.5256, -4.5399, -4.4282, -4.5082, -4.2634, -4.2894, -4.3412, -4.2100,\n",
      "         -4.3633, -4.2447, -4.5035, -4.4957, -4.5634, -4.4527, -4.4232, -4.5213]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "[1,   100] loss: 0.022\n",
      "BATCH 100\n",
      "OUTPUTS tensor([[-4.2648, -4.2694, -4.3638, -4.4888, -4.2879, -4.5552, -4.5312, -4.5290,\n",
      "         -4.2554, -4.2518, -4.5745, -4.4952, -4.2997, -4.2706, -4.2393, -4.4353,\n",
      "         -4.3481, -4.5200, -4.3883, -4.4785, -4.5051, -4.2545, -4.4462, -4.2966,\n",
      "         -4.5532, -4.3236, -4.5019, -4.2768, -4.3012, -4.3500, -4.2863, -4.2531,\n",
      "         -4.4534, -4.2629, -4.5457, -4.5354, -4.2735, -4.2348, -4.4694, -4.5882,\n",
      "         -4.2405, -4.5137, -4.5373, -4.5088, -4.3079, -4.4644, -4.4506, -4.3129,\n",
      "         -4.2839, -4.2326, -4.2491, -4.3233, -4.2894, -4.4638, -4.4782, -4.5463,\n",
      "         -4.2203, -4.2823, -4.3463, -4.5876, -4.3916, -4.2692, -4.2527, -4.3198,\n",
      "         -4.5289, -4.5430, -4.4273, -4.5108, -4.2628, -4.2888, -4.3407, -4.2092,\n",
      "         -4.3628, -4.2438, -4.5018, -4.4932, -4.5668, -4.4504, -4.4248, -4.5244]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 101\n",
      "OUTPUTS tensor([[-4.2642, -4.2685, -4.3633, -4.4868, -4.2873, -4.5580, -4.5339, -4.5322,\n",
      "         -4.2549, -4.2511, -4.5778, -4.4939, -4.2992, -4.2697, -4.2389, -4.4346,\n",
      "         -4.3473, -4.5218, -4.3895, -4.4816, -4.5070, -4.2540, -4.4456, -4.2961,\n",
      "         -4.5567, -4.3230, -4.5043, -4.2764, -4.3006, -4.3496, -4.2854, -4.2527,\n",
      "         -4.4517, -4.2622, -4.5432, -4.5378, -4.2731, -4.2337, -4.4662, -4.5908,\n",
      "         -4.2397, -4.5162, -4.5391, -4.5111, -4.3073, -4.4612, -4.4478, -4.3125,\n",
      "         -4.2834, -4.2319, -4.2485, -4.3229, -4.2889, -4.4609, -4.4807, -4.5487,\n",
      "         -4.2195, -4.2817, -4.3459, -4.5907, -4.3902, -4.2686, -4.2521, -4.3194,\n",
      "         -4.5323, -4.5460, -4.4264, -4.5134, -4.2622, -4.2882, -4.3403, -4.2085,\n",
      "         -4.3623, -4.2428, -4.5002, -4.4909, -4.5702, -4.4482, -4.4263, -4.5276]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 102\n",
      "OUTPUTS tensor([[-4.2635, -4.2677, -4.3628, -4.4850, -4.2868, -4.5609, -4.5366, -4.5354,\n",
      "         -4.2545, -4.2492, -4.5811, -4.4925, -4.2987, -4.2687, -4.2384, -4.4340,\n",
      "         -4.3466, -4.5237, -4.3906, -4.4848, -4.5089, -4.2535, -4.4449, -4.2957,\n",
      "         -4.5602, -4.3224, -4.5067, -4.2759, -4.2999, -4.3491, -4.2845, -4.2522,\n",
      "         -4.4501, -4.2616, -4.5409, -4.5402, -4.2726, -4.2326, -4.4632, -4.5935,\n",
      "         -4.2389, -4.5188, -4.5409, -4.5134, -4.3068, -4.4582, -4.4452, -4.3121,\n",
      "         -4.2828, -4.2311, -4.2479, -4.3225, -4.2885, -4.4583, -4.4833, -4.5511,\n",
      "         -4.2186, -4.2810, -4.3454, -4.5938, -4.3889, -4.2680, -4.2515, -4.3189,\n",
      "         -4.5356, -4.5491, -4.4255, -4.5160, -4.2616, -4.2876, -4.3398, -4.2078,\n",
      "         -4.3619, -4.2420, -4.4987, -4.4888, -4.5735, -4.4462, -4.4279, -4.5307]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 103\n",
      "OUTPUTS tensor([[-4.2630, -4.2667, -4.3624, -4.4832, -4.2860, -4.5636, -4.5392, -4.5384,\n",
      "         -4.2541, -4.2476, -4.5845, -4.4911, -4.2982, -4.2678, -4.2379, -4.4334,\n",
      "         -4.3461, -4.5255, -4.3917, -4.4877, -4.5109, -4.2533, -4.4443, -4.2951,\n",
      "         -4.5640, -4.3219, -4.5090, -4.2756, -4.2994, -4.3489, -4.2835, -4.2519,\n",
      "         -4.4469, -4.2611, -4.5387, -4.5428, -4.2723, -4.2318, -4.4605, -4.5961,\n",
      "         -4.2379, -4.5212, -4.5425, -4.5157, -4.3060, -4.4555, -4.4427, -4.3116,\n",
      "         -4.2820, -4.2303, -4.2473, -4.3219, -4.2882, -4.4559, -4.4858, -4.5534,\n",
      "         -4.2180, -4.2805, -4.3452, -4.5969, -4.3875, -4.2673, -4.2508, -4.3187,\n",
      "         -4.5388, -4.5524, -4.4245, -4.5185, -4.2613, -4.2869, -4.3392, -4.2069,\n",
      "         -4.3613, -4.2409, -4.4973, -4.4868, -4.5770, -4.4441, -4.4298, -4.5339]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 104\n",
      "OUTPUTS tensor([[-4.2623, -4.2660, -4.3618, -4.4817, -4.2856, -4.5666, -4.5419, -4.5417,\n",
      "         -4.2536, -4.2456, -4.5876, -4.4901, -4.2977, -4.2670, -4.2375, -4.4329,\n",
      "         -4.3453, -4.5274, -4.3929, -4.4910, -4.5128, -4.2526, -4.4437, -4.2948,\n",
      "         -4.5672, -4.3212, -4.5116, -4.2750, -4.2987, -4.3481, -4.2829, -4.2513,\n",
      "         -4.4436, -4.2585, -4.5367, -4.5450, -4.2718, -4.2307, -4.4578, -4.5987,\n",
      "         -4.2373, -4.5239, -4.5444, -4.5181, -4.3056, -4.4530, -4.4405, -4.3112,\n",
      "         -4.2817, -4.2296, -4.2467, -4.3216, -4.2876, -4.4536, -4.4884, -4.5558,\n",
      "         -4.2171, -4.2799, -4.3444, -4.6001, -4.3864, -4.2669, -4.2503, -4.3180,\n",
      "         -4.5422, -4.5552, -4.4239, -4.5211, -4.2605, -4.2863, -4.3388, -4.2063,\n",
      "         -4.3609, -4.2403, -4.4960, -4.4850, -4.5802, -4.4425, -4.4310, -4.5370]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 105\n",
      "OUTPUTS tensor([[-4.2616, -4.2651, -4.3613, -4.4803, -4.2851, -4.5694, -4.5446, -4.5448,\n",
      "         -4.2531, -4.2440, -4.5909, -4.4890, -4.2972, -4.2662, -4.2371, -4.4323,\n",
      "         -4.3446, -4.5293, -4.3941, -4.4941, -4.5147, -4.2522, -4.4432, -4.2943,\n",
      "         -4.5706, -4.3206, -4.5140, -4.2746, -4.2981, -4.3454, -4.2821, -4.2509,\n",
      "         -4.4407, -4.2562, -4.5349, -4.5474, -4.2714, -4.2298, -4.4555, -4.6013,\n",
      "         -4.2366, -4.5265, -4.5462, -4.5204, -4.3051, -4.4507, -4.4384, -4.3108,\n",
      "         -4.2811, -4.2288, -4.2462, -4.3212, -4.2872, -4.4515, -4.4910, -4.5581,\n",
      "         -4.2163, -4.2793, -4.3440, -4.6032, -4.3853, -4.2663, -4.2498, -4.3175,\n",
      "         -4.5455, -4.5582, -4.4231, -4.5237, -4.2599, -4.2857, -4.3384, -4.2056,\n",
      "         -4.3605, -4.2395, -4.4948, -4.4833, -4.5836, -4.4409, -4.4327, -4.5401]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 106\n",
      "OUTPUTS tensor([[-4.2610, -4.2644, -4.3609, -4.4789, -4.2846, -4.5723, -4.5473, -4.5480,\n",
      "         -4.2527, -4.2425, -4.5942, -4.4880, -4.2968, -4.2654, -4.2367, -4.4317,\n",
      "         -4.3440, -4.5311, -4.3953, -4.4972, -4.5166, -4.2517, -4.4426, -4.2939,\n",
      "         -4.5741, -4.3201, -4.5164, -4.2742, -4.2976, -4.3429, -4.2813, -4.2505,\n",
      "         -4.4380, -4.2542, -4.5332, -4.5498, -4.2709, -4.2289, -4.4533, -4.6039,\n",
      "         -4.2359, -4.5291, -4.5480, -4.5227, -4.3045, -4.4486, -4.4365, -4.3104,\n",
      "         -4.2805, -4.2281, -4.2456, -4.3208, -4.2868, -4.4495, -4.4936, -4.5605,\n",
      "         -4.2156, -4.2787, -4.3435, -4.6063, -4.3842, -4.2657, -4.2492, -4.3171,\n",
      "         -4.5487, -4.5612, -4.4223, -4.5263, -4.2594, -4.2851, -4.3379, -4.2049,\n",
      "         -4.3577, -4.2387, -4.4936, -4.4817, -4.5869, -4.4393, -4.4343, -4.5432]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 107\n",
      "OUTPUTS tensor([[-4.2604, -4.2636, -4.3604, -4.4776, -4.2840, -4.5751, -4.5500, -4.5511,\n",
      "         -4.2523, -4.2411, -4.5974, -4.4870, -4.2963, -4.2646, -4.2363, -4.4312,\n",
      "         -4.3433, -4.5330, -4.3965, -4.5003, -4.5185, -4.2513, -4.4421, -4.2934,\n",
      "         -4.5775, -4.3195, -4.5188, -4.2737, -4.2970, -4.3406, -4.2806, -4.2501,\n",
      "         -4.4356, -4.2522, -4.5316, -4.5522, -4.2705, -4.2281, -4.4512, -4.6065,\n",
      "         -4.2352, -4.5317, -4.5497, -4.5250, -4.3040, -4.4466, -4.4347, -4.3100,\n",
      "         -4.2800, -4.2274, -4.2451, -4.3204, -4.2864, -4.4477, -4.4962, -4.5628,\n",
      "         -4.2149, -4.2763, -4.3431, -4.6094, -4.3832, -4.2652, -4.2486, -4.3166,\n",
      "         -4.5520, -4.5643, -4.4216, -4.5288, -4.2588, -4.2845, -4.3375, -4.2043,\n",
      "         -4.3552, -4.2380, -4.4925, -4.4803, -4.5902, -4.4379, -4.4359, -4.5464]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 108\n",
      "OUTPUTS tensor([[-4.2598, -4.2628, -4.3600, -4.4764, -4.2835, -4.5780, -4.5527, -4.5542,\n",
      "         -4.2519, -4.2398, -4.6007, -4.4861, -4.2959, -4.2639, -4.2359, -4.4307,\n",
      "         -4.3427, -4.5348, -4.3977, -4.5034, -4.5204, -4.2509, -4.4415, -4.2930,\n",
      "         -4.5810, -4.3190, -4.5212, -4.2733, -4.2964, -4.3384, -4.2799, -4.2497,\n",
      "         -4.4333, -4.2504, -4.5301, -4.5546, -4.2701, -4.2273, -4.4493, -4.6091,\n",
      "         -4.2345, -4.5342, -4.5515, -4.5273, -4.3035, -4.4448, -4.4330, -4.3096,\n",
      "         -4.2795, -4.2267, -4.2445, -4.3200, -4.2860, -4.4460, -4.4988, -4.5652,\n",
      "         -4.2142, -4.2741, -4.3403, -4.6124, -4.3822, -4.2646, -4.2480, -4.3162,\n",
      "         -4.5553, -4.5673, -4.4209, -4.5314, -4.2583, -4.2840, -4.3370, -4.2036,\n",
      "         -4.3529, -4.2373, -4.4915, -4.4789, -4.5935, -4.4365, -4.4375, -4.5495]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS tensor([[-4.2592, -4.2621, -4.3595, -4.4753, -4.2829, -4.5808, -4.5554, -4.5574,\n",
      "         -4.2515, -4.2385, -4.6039, -4.4852, -4.2954, -4.2631, -4.2332, -4.4302,\n",
      "         -4.3421, -4.5367, -4.3989, -4.5065, -4.5224, -4.2505, -4.4410, -4.2926,\n",
      "         -4.5844, -4.3184, -4.5237, -4.2729, -4.2959, -4.3364, -4.2792, -4.2493,\n",
      "         -4.4312, -4.2487, -4.5287, -4.5570, -4.2698, -4.2266, -4.4476, -4.6117,\n",
      "         -4.2339, -4.5368, -4.5533, -4.5296, -4.3030, -4.4431, -4.4314, -4.3092,\n",
      "         -4.2789, -4.2260, -4.2440, -4.3197, -4.2856, -4.4444, -4.5015, -4.5676,\n",
      "         -4.2136, -4.2720, -4.3378, -4.6155, -4.3813, -4.2641, -4.2475, -4.3158,\n",
      "         -4.5585, -4.5703, -4.4202, -4.5339, -4.2578, -4.2834, -4.3366, -4.2029,\n",
      "         -4.3507, -4.2366, -4.4906, -4.4776, -4.5968, -4.4352, -4.4392, -4.5526]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "[1,   110] loss: 0.022\n",
      "BATCH 110\n",
      "OUTPUTS tensor([[-4.2587, -4.2615, -4.3591, -4.4743, -4.2825, -4.5837, -4.5581, -4.5605,\n",
      "         -4.2512, -4.2373, -4.6072, -4.4845, -4.2950, -4.2625, -4.2307, -4.4297,\n",
      "         -4.3416, -4.5386, -4.4002, -4.5096, -4.5243, -4.2501, -4.4406, -4.2923,\n",
      "         -4.5879, -4.3179, -4.5261, -4.2726, -4.2954, -4.3346, -4.2786, -4.2490,\n",
      "         -4.4293, -4.2472, -4.5275, -4.5594, -4.2694, -4.2259, -4.4460, -4.6144,\n",
      "         -4.2333, -4.5395, -4.5551, -4.5320, -4.3025, -4.4416, -4.4300, -4.3088,\n",
      "         -4.2784, -4.2254, -4.2435, -4.3193, -4.2852, -4.4430, -4.5041, -4.5640,\n",
      "         -4.2129, -4.2701, -4.3355, -4.6186, -4.3804, -4.2635, -4.2470, -4.3154,\n",
      "         -4.5618, -4.5734, -4.4195, -4.5365, -4.2573, -4.2829, -4.3362, -4.2023,\n",
      "         -4.3488, -4.2360, -4.4897, -4.4765, -4.6002, -4.4341, -4.4409, -4.5557]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 111\n",
      "OUTPUTS tensor([[-4.2581, -4.2608, -4.3588, -4.4733, -4.2820, -4.5866, -4.5608, -4.5637,\n",
      "         -4.2508, -4.2362, -4.6104, -4.4837, -4.2947, -4.2618, -4.2284, -4.4292,\n",
      "         -4.3410, -4.5405, -4.4014, -4.5127, -4.5263, -4.2498, -4.4401, -4.2919,\n",
      "         -4.5913, -4.3174, -4.5286, -4.2722, -4.2949, -4.3329, -4.2780, -4.2486,\n",
      "         -4.4275, -4.2458, -4.5263, -4.5619, -4.2691, -4.2252, -4.4445, -4.6170,\n",
      "         -4.2327, -4.5421, -4.5569, -4.5343, -4.3020, -4.4402, -4.4287, -4.3085,\n",
      "         -4.2780, -4.2248, -4.2431, -4.3190, -4.2849, -4.4417, -4.5068, -4.5608,\n",
      "         -4.2123, -4.2684, -4.3334, -4.6217, -4.3795, -4.2630, -4.2448, -4.3150,\n",
      "         -4.5650, -4.5764, -4.4189, -4.5391, -4.2568, -4.2823, -4.3358, -4.2017,\n",
      "         -4.3470, -4.2354, -4.4889, -4.4754, -4.6035, -4.4330, -4.4426, -4.5588]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 112\n",
      "OUTPUTS tensor([[-4.2576, -4.2601, -4.3584, -4.4724, -4.2815, -4.5894, -4.5635, -4.5668,\n",
      "         -4.2505, -4.2351, -4.6136, -4.4830, -4.2943, -4.2612, -4.2263, -4.4288,\n",
      "         -4.3405, -4.5424, -4.4027, -4.5158, -4.5282, -4.2494, -4.4396, -4.2915,\n",
      "         -4.5947, -4.3169, -4.5310, -4.2719, -4.2944, -4.3314, -4.2774, -4.2483,\n",
      "         -4.4258, -4.2444, -4.5252, -4.5643, -4.2687, -4.2246, -4.4431, -4.6196,\n",
      "         -4.2321, -4.5447, -4.5587, -4.5367, -4.3015, -4.4389, -4.4274, -4.3081,\n",
      "         -4.2775, -4.2241, -4.2426, -4.3186, -4.2845, -4.4404, -4.5094, -4.5578,\n",
      "         -4.2117, -4.2667, -4.3314, -4.6247, -4.3787, -4.2625, -4.2427, -4.3147,\n",
      "         -4.5683, -4.5794, -4.4183, -4.5416, -4.2564, -4.2818, -4.3355, -4.1998,\n",
      "         -4.3453, -4.2348, -4.4881, -4.4744, -4.6068, -4.4319, -4.4443, -4.5619]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 113\n",
      "OUTPUTS tensor([[-4.2570, -4.2595, -4.3556, -4.4715, -4.2810, -4.5923, -4.5662, -4.5700,\n",
      "         -4.2501, -4.2341, -4.6168, -4.4823, -4.2939, -4.2606, -4.2244, -4.4283,\n",
      "         -4.3400, -4.5443, -4.4040, -4.5189, -4.5302, -4.2491, -4.4392, -4.2912,\n",
      "         -4.5981, -4.3164, -4.5334, -4.2715, -4.2939, -4.3299, -4.2768, -4.2479,\n",
      "         -4.4243, -4.2432, -4.5241, -4.5667, -4.2684, -4.2239, -4.4418, -4.6222,\n",
      "         -4.2315, -4.5473, -4.5605, -4.5390, -4.3011, -4.4377, -4.4262, -4.3078,\n",
      "         -4.2770, -4.2235, -4.2421, -4.3183, -4.2842, -4.4392, -4.5120, -4.5551,\n",
      "         -4.2112, -4.2652, -4.3296, -4.6278, -4.3779, -4.2620, -4.2408, -4.3143,\n",
      "         -4.5715, -4.5825, -4.4177, -4.5442, -4.2559, -4.2813, -4.3351, -4.1980,\n",
      "         -4.3437, -4.2342, -4.4873, -4.4734, -4.6101, -4.4309, -4.4460, -4.5650]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 114\n",
      "OUTPUTS tensor([[-4.2565, -4.2589, -4.3531, -4.4707, -4.2805, -4.5951, -4.5689, -4.5731,\n",
      "         -4.2498, -4.2331, -4.6200, -4.4816, -4.2935, -4.2600, -4.2226, -4.4279,\n",
      "         -4.3394, -4.5461, -4.4053, -4.5219, -4.5321, -4.2487, -4.4388, -4.2908,\n",
      "         -4.6014, -4.3160, -4.5359, -4.2712, -4.2934, -4.3285, -4.2763, -4.2476,\n",
      "         -4.4228, -4.2420, -4.5232, -4.5691, -4.2680, -4.2233, -4.4405, -4.6248,\n",
      "         -4.2309, -4.5499, -4.5623, -4.5414, -4.3006, -4.4365, -4.4251, -4.3074,\n",
      "         -4.2765, -4.2216, -4.2417, -4.3179, -4.2838, -4.4381, -4.5147, -4.5525,\n",
      "         -4.2106, -4.2638, -4.3279, -4.6308, -4.3772, -4.2615, -4.2390, -4.3140,\n",
      "         -4.5747, -4.5855, -4.4171, -4.5467, -4.2554, -4.2808, -4.3347, -4.1964,\n",
      "         -4.3422, -4.2336, -4.4866, -4.4725, -4.6133, -4.4300, -4.4478, -4.5680]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 115\n",
      "OUTPUTS tensor([[-4.2559, -4.2583, -4.3508, -4.4699, -4.2800, -4.5979, -4.5715, -4.5761,\n",
      "         -4.2495, -4.2322, -4.6232, -4.4810, -4.2931, -4.2594, -4.2209, -4.4275,\n",
      "         -4.3389, -4.5480, -4.4065, -4.5250, -4.5340, -4.2484, -4.4383, -4.2905,\n",
      "         -4.6048, -4.3155, -4.5383, -4.2708, -4.2929, -4.3272, -4.2757, -4.2473,\n",
      "         -4.4215, -4.2409, -4.5222, -4.5715, -4.2677, -4.2227, -4.4394, -4.6274,\n",
      "         -4.2304, -4.5525, -4.5641, -4.5437, -4.3001, -4.4355, -4.4240, -4.3071,\n",
      "         -4.2761, -4.2198, -4.2412, -4.3176, -4.2835, -4.4370, -4.5173, -4.5502,\n",
      "         -4.2084, -4.2624, -4.3263, -4.6338, -4.3764, -4.2610, -4.2374, -4.3136,\n",
      "         -4.5779, -4.5884, -4.4165, -4.5492, -4.2550, -4.2802, -4.3343, -4.1948,\n",
      "         -4.3409, -4.2331, -4.4859, -4.4717, -4.6165, -4.4291, -4.4495, -4.5711]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 116\n",
      "OUTPUTS tensor([[-4.2554, -4.2576, -4.3486, -4.4692, -4.2795, -4.6007, -4.5742, -4.5792,\n",
      "         -4.2491, -4.2313, -4.6263, -4.4804, -4.2928, -4.2571, -4.2194, -4.4270,\n",
      "         -4.3384, -4.5499, -4.4078, -4.5280, -4.5360, -4.2480, -4.4379, -4.2901,\n",
      "         -4.6081, -4.3150, -4.5407, -4.2705, -4.2925, -4.3260, -4.2752, -4.2469,\n",
      "         -4.4202, -4.2398, -4.5214, -4.5739, -4.2674, -4.2222, -4.4383, -4.6299,\n",
      "         -4.2298, -4.5551, -4.5659, -4.5460, -4.2996, -4.4344, -4.4230, -4.3067,\n",
      "         -4.2756, -4.2182, -4.2407, -4.3173, -4.2831, -4.4360, -4.5199, -4.5481,\n",
      "         -4.2063, -4.2612, -4.3248, -4.6368, -4.3757, -4.2605, -4.2358, -4.3132,\n",
      "         -4.5810, -4.5914, -4.4159, -4.5518, -4.2545, -4.2797, -4.3340, -4.1933,\n",
      "         -4.3396, -4.2326, -4.4853, -4.4708, -4.6198, -4.4282, -4.4512, -4.5741]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 117\n",
      "OUTPUTS tensor([[-4.2548, -4.2570, -4.3466, -4.4685, -4.2791, -4.6035, -4.5769, -4.5823,\n",
      "         -4.2488, -4.2304, -4.6295, -4.4798, -4.2924, -4.2550, -4.2179, -4.4266,\n",
      "         -4.3379, -4.5518, -4.4091, -4.5311, -4.5379, -4.2477, -4.4375, -4.2898,\n",
      "         -4.6114, -4.3145, -4.5431, -4.2702, -4.2920, -4.3249, -4.2747, -4.2466,\n",
      "         -4.4190, -4.2388, -4.5205, -4.5764, -4.2671, -4.2216, -4.4372, -4.6325,\n",
      "         -4.2292, -4.5577, -4.5677, -4.5484, -4.2992, -4.4335, -4.4221, -4.3064,\n",
      "         -4.2751, -4.2166, -4.2403, -4.3169, -4.2828, -4.4350, -4.5225, -4.5461,\n",
      "         -4.2044, -4.2600, -4.3235, -4.6398, -4.3750, -4.2600, -4.2344, -4.3129,\n",
      "         -4.5842, -4.5944, -4.4154, -4.5543, -4.2540, -4.2792, -4.3336, -4.1919,\n",
      "         -4.3384, -4.2301, -4.4846, -4.4701, -4.6230, -4.4274, -4.4530, -4.5772]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 118\n",
      "OUTPUTS tensor([[-4.2543, -4.2565, -4.3448, -4.4678, -4.2786, -4.6063, -4.5795, -4.5854,\n",
      "         -4.2485, -4.2296, -4.6326, -4.4792, -4.2921, -4.2531, -4.2166, -4.4262,\n",
      "         -4.3374, -4.5537, -4.4104, -4.5341, -4.5399, -4.2474, -4.4371, -4.2895,\n",
      "         -4.6147, -4.3141, -4.5455, -4.2698, -4.2915, -4.3239, -4.2741, -4.2463,\n",
      "         -4.4179, -4.2379, -4.5197, -4.5788, -4.2668, -4.2211, -4.4363, -4.6351,\n",
      "         -4.2287, -4.5603, -4.5695, -4.5507, -4.2987, -4.4326, -4.4212, -4.3061,\n",
      "         -4.2747, -4.2151, -4.2398, -4.3166, -4.2825, -4.4341, -4.5251, -4.5443,\n",
      "         -4.2027, -4.2589, -4.3222, -4.6428, -4.3744, -4.2595, -4.2330, -4.3126,\n",
      "         -4.5873, -4.5973, -4.4148, -4.5568, -4.2536, -4.2787, -4.3309, -4.1907,\n",
      "         -4.3373, -4.2279, -4.4840, -4.4694, -4.6262, -4.4266, -4.4547, -4.5802]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS tensor([[-4.2538, -4.2559, -4.3431, -4.4672, -4.2781, -4.6091, -4.5822, -4.5884,\n",
      "         -4.2482, -4.2288, -4.6357, -4.4787, -4.2917, -4.2500, -4.2153, -4.4258,\n",
      "         -4.3369, -4.5556, -4.4117, -4.5371, -4.5418, -4.2470, -4.4367, -4.2892,\n",
      "         -4.6180, -4.3136, -4.5480, -4.2695, -4.2911, -4.3229, -4.2736, -4.2460,\n",
      "         -4.4168, -4.2369, -4.5190, -4.5812, -4.2664, -4.2205, -4.4353, -4.6376,\n",
      "         -4.2282, -4.5628, -4.5713, -4.5530, -4.2983, -4.4318, -4.4203, -4.3057,\n",
      "         -4.2742, -4.2138, -4.2394, -4.3163, -4.2822, -4.4333, -4.5277, -4.5426,\n",
      "         -4.2010, -4.2578, -4.3210, -4.6458, -4.3737, -4.2590, -4.2318, -4.3122,\n",
      "         -4.5904, -4.6003, -4.4143, -4.5593, -4.2532, -4.2782, -4.3283, -4.1894,\n",
      "         -4.3362, -4.2258, -4.4835, -4.4687, -4.6293, -4.4259, -4.4565, -4.5832]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "[1,   120] loss: 0.021\n",
      "BATCH 120\n",
      "OUTPUTS tensor([[-4.2533, -4.2553, -4.3416, -4.4666, -4.2777, -4.6119, -4.5849, -4.5915,\n",
      "         -4.2479, -4.2281, -4.6388, -4.4782, -4.2914, -4.2472, -4.2142, -4.4255,\n",
      "         -4.3365, -4.5575, -4.4082, -4.5401, -4.5438, -4.2468, -4.4363, -4.2889,\n",
      "         -4.6212, -4.3132, -4.5504, -4.2692, -4.2906, -4.3220, -4.2732, -4.2457,\n",
      "         -4.4159, -4.2361, -4.5183, -4.5836, -4.2662, -4.2201, -4.4345, -4.6402,\n",
      "         -4.2277, -4.5654, -4.5732, -4.5553, -4.2979, -4.4310, -4.4195, -4.3055,\n",
      "         -4.2738, -4.2125, -4.2390, -4.3160, -4.2819, -4.4325, -4.5304, -4.5411,\n",
      "         -4.1995, -4.2569, -4.3199, -4.6488, -4.3731, -4.2585, -4.2306, -4.3119,\n",
      "         -4.5936, -4.6032, -4.4138, -4.5618, -4.2528, -4.2777, -4.3260, -4.1883,\n",
      "         -4.3352, -4.2239, -4.4829, -4.4680, -4.6325, -4.4252, -4.4582, -4.5862]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 121\n",
      "OUTPUTS tensor([[-4.2528, -4.2548, -4.3401, -4.4661, -4.2773, -4.6147, -4.5875, -4.5945,\n",
      "         -4.2477, -4.2273, -4.6419, -4.4777, -4.2911, -4.2445, -4.2132, -4.4251,\n",
      "         -4.3360, -4.5594, -4.4050, -4.5431, -4.5458, -4.2465, -4.4359, -4.2886,\n",
      "         -4.6245, -4.3127, -4.5528, -4.2689, -4.2902, -4.3211, -4.2727, -4.2455,\n",
      "         -4.4149, -4.2353, -4.5177, -4.5860, -4.2659, -4.2196, -4.4336, -4.6428,\n",
      "         -4.2272, -4.5680, -4.5750, -4.5577, -4.2974, -4.4303, -4.4169, -4.3052,\n",
      "         -4.2734, -4.2113, -4.2386, -4.3157, -4.2816, -4.4317, -4.5330, -4.5397,\n",
      "         -4.1981, -4.2560, -4.3189, -4.6517, -4.3725, -4.2581, -4.2295, -4.3116,\n",
      "         -4.5967, -4.6062, -4.4133, -4.5644, -4.2523, -4.2773, -4.3239, -4.1872,\n",
      "         -4.3343, -4.2221, -4.4824, -4.4674, -4.6357, -4.4245, -4.4600, -4.5892]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 122\n",
      "OUTPUTS tensor([[-4.2523, -4.2543, -4.3388, -4.4655, -4.2769, -4.6175, -4.5902, -4.5975,\n",
      "         -4.2474, -4.2266, -4.6450, -4.4772, -4.2908, -4.2421, -4.2122, -4.4247,\n",
      "         -4.3356, -4.5613, -4.4021, -4.5461, -4.5477, -4.2462, -4.4356, -4.2883,\n",
      "         -4.6278, -4.3123, -4.5552, -4.2687, -4.2898, -4.3203, -4.2723, -4.2452,\n",
      "         -4.4141, -4.2346, -4.5171, -4.5884, -4.2656, -4.2191, -4.4329, -4.6453,\n",
      "         -4.2267, -4.5706, -4.5768, -4.5600, -4.2970, -4.4297, -4.4145, -4.3049,\n",
      "         -4.2730, -4.2102, -4.2382, -4.3154, -4.2789, -4.4310, -4.5356, -4.5384,\n",
      "         -4.1968, -4.2551, -4.3179, -4.6547, -4.3719, -4.2576, -4.2285, -4.3114,\n",
      "         -4.5998, -4.6091, -4.4128, -4.5669, -4.2520, -4.2768, -4.3219, -4.1862,\n",
      "         -4.3335, -4.2205, -4.4819, -4.4669, -4.6388, -4.4239, -4.4618, -4.5922]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 123\n",
      "OUTPUTS tensor([[-4.2519, -4.2538, -4.3376, -4.4651, -4.2765, -4.6203, -4.5929, -4.6006,\n",
      "         -4.2472, -4.2260, -4.6481, -4.4768, -4.2905, -4.2399, -4.2113, -4.4244,\n",
      "         -4.3352, -4.5633, -4.3994, -4.5492, -4.5497, -4.2460, -4.4353, -4.2881,\n",
      "         -4.6310, -4.3119, -4.5577, -4.2684, -4.2894, -4.3195, -4.2719, -4.2450,\n",
      "         -4.4133, -4.2339, -4.5165, -4.5909, -4.2654, -4.2187, -4.4322, -4.6479,\n",
      "         -4.2263, -4.5732, -4.5732, -4.5624, -4.2967, -4.4291, -4.4123, -4.3047,\n",
      "         -4.2726, -4.2092, -4.2378, -4.3152, -4.2764, -4.4303, -4.5383, -4.5372,\n",
      "         -4.1957, -4.2543, -4.3171, -4.6577, -4.3714, -4.2572, -4.2275, -4.3111,\n",
      "         -4.6029, -4.6120, -4.4124, -4.5694, -4.2516, -4.2764, -4.3202, -4.1853,\n",
      "         -4.3327, -4.2191, -4.4815, -4.4664, -4.6420, -4.4233, -4.4637, -4.5952]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 124\n",
      "OUTPUTS tensor([[-4.2514, -4.2533, -4.3364, -4.4646, -4.2761, -4.6231, -4.5956, -4.6036,\n",
      "         -4.2470, -4.2253, -4.6512, -4.4764, -4.2902, -4.2366, -4.2104, -4.4241,\n",
      "         -4.3348, -4.5652, -4.3970, -4.5522, -4.5517, -4.2458, -4.4349, -4.2878,\n",
      "         -4.6343, -4.3116, -4.5601, -4.2682, -4.2890, -4.3188, -4.2715, -4.2448,\n",
      "         -4.4125, -4.2332, -4.5160, -4.5933, -4.2652, -4.2183, -4.4315, -4.6505,\n",
      "         -4.2259, -4.5759, -4.5700, -4.5648, -4.2963, -4.4285, -4.4102, -4.3044,\n",
      "         -4.2722, -4.2082, -4.2375, -4.3149, -4.2741, -4.4297, -4.5409, -4.5361,\n",
      "         -4.1945, -4.2536, -4.3162, -4.6606, -4.3708, -4.2568, -4.2267, -4.3109,\n",
      "         -4.6060, -4.6150, -4.4119, -4.5720, -4.2512, -4.2760, -4.3185, -4.1844,\n",
      "         -4.3319, -4.2177, -4.4811, -4.4659, -4.6452, -4.4227, -4.4655, -4.5982]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 125\n",
      "OUTPUTS tensor([[-4.2510, -4.2528, -4.3354, -4.4642, -4.2757, -4.6259, -4.5983, -4.6067,\n",
      "         -4.2468, -4.2247, -4.6543, -4.4760, -4.2900, -4.2336, -4.2097, -4.4238,\n",
      "         -4.3344, -4.5672, -4.3948, -4.5552, -4.5537, -4.2455, -4.4346, -4.2851,\n",
      "         -4.6375, -4.3112, -4.5626, -4.2680, -4.2887, -4.3182, -4.2711, -4.2446,\n",
      "         -4.4118, -4.2326, -4.5155, -4.5957, -4.2650, -4.2179, -4.4308, -4.6531,\n",
      "         -4.2254, -4.5785, -4.5670, -4.5671, -4.2959, -4.4280, -4.4083, -4.3042,\n",
      "         -4.2719, -4.2073, -4.2371, -4.3147, -4.2721, -4.4291, -4.5436, -4.5351,\n",
      "         -4.1935, -4.2529, -4.3155, -4.6636, -4.3703, -4.2564, -4.2258, -4.3106,\n",
      "         -4.6091, -4.6179, -4.4115, -4.5745, -4.2509, -4.2756, -4.3170, -4.1836,\n",
      "         -4.3312, -4.2164, -4.4807, -4.4654, -4.6483, -4.4222, -4.4673, -4.6012]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 126\n",
      "OUTPUTS tensor([[-4.2505, -4.2524, -4.3325, -4.4637, -4.2753, -4.6287, -4.6009, -4.6097,\n",
      "         -4.2466, -4.2241, -4.6573, -4.4756, -4.2897, -4.2309, -4.2089, -4.4235,\n",
      "         -4.3340, -4.5691, -4.3928, -4.5581, -4.5557, -4.2453, -4.4343, -4.2826,\n",
      "         -4.6407, -4.3108, -4.5650, -4.2677, -4.2883, -4.3175, -4.2707, -4.2444,\n",
      "         -4.4112, -4.2320, -4.5150, -4.5982, -4.2648, -4.2175, -4.4302, -4.6556,\n",
      "         -4.2250, -4.5811, -4.5642, -4.5695, -4.2956, -4.4274, -4.4066, -4.3039,\n",
      "         -4.2715, -4.2064, -4.2368, -4.3145, -4.2701, -4.4285, -4.5462, -4.5342,\n",
      "         -4.1925, -4.2522, -4.3147, -4.6665, -4.3698, -4.2560, -4.2250, -4.3104,\n",
      "         -4.6122, -4.6208, -4.4111, -4.5770, -4.2505, -4.2751, -4.3156, -4.1828,\n",
      "         -4.3306, -4.2152, -4.4803, -4.4649, -4.6514, -4.4216, -4.4692, -4.6041]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 127\n",
      "OUTPUTS tensor([[-4.2501, -4.2519, -4.3300, -4.4634, -4.2750, -4.6315, -4.6036, -4.6127,\n",
      "         -4.2464, -4.2236, -4.6604, -4.4753, -4.2895, -4.2284, -4.2083, -4.4233,\n",
      "         -4.3337, -4.5711, -4.3910, -4.5611, -4.5578, -4.2451, -4.4341, -4.2804,\n",
      "         -4.6439, -4.3105, -4.5675, -4.2676, -4.2880, -4.3170, -4.2704, -4.2442,\n",
      "         -4.4105, -4.2314, -4.5146, -4.6006, -4.2646, -4.2172, -4.4297, -4.6582,\n",
      "         -4.2246, -4.5837, -4.5618, -4.5719, -4.2953, -4.4270, -4.4050, -4.3038,\n",
      "         -4.2712, -4.2056, -4.2365, -4.3143, -4.2684, -4.4280, -4.5488, -4.5333,\n",
      "         -4.1916, -4.2516, -4.3141, -4.6629, -4.3694, -4.2556, -4.2243, -4.3102,\n",
      "         -4.6153, -4.6238, -4.4107, -4.5796, -4.2502, -4.2748, -4.3144, -4.1821,\n",
      "         -4.3300, -4.2142, -4.4799, -4.4645, -4.6546, -4.4211, -4.4710, -4.6071]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 128\n",
      "OUTPUTS tensor([[-4.2497, -4.2515, -4.3276, -4.4630, -4.2747, -4.6343, -4.6063, -4.6157,\n",
      "         -4.2463, -4.2230, -4.6635, -4.4750, -4.2893, -4.2261, -4.2076, -4.4230,\n",
      "         -4.3333, -4.5731, -4.3893, -4.5641, -4.5598, -4.2450, -4.4338, -4.2783,\n",
      "         -4.6471, -4.3102, -4.5700, -4.2674, -4.2877, -4.3164, -4.2700, -4.2441,\n",
      "         -4.4100, -4.2309, -4.5142, -4.6031, -4.2644, -4.2168, -4.4291, -4.6608,\n",
      "         -4.2243, -4.5863, -4.5595, -4.5743, -4.2949, -4.4266, -4.4036, -4.3036,\n",
      "         -4.2709, -4.2048, -4.2362, -4.3141, -4.2668, -4.4274, -4.5515, -4.5325,\n",
      "         -4.1908, -4.2510, -4.3135, -4.6596, -4.3689, -4.2552, -4.2236, -4.3100,\n",
      "         -4.6184, -4.6267, -4.4103, -4.5821, -4.2499, -4.2744, -4.3132, -4.1814,\n",
      "         -4.3275, -4.2132, -4.4796, -4.4642, -4.6577, -4.4207, -4.4729, -4.6101]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS tensor([[-4.2493, -4.2511, -4.3239, -4.4627, -4.2743, -4.6371, -4.6090, -4.6187,\n",
      "         -4.2461, -4.2225, -4.6665, -4.4747, -4.2891, -4.2240, -4.2071, -4.4228,\n",
      "         -4.3330, -4.5751, -4.3878, -4.5671, -4.5618, -4.2448, -4.4336, -4.2764,\n",
      "         -4.6503, -4.3098, -4.5724, -4.2672, -4.2874, -4.3159, -4.2697, -4.2439,\n",
      "         -4.4094, -4.2304, -4.5138, -4.6055, -4.2643, -4.2165, -4.4286, -4.6634,\n",
      "         -4.2239, -4.5889, -4.5575, -4.5767, -4.2946, -4.4262, -4.4022, -4.3034,\n",
      "         -4.2706, -4.2041, -4.2359, -4.3139, -4.2654, -4.4269, -4.5541, -4.5318,\n",
      "         -4.1900, -4.2504, -4.3129, -4.6565, -4.3685, -4.2549, -4.2229, -4.3098,\n",
      "         -4.6214, -4.6296, -4.4099, -4.5847, -4.2496, -4.2740, -4.3121, -4.1807,\n",
      "         -4.3251, -4.2122, -4.4792, -4.4638, -4.6608, -4.4202, -4.4748, -4.6131]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "[1,   130] loss: 0.022\n",
      "BATCH 130\n",
      "OUTPUTS tensor([[-4.2490, -4.2507, -4.3206, -4.4624, -4.2740, -4.6399, -4.6117, -4.6217,\n",
      "         -4.2460, -4.2220, -4.6696, -4.4744, -4.2889, -4.2220, -4.2065, -4.4226,\n",
      "         -4.3327, -4.5771, -4.3864, -4.5701, -4.5639, -4.2446, -4.4334, -4.2747,\n",
      "         -4.6535, -4.3095, -4.5749, -4.2671, -4.2871, -4.3154, -4.2694, -4.2438,\n",
      "         -4.4089, -4.2300, -4.5134, -4.6080, -4.2641, -4.2162, -4.4281, -4.6660,\n",
      "         -4.2235, -4.5915, -4.5556, -4.5791, -4.2943, -4.4258, -4.4010, -4.3032,\n",
      "         -4.2703, -4.2034, -4.2356, -4.3137, -4.2640, -4.4265, -4.5506, -4.5311,\n",
      "         -4.1893, -4.2499, -4.3123, -4.6538, -4.3681, -4.2545, -4.2224, -4.3097,\n",
      "         -4.6245, -4.6325, -4.4096, -4.5872, -4.2494, -4.2737, -4.3112, -4.1801,\n",
      "         -4.3230, -4.2114, -4.4790, -4.4635, -4.6639, -4.4198, -4.4767, -4.6160]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 131\n",
      "OUTPUTS tensor([[-4.2486, -4.2503, -4.3176, -4.4621, -4.2737, -4.6427, -4.6144, -4.6248,\n",
      "         -4.2459, -4.2215, -4.6727, -4.4741, -4.2888, -4.2203, -4.2060, -4.4224,\n",
      "         -4.3324, -4.5791, -4.3852, -4.5731, -4.5660, -4.2445, -4.4332, -4.2731,\n",
      "         -4.6567, -4.3093, -4.5774, -4.2669, -4.2868, -4.3150, -4.2691, -4.2437,\n",
      "         -4.4084, -4.2295, -4.5131, -4.6105, -4.2640, -4.2159, -4.4277, -4.6686,\n",
      "         -4.2232, -4.5942, -4.5539, -4.5815, -4.2941, -4.4255, -4.3998, -4.3031,\n",
      "         -4.2700, -4.2028, -4.2354, -4.3136, -4.2628, -4.4261, -4.5473, -4.5305,\n",
      "         -4.1871, -4.2494, -4.3118, -4.6513, -4.3677, -4.2542, -4.2218, -4.3095,\n",
      "         -4.6276, -4.6355, -4.4093, -4.5898, -4.2491, -4.2734, -4.3102, -4.1795,\n",
      "         -4.3211, -4.2106, -4.4787, -4.4632, -4.6670, -4.4194, -4.4786, -4.6190]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 132\n",
      "OUTPUTS tensor([[-4.2483, -4.2500, -4.3149, -4.4618, -4.2735, -4.6455, -4.6171, -4.6278,\n",
      "         -4.2458, -4.2211, -4.6757, -4.4739, -4.2887, -4.2187, -4.2056, -4.4222,\n",
      "         -4.3321, -4.5812, -4.3841, -4.5761, -4.5681, -4.2444, -4.4330, -4.2717,\n",
      "         -4.6531, -4.3090, -4.5799, -4.2668, -4.2866, -4.3146, -4.2689, -4.2436,\n",
      "         -4.4080, -4.2291, -4.5128, -4.6130, -4.2639, -4.2157, -4.4273, -4.6712,\n",
      "         -4.2229, -4.5968, -4.5524, -4.5839, -4.2938, -4.4252, -4.3988, -4.3030,\n",
      "         -4.2698, -4.2022, -4.2352, -4.3135, -4.2618, -4.4257, -4.5444, -4.5299,\n",
      "         -4.1851, -4.2490, -4.3114, -4.6491, -4.3673, -4.2539, -4.2213, -4.3094,\n",
      "         -4.6307, -4.6384, -4.4090, -4.5924, -4.2489, -4.2731, -4.3094, -4.1789,\n",
      "         -4.3194, -4.2099, -4.4784, -4.4629, -4.6701, -4.4191, -4.4805, -4.6220]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 133\n",
      "OUTPUTS tensor([[-4.2480, -4.2497, -4.3124, -4.4616, -4.2733, -4.6484, -4.6135, -4.6308,\n",
      "         -4.2457, -4.2207, -4.6788, -4.4737, -4.2886, -4.2172, -4.2052, -4.4221,\n",
      "         -4.3319, -4.5832, -4.3831, -4.5791, -4.5702, -4.2444, -4.4329, -4.2705,\n",
      "         -4.6498, -4.3088, -4.5824, -4.2668, -4.2864, -4.3143, -4.2686, -4.2435,\n",
      "         -4.4076, -4.2288, -4.5125, -4.6155, -4.2639, -4.2155, -4.4269, -4.6739,\n",
      "         -4.2227, -4.5995, -4.5510, -4.5864, -4.2936, -4.4249, -4.3979, -4.3029,\n",
      "         -4.2696, -4.2017, -4.2350, -4.3134, -4.2608, -4.4253, -4.5417, -4.5295,\n",
      "         -4.1833, -4.2486, -4.3110, -4.6471, -4.3670, -4.2537, -4.2208, -4.3094,\n",
      "         -4.6338, -4.6414, -4.4087, -4.5950, -4.2487, -4.2728, -4.3087, -4.1785,\n",
      "         -4.3178, -4.2093, -4.4783, -4.4627, -4.6733, -4.4188, -4.4825, -4.6250]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 134\n",
      "OUTPUTS tensor([[-4.2477, -4.2494, -4.3101, -4.4614, -4.2730, -4.6512, -4.6103, -4.6339,\n",
      "         -4.2457, -4.2203, -4.6819, -4.4735, -4.2885, -4.2159, -4.2049, -4.4219,\n",
      "         -4.3317, -4.5853, -4.3822, -4.5821, -4.5723, -4.2443, -4.4327, -4.2693,\n",
      "         -4.6469, -4.3086, -4.5850, -4.2667, -4.2862, -4.3139, -4.2684, -4.2435,\n",
      "         -4.4072, -4.2284, -4.5123, -4.6181, -4.2638, -4.2153, -4.4266, -4.6765,\n",
      "         -4.2224, -4.6022, -4.5498, -4.5888, -4.2934, -4.4247, -4.3970, -4.3028,\n",
      "         -4.2694, -4.2012, -4.2348, -4.3134, -4.2599, -4.4250, -4.5375, -4.5290,\n",
      "         -4.1816, -4.2482, -4.3106, -4.6452, -4.3667, -4.2534, -4.2204, -4.3093,\n",
      "         -4.6368, -4.6443, -4.4085, -4.5976, -4.2485, -4.2726, -4.3081, -4.1780,\n",
      "         -4.3163, -4.2087, -4.4781, -4.4625, -4.6764, -4.4185, -4.4845, -4.6280]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 135\n",
      "OUTPUTS tensor([[-4.2474, -4.2491, -4.3081, -4.4612, -4.2728, -4.6540, -4.6073, -4.6369,\n",
      "         -4.2457, -4.2200, -4.6850, -4.4734, -4.2884, -4.2146, -4.2046, -4.4218,\n",
      "         -4.3315, -4.5874, -4.3814, -4.5851, -4.5745, -4.2443, -4.4326, -4.2683,\n",
      "         -4.6442, -4.3063, -4.5875, -4.2666, -4.2860, -4.3136, -4.2682, -4.2435,\n",
      "         -4.4069, -4.2281, -4.5121, -4.6206, -4.2638, -4.2151, -4.4262, -4.6792,\n",
      "         -4.2222, -4.6049, -4.5486, -4.5913, -4.2932, -4.4245, -4.3962, -4.3028,\n",
      "         -4.2692, -4.2007, -4.2346, -4.3133, -4.2591, -4.4247, -4.5336, -4.5286,\n",
      "         -4.1801, -4.2479, -4.3103, -4.6436, -4.3664, -4.2532, -4.2200, -4.3092,\n",
      "         -4.6399, -4.6473, -4.4082, -4.6002, -4.2483, -4.2723, -4.3074, -4.1775,\n",
      "         -4.3150, -4.2081, -4.4779, -4.4623, -4.6795, -4.4182, -4.4865, -4.6310]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 136\n",
      "OUTPUTS tensor([[-4.2472, -4.2488, -4.3062, -4.4611, -4.2726, -4.6569, -4.6047, -4.6399,\n",
      "         -4.2456, -4.2196, -4.6880, -4.4732, -4.2883, -4.2135, -4.2042, -4.4217,\n",
      "         -4.3313, -4.5895, -4.3807, -4.5881, -4.5766, -4.2442, -4.4325, -4.2673,\n",
      "         -4.6418, -4.3042, -4.5900, -4.2666, -4.2858, -4.3133, -4.2680, -4.2434,\n",
      "         -4.4065, -4.2278, -4.5119, -4.6231, -4.2638, -4.2149, -4.4259, -4.6818,\n",
      "         -4.2219, -4.6075, -4.5476, -4.5937, -4.2910, -4.4243, -4.3954, -4.3027,\n",
      "         -4.2690, -4.2002, -4.2344, -4.3132, -4.2583, -4.4244, -4.5301, -4.5282,\n",
      "         -4.1787, -4.2475, -4.3099, -4.6420, -4.3661, -4.2529, -4.2196, -4.3092,\n",
      "         -4.6430, -4.6502, -4.4080, -4.6028, -4.2482, -4.2721, -4.3069, -4.1771,\n",
      "         -4.3138, -4.2076, -4.4777, -4.4621, -4.6826, -4.4179, -4.4885, -4.6340]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 137\n",
      "OUTPUTS tensor([[-4.2469, -4.2485, -4.3045, -4.4609, -4.2724, -4.6597, -4.6022, -4.6429,\n",
      "         -4.2456, -4.2192, -4.6910, -4.4730, -4.2882, -4.2124, -4.2019, -4.4216,\n",
      "         -4.3311, -4.5916, -4.3800, -4.5911, -4.5787, -4.2442, -4.4324, -4.2664,\n",
      "         -4.6396, -4.3023, -4.5925, -4.2666, -4.2856, -4.3130, -4.2679, -4.2434,\n",
      "         -4.4062, -4.2275, -4.5117, -4.6256, -4.2637, -4.2147, -4.4256, -4.6844,\n",
      "         -4.2217, -4.6102, -4.5466, -4.5962, -4.2889, -4.4241, -4.3947, -4.3027,\n",
      "         -4.2688, -4.1998, -4.2343, -4.3132, -4.2576, -4.4241, -4.5269, -4.5279,\n",
      "         -4.1774, -4.2472, -4.3096, -4.6406, -4.3658, -4.2527, -4.2192, -4.3091,\n",
      "         -4.6460, -4.6531, -4.4078, -4.6054, -4.2480, -4.2719, -4.3063, -4.1767,\n",
      "         -4.3127, -4.2071, -4.4776, -4.4619, -4.6857, -4.4176, -4.4905, -4.6369]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 138\n",
      "OUTPUTS tensor([[-4.2466, -4.2483, -4.3029, -4.4607, -4.2722, -4.6625, -4.6000, -4.6459,\n",
      "         -4.2456, -4.2189, -4.6941, -4.4703, -4.2882, -4.2114, -4.1998, -4.4215,\n",
      "         -4.3309, -4.5936, -4.3794, -4.5941, -4.5809, -4.2442, -4.4323, -4.2656,\n",
      "         -4.6375, -4.3006, -4.5950, -4.2665, -4.2854, -4.3128, -4.2677, -4.2434,\n",
      "         -4.4059, -4.2272, -4.5115, -4.6281, -4.2637, -4.2145, -4.4253, -4.6870,\n",
      "         -4.2215, -4.6128, -4.5458, -4.5986, -4.2870, -4.4239, -4.3941, -4.3026,\n",
      "         -4.2687, -4.1993, -4.2341, -4.3131, -4.2570, -4.4238, -4.5240, -4.5275,\n",
      "         -4.1762, -4.2469, -4.3093, -4.6394, -4.3655, -4.2525, -4.2188, -4.3091,\n",
      "         -4.6491, -4.6560, -4.4075, -4.6080, -4.2478, -4.2716, -4.3058, -4.1763,\n",
      "         -4.3117, -4.2067, -4.4774, -4.4618, -4.6888, -4.4173, -4.4925, -4.6399]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS tensor([[-4.2464, -4.2480, -4.3014, -4.4606, -4.2720, -4.6653, -4.5980, -4.6489,\n",
      "         -4.2456, -4.2185, -4.6971, -4.4678, -4.2881, -4.2105, -4.1978, -4.4214,\n",
      "         -4.3307, -4.5957, -4.3788, -4.5970, -4.5830, -4.2441, -4.4322, -4.2648,\n",
      "         -4.6357, -4.2990, -4.5975, -4.2665, -4.2852, -4.3125, -4.2675, -4.2434,\n",
      "         -4.4056, -4.2269, -4.5113, -4.6306, -4.2637, -4.2144, -4.4250, -4.6897,\n",
      "         -4.2212, -4.6155, -4.5450, -4.6011, -4.2852, -4.4238, -4.3934, -4.3026,\n",
      "         -4.2685, -4.1989, -4.2340, -4.3131, -4.2564, -4.4235, -4.5213, -4.5272,\n",
      "         -4.1751, -4.2466, -4.3091, -4.6382, -4.3652, -4.2504, -4.2185, -4.3090,\n",
      "         -4.6521, -4.6589, -4.4073, -4.6105, -4.2477, -4.2714, -4.3054, -4.1759,\n",
      "         -4.3107, -4.2062, -4.4773, -4.4616, -4.6919, -4.4171, -4.4944, -4.6428]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "[1,   140] loss: 0.022\n",
      "BATCH 140\n",
      "OUTPUTS tensor([[-4.2461, -4.2477, -4.3001, -4.4605, -4.2718, -4.6681, -4.5962, -4.6519,\n",
      "         -4.2456, -4.2182, -4.7001, -4.4656, -4.2881, -4.2097, -4.1960, -4.4213,\n",
      "         -4.3305, -4.5978, -4.3783, -4.6000, -4.5851, -4.2441, -4.4321, -4.2642,\n",
      "         -4.6340, -4.2976, -4.6001, -4.2665, -4.2851, -4.3123, -4.2673, -4.2434,\n",
      "         -4.4053, -4.2267, -4.5111, -4.6331, -4.2637, -4.2142, -4.4248, -4.6923,\n",
      "         -4.2210, -4.6181, -4.5442, -4.6035, -4.2836, -4.4236, -4.3929, -4.2999,\n",
      "         -4.2683, -4.1985, -4.2338, -4.3130, -4.2559, -4.4233, -4.5189, -4.5269,\n",
      "         -4.1741, -4.2463, -4.3088, -4.6371, -4.3649, -4.2485, -4.2182, -4.3090,\n",
      "         -4.6551, -4.6618, -4.4071, -4.6131, -4.2475, -4.2712, -4.3049, -4.1755,\n",
      "         -4.3098, -4.2058, -4.4771, -4.4615, -4.6949, -4.4168, -4.4964, -4.6458]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 141\n",
      "OUTPUTS tensor([[-4.2459, -4.2475, -4.2988, -4.4603, -4.2716, -4.6708, -4.5945, -4.6548,\n",
      "         -4.2456, -4.2179, -4.7031, -4.4635, -4.2880, -4.2089, -4.1944, -4.4212,\n",
      "         -4.3303, -4.5999, -4.3778, -4.6029, -4.5873, -4.2441, -4.4293, -4.2635,\n",
      "         -4.6325, -4.2962, -4.6026, -4.2664, -4.2849, -4.3120, -4.2672, -4.2434,\n",
      "         -4.4051, -4.2264, -4.5110, -4.6356, -4.2637, -4.2141, -4.4245, -4.6949,\n",
      "         -4.2208, -4.6208, -4.5436, -4.6060, -4.2822, -4.4235, -4.3923, -4.2975,\n",
      "         -4.2682, -4.1981, -4.2337, -4.3130, -4.2554, -4.4230, -4.5167, -4.5266,\n",
      "         -4.1732, -4.2460, -4.3085, -4.6362, -4.3647, -4.2468, -4.2179, -4.3090,\n",
      "         -4.6581, -4.6647, -4.4069, -4.6157, -4.2474, -4.2710, -4.3045, -4.1752,\n",
      "         -4.3090, -4.2055, -4.4770, -4.4613, -4.6980, -4.4166, -4.4984, -4.6487]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 142\n",
      "OUTPUTS tensor([[-4.2456, -4.2472, -4.2977, -4.4602, -4.2715, -4.6736, -4.5930, -4.6578,\n",
      "         -4.2456, -4.2176, -4.7061, -4.4617, -4.2880, -4.2081, -4.1929, -4.4211,\n",
      "         -4.3301, -4.6020, -4.3774, -4.6058, -4.5894, -4.2441, -4.4269, -4.2630,\n",
      "         -4.6311, -4.2950, -4.6051, -4.2664, -4.2847, -4.3118, -4.2670, -4.2434,\n",
      "         -4.4048, -4.2262, -4.5108, -4.6381, -4.2637, -4.2139, -4.4243, -4.6975,\n",
      "         -4.2206, -4.6234, -4.5430, -4.6084, -4.2809, -4.4234, -4.3918, -4.2953,\n",
      "         -4.2680, -4.1978, -4.2314, -4.3130, -4.2549, -4.4228, -4.5147, -4.5264,\n",
      "         -4.1723, -4.2458, -4.3083, -4.6353, -4.3644, -4.2453, -4.2176, -4.3089,\n",
      "         -4.6611, -4.6676, -4.4067, -4.6183, -4.2472, -4.2708, -4.3042, -4.1748,\n",
      "         -4.3083, -4.2051, -4.4769, -4.4612, -4.7010, -4.4164, -4.5005, -4.6516]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 143\n",
      "OUTPUTS tensor([[-4.2454, -4.2470, -4.2967, -4.4601, -4.2713, -4.6764, -4.5917, -4.6607,\n",
      "         -4.2456, -4.2173, -4.7091, -4.4600, -4.2879, -4.2075, -4.1916, -4.4211,\n",
      "         -4.3300, -4.6041, -4.3771, -4.6088, -4.5915, -4.2441, -4.4246, -4.2624,\n",
      "         -4.6299, -4.2939, -4.6076, -4.2664, -4.2846, -4.3095, -4.2669, -4.2434,\n",
      "         -4.4046, -4.2259, -4.5107, -4.6406, -4.2637, -4.2138, -4.4240, -4.7001,\n",
      "         -4.2204, -4.6260, -4.5424, -4.6108, -4.2796, -4.4232, -4.3913, -4.2933,\n",
      "         -4.2679, -4.1974, -4.2293, -4.3130, -4.2545, -4.4226, -4.5129, -4.5262,\n",
      "         -4.1715, -4.2455, -4.3081, -4.6345, -4.3642, -4.2438, -4.2173, -4.3089,\n",
      "         -4.6641, -4.6705, -4.4065, -4.6208, -4.2471, -4.2706, -4.3038, -4.1745,\n",
      "         -4.3076, -4.2048, -4.4768, -4.4611, -4.7040, -4.4162, -4.5025, -4.6546]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 144\n",
      "OUTPUTS tensor([[-4.2452, -4.2467, -4.2957, -4.4600, -4.2711, -4.6792, -4.5904, -4.6637,\n",
      "         -4.2456, -4.2170, -4.7120, -4.4584, -4.2879, -4.2068, -4.1903, -4.4210,\n",
      "         -4.3298, -4.6062, -4.3767, -4.6117, -4.5937, -4.2441, -4.4226, -4.2619,\n",
      "         -4.6287, -4.2929, -4.6101, -4.2664, -4.2844, -4.3074, -4.2667, -4.2434,\n",
      "         -4.4043, -4.2257, -4.5105, -4.6431, -4.2637, -4.2136, -4.4238, -4.7027,\n",
      "         -4.2202, -4.6286, -4.5419, -4.6133, -4.2785, -4.4231, -4.3909, -4.2915,\n",
      "         -4.2677, -4.1971, -4.2274, -4.3129, -4.2541, -4.4223, -4.5112, -4.5259,\n",
      "         -4.1708, -4.2453, -4.3079, -4.6337, -4.3639, -4.2410, -4.2170, -4.3089,\n",
      "         -4.6671, -4.6733, -4.4063, -4.6234, -4.2470, -4.2703, -4.3035, -4.1742,\n",
      "         -4.3069, -4.2045, -4.4766, -4.4610, -4.7070, -4.4159, -4.5045, -4.6574]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 145\n",
      "OUTPUTS tensor([[-4.2449, -4.2465, -4.2948, -4.4599, -4.2709, -4.6819, -4.5893, -4.6666,\n",
      "         -4.2457, -4.2167, -4.7150, -4.4570, -4.2879, -4.2062, -4.1892, -4.4209,\n",
      "         -4.3275, -4.6083, -4.3764, -4.6146, -4.5958, -4.2441, -4.4207, -4.2615,\n",
      "         -4.6276, -4.2919, -4.6125, -4.2664, -4.2843, -4.3054, -4.2665, -4.2434,\n",
      "         -4.4041, -4.2255, -4.5104, -4.6456, -4.2637, -4.2135, -4.4236, -4.7053,\n",
      "         -4.2200, -4.6313, -4.5414, -4.6157, -4.2775, -4.4230, -4.3904, -4.2898,\n",
      "         -4.2676, -4.1967, -4.2257, -4.3129, -4.2537, -4.4221, -4.5097, -4.5257,\n",
      "         -4.1701, -4.2450, -4.3076, -4.6330, -4.3637, -4.2384, -4.2167, -4.3089,\n",
      "         -4.6701, -4.6762, -4.4061, -4.6259, -4.2468, -4.2701, -4.3032, -4.1738,\n",
      "         -4.3063, -4.2042, -4.4765, -4.4608, -4.7100, -4.4157, -4.5065, -4.6603]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 146\n",
      "OUTPUTS tensor([[-4.2447, -4.2462, -4.2940, -4.4598, -4.2708, -4.6847, -4.5882, -4.6695,\n",
      "         -4.2457, -4.2164, -4.7179, -4.4557, -4.2879, -4.2056, -4.1882, -4.4208,\n",
      "         -4.3254, -4.6104, -4.3761, -4.6175, -4.5979, -4.2441, -4.4190, -4.2611,\n",
      "         -4.6267, -4.2911, -4.6150, -4.2664, -4.2842, -4.3037, -4.2664, -4.2408,\n",
      "         -4.4039, -4.2253, -4.5103, -4.6481, -4.2637, -4.2134, -4.4233, -4.7078,\n",
      "         -4.2198, -4.6339, -4.5410, -4.6181, -4.2766, -4.4229, -4.3900, -4.2883,\n",
      "         -4.2674, -4.1964, -4.2241, -4.3129, -4.2534, -4.4219, -4.5083, -4.5255,\n",
      "         -4.1694, -4.2448, -4.3074, -4.6324, -4.3634, -4.2360, -4.2164, -4.3089,\n",
      "         -4.6730, -4.6790, -4.4059, -4.6285, -4.2467, -4.2699, -4.3029, -4.1735,\n",
      "         -4.3057, -4.2039, -4.4764, -4.4607, -4.7130, -4.4155, -4.5085, -4.6632]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 147\n",
      "OUTPUTS tensor([[-4.2445, -4.2460, -4.2932, -4.4597, -4.2706, -4.6874, -4.5873, -4.6724,\n",
      "         -4.2457, -4.2161, -4.7209, -4.4545, -4.2878, -4.2051, -4.1855, -4.4208,\n",
      "         -4.3234, -4.6124, -4.3758, -4.6203, -4.6001, -4.2441, -4.4175, -4.2607,\n",
      "         -4.6258, -4.2903, -4.6175, -4.2664, -4.2840, -4.3021, -4.2663, -4.2383,\n",
      "         -4.4036, -4.2251, -4.5102, -4.6505, -4.2637, -4.2132, -4.4231, -4.7104,\n",
      "         -4.2196, -4.6365, -4.5406, -4.6205, -4.2757, -4.4228, -4.3896, -4.2869,\n",
      "         -4.2673, -4.1961, -4.2227, -4.3129, -4.2531, -4.4217, -4.5070, -4.5254,\n",
      "         -4.1688, -4.2446, -4.3073, -4.6318, -4.3632, -4.2339, -4.2162, -4.3089,\n",
      "         -4.6760, -4.6819, -4.4057, -4.6310, -4.2466, -4.2698, -4.3026, -4.1732,\n",
      "         -4.3052, -4.2036, -4.4763, -4.4607, -4.7160, -4.4153, -4.5105, -4.6661]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 148\n",
      "OUTPUTS tensor([[-4.2443, -4.2458, -4.2925, -4.4596, -4.2705, -4.6901, -4.5864, -4.6753,\n",
      "         -4.2458, -4.2158, -4.7238, -4.4534, -4.2878, -4.2045, -4.1830, -4.4207,\n",
      "         -4.3217, -4.6145, -4.3756, -4.6232, -4.6022, -4.2441, -4.4161, -4.2603,\n",
      "         -4.6250, -4.2895, -4.6200, -4.2664, -4.2839, -4.3006, -4.2661, -4.2361,\n",
      "         -4.4034, -4.2249, -4.5101, -4.6530, -4.2638, -4.2131, -4.4229, -4.7130,\n",
      "         -4.2194, -4.6391, -4.5402, -4.6230, -4.2749, -4.4227, -4.3893, -4.2857,\n",
      "         -4.2672, -4.1958, -4.2214, -4.3129, -4.2528, -4.4215, -4.5058, -4.5252,\n",
      "         -4.1668, -4.2444, -4.3071, -4.6313, -4.3630, -4.2319, -4.2159, -4.3089,\n",
      "         -4.6789, -4.6847, -4.4055, -4.6336, -4.2465, -4.2696, -4.3024, -4.1729,\n",
      "         -4.3047, -4.2033, -4.4763, -4.4606, -4.7190, -4.4151, -4.5125, -4.6689]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS tensor([[-4.2440, -4.2455, -4.2918, -4.4595, -4.2703, -4.6928, -4.5856, -4.6782,\n",
      "         -4.2458, -4.2155, -4.7267, -4.4524, -4.2878, -4.2040, -4.1808, -4.4207,\n",
      "         -4.3201, -4.6166, -4.3754, -4.6261, -4.6043, -4.2442, -4.4148, -4.2600,\n",
      "         -4.6242, -4.2888, -4.6224, -4.2664, -4.2837, -4.2992, -4.2660, -4.2341,\n",
      "         -4.4032, -4.2246, -4.5099, -4.6555, -4.2638, -4.2130, -4.4227, -4.7155,\n",
      "         -4.2192, -4.6417, -4.5398, -4.6254, -4.2741, -4.4227, -4.3889, -4.2845,\n",
      "         -4.2670, -4.1954, -4.2202, -4.3129, -4.2525, -4.4213, -4.5047, -4.5250,\n",
      "         -4.1636, -4.2441, -4.3069, -4.6307, -4.3627, -4.2301, -4.2157, -4.3089,\n",
      "         -4.6818, -4.6875, -4.4053, -4.6361, -4.2464, -4.2694, -4.3021, -4.1726,\n",
      "         -4.3042, -4.2031, -4.4762, -4.4605, -4.7219, -4.4149, -4.5145, -4.6718]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "[1,   150] loss: 0.021\n",
      "BATCH 150\n",
      "OUTPUTS tensor([[-4.2438, -4.2453, -4.2911, -4.4594, -4.2701, -4.6955, -4.5849, -4.6810,\n",
      "         -4.2458, -4.2152, -4.7296, -4.4514, -4.2878, -4.2022, -4.1787, -4.4206,\n",
      "         -4.3186, -4.6187, -4.3752, -4.6289, -4.6064, -4.2442, -4.4136, -4.2596,\n",
      "         -4.6235, -4.2881, -4.6249, -4.2664, -4.2836, -4.2980, -4.2659, -4.2323,\n",
      "         -4.4030, -4.2244, -4.5098, -4.6579, -4.2638, -4.2129, -4.4225, -4.7181,\n",
      "         -4.2190, -4.6442, -4.5395, -4.6277, -4.2734, -4.4226, -4.3885, -4.2835,\n",
      "         -4.2669, -4.1951, -4.2190, -4.3129, -4.2522, -4.4211, -4.5037, -4.5248,\n",
      "         -4.1607, -4.2439, -4.3067, -4.6303, -4.3625, -4.2284, -4.2154, -4.3089,\n",
      "         -4.6847, -4.6903, -4.4052, -4.6386, -4.2462, -4.2692, -4.3019, -4.1723,\n",
      "         -4.3038, -4.2028, -4.4761, -4.4604, -4.7248, -4.4147, -4.5165, -4.6746]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 151\n",
      "OUTPUTS tensor([[-4.2436, -4.2450, -4.2905, -4.4593, -4.2700, -4.6982, -4.5842, -4.6839,\n",
      "         -4.2458, -4.2149, -4.7324, -4.4505, -4.2877, -4.2004, -4.1754, -4.4205,\n",
      "         -4.3172, -4.6207, -4.3750, -4.6317, -4.6085, -4.2442, -4.4125, -4.2593,\n",
      "         -4.6229, -4.2875, -4.6273, -4.2664, -4.2834, -4.2968, -4.2657, -4.2306,\n",
      "         -4.4028, -4.2242, -4.5097, -4.6603, -4.2638, -4.2127, -4.4223, -4.7206,\n",
      "         -4.2188, -4.6468, -4.5392, -4.6301, -4.2728, -4.4225, -4.3882, -4.2825,\n",
      "         -4.2668, -4.1948, -4.2180, -4.3129, -4.2519, -4.4208, -4.5028, -4.5247,\n",
      "         -4.1581, -4.2437, -4.3065, -4.6298, -4.3622, -4.2269, -4.2152, -4.3089,\n",
      "         -4.6876, -4.6930, -4.4050, -4.6411, -4.2461, -4.2690, -4.3016, -4.1720,\n",
      "         -4.3034, -4.2026, -4.4760, -4.4603, -4.7277, -4.4145, -4.5184, -4.6774]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 152\n",
      "OUTPUTS tensor([[-4.2433, -4.2448, -4.2900, -4.4592, -4.2698, -4.7009, -4.5836, -4.6867,\n",
      "         -4.2459, -4.2146, -4.7353, -4.4497, -4.2877, -4.1989, -4.1723, -4.4205,\n",
      "         -4.3160, -4.6228, -4.3748, -4.6345, -4.6106, -4.2442, -4.4114, -4.2590,\n",
      "         -4.6223, -4.2870, -4.6298, -4.2664, -4.2833, -4.2957, -4.2656, -4.2291,\n",
      "         -4.4026, -4.2240, -4.5096, -4.6628, -4.2638, -4.2126, -4.4221, -4.7231,\n",
      "         -4.2186, -4.6493, -4.5389, -4.6325, -4.2722, -4.4224, -4.3879, -4.2816,\n",
      "         -4.2666, -4.1945, -4.2171, -4.3129, -4.2517, -4.4206, -4.5019, -4.5245,\n",
      "         -4.1557, -4.2416, -4.3063, -4.6294, -4.3620, -4.2255, -4.2149, -4.3089,\n",
      "         -4.6904, -4.6958, -4.4048, -4.6436, -4.2460, -4.2688, -4.3014, -4.1717,\n",
      "         -4.3030, -4.2023, -4.4759, -4.4602, -4.7306, -4.4143, -4.5204, -4.6802]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 153\n",
      "OUTPUTS tensor([[-4.2431, -4.2446, -4.2894, -4.4591, -4.2697, -4.7036, -4.5830, -4.6895,\n",
      "         -4.2459, -4.2143, -4.7381, -4.4490, -4.2877, -4.1974, -4.1695, -4.4204,\n",
      "         -4.3148, -4.6249, -4.3746, -4.6373, -4.6127, -4.2442, -4.4105, -4.2588,\n",
      "         -4.6217, -4.2864, -4.6322, -4.2665, -4.2832, -4.2948, -4.2654, -4.2277,\n",
      "         -4.4024, -4.2238, -4.5095, -4.6652, -4.2639, -4.2125, -4.4219, -4.7257,\n",
      "         -4.2185, -4.6519, -4.5386, -4.6349, -4.2716, -4.4223, -4.3875, -4.2808,\n",
      "         -4.2665, -4.1942, -4.2162, -4.3129, -4.2515, -4.4204, -4.5011, -4.5244,\n",
      "         -4.1535, -4.2396, -4.3061, -4.6290, -4.3618, -4.2242, -4.2147, -4.3089,\n",
      "         -4.6933, -4.6986, -4.4046, -4.6461, -4.2459, -4.2686, -4.2990, -4.1714,\n",
      "         -4.3026, -4.2021, -4.4758, -4.4601, -4.7335, -4.4141, -4.5224, -4.6830]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 154\n",
      "OUTPUTS tensor([[-4.2429, -4.2443, -4.2889, -4.4591, -4.2695, -4.7062, -4.5824, -4.6923,\n",
      "         -4.2459, -4.2140, -4.7410, -4.4483, -4.2877, -4.1961, -4.1670, -4.4204,\n",
      "         -4.3120, -4.6269, -4.3745, -4.6401, -4.6148, -4.2442, -4.4097, -4.2585,\n",
      "         -4.6212, -4.2859, -4.6346, -4.2665, -4.2830, -4.2939, -4.2653, -4.2265,\n",
      "         -4.4022, -4.2236, -4.5094, -4.6676, -4.2639, -4.2124, -4.4216, -4.7282,\n",
      "         -4.2183, -4.6544, -4.5384, -4.6373, -4.2711, -4.4222, -4.3872, -4.2801,\n",
      "         -4.2664, -4.1939, -4.2154, -4.3129, -4.2512, -4.4202, -4.5004, -4.5242,\n",
      "         -4.1515, -4.2379, -4.3060, -4.6287, -4.3615, -4.2230, -4.2145, -4.3089,\n",
      "         -4.6961, -4.7013, -4.4044, -4.6486, -4.2457, -4.2684, -4.2968, -4.1712,\n",
      "         -4.3022, -4.2019, -4.4757, -4.4600, -4.7364, -4.4139, -4.5244, -4.6858]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 155\n",
      "OUTPUTS tensor([[-4.2427, -4.2441, -4.2885, -4.4590, -4.2694, -4.7089, -4.5820, -4.6951,\n",
      "         -4.2432, -4.2138, -4.7438, -4.4477, -4.2877, -4.1949, -4.1647, -4.4203,\n",
      "         -4.3094, -4.6290, -4.3743, -4.6429, -4.6169, -4.2443, -4.4089, -4.2583,\n",
      "         -4.6208, -4.2855, -4.6371, -4.2665, -4.2829, -4.2931, -4.2652, -4.2253,\n",
      "         -4.4020, -4.2234, -4.5093, -4.6701, -4.2639, -4.2123, -4.4215, -4.7307,\n",
      "         -4.2181, -4.6570, -4.5381, -4.6396, -4.2706, -4.4222, -4.3869, -4.2794,\n",
      "         -4.2662, -4.1936, -4.2147, -4.3129, -4.2510, -4.4201, -4.4997, -4.5241,\n",
      "         -4.1496, -4.2363, -4.3058, -4.6283, -4.3613, -4.2219, -4.2142, -4.3089,\n",
      "         -4.6990, -4.7041, -4.4042, -4.6511, -4.2456, -4.2682, -4.2948, -4.1709,\n",
      "         -4.3019, -4.2016, -4.4756, -4.4599, -4.7393, -4.4137, -4.5264, -4.6886]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 156\n",
      "OUTPUTS tensor([[-4.2425, -4.2439, -4.2880, -4.4589, -4.2692, -4.7116, -4.5815, -4.6979,\n",
      "         -4.2407, -4.2135, -4.7466, -4.4471, -4.2877, -4.1937, -4.1626, -4.4203,\n",
      "         -4.3071, -4.6311, -4.3742, -4.6457, -4.6191, -4.2443, -4.4082, -4.2581,\n",
      "         -4.6204, -4.2850, -4.6395, -4.2665, -4.2828, -4.2923, -4.2651, -4.2243,\n",
      "         -4.4018, -4.2233, -4.5092, -4.6725, -4.2640, -4.2122, -4.4213, -4.7332,\n",
      "         -4.2179, -4.6595, -4.5379, -4.6420, -4.2701, -4.4221, -4.3866, -4.2788,\n",
      "         -4.2661, -4.1933, -4.2140, -4.3129, -4.2487, -4.4199, -4.4991, -4.5240,\n",
      "         -4.1480, -4.2348, -4.3057, -4.6280, -4.3611, -4.2209, -4.2140, -4.3089,\n",
      "         -4.7018, -4.7068, -4.4041, -4.6536, -4.2455, -4.2680, -4.2931, -4.1706,\n",
      "         -4.3015, -4.2015, -4.4756, -4.4599, -4.7421, -4.4136, -4.5284, -4.6913]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 157\n",
      "OUTPUTS tensor([[-4.2423, -4.2437, -4.2876, -4.4589, -4.2691, -4.7142, -4.5811, -4.7007,\n",
      "         -4.2385, -4.2133, -4.7495, -4.4466, -4.2877, -4.1927, -4.1607, -4.4203,\n",
      "         -4.3050, -4.6332, -4.3741, -4.6485, -4.6212, -4.2444, -4.4075, -4.2579,\n",
      "         -4.6200, -4.2847, -4.6419, -4.2666, -4.2827, -4.2916, -4.2650, -4.2234,\n",
      "         -4.4017, -4.2231, -4.5091, -4.6749, -4.2640, -4.2121, -4.4211, -4.7357,\n",
      "         -4.2178, -4.6621, -4.5377, -4.6444, -4.2697, -4.4221, -4.3864, -4.2782,\n",
      "         -4.2660, -4.1931, -4.2134, -4.3129, -4.2465, -4.4197, -4.4985, -4.5239,\n",
      "         -4.1464, -4.2335, -4.3056, -4.6278, -4.3609, -4.2200, -4.2138, -4.3062,\n",
      "         -4.7047, -4.7096, -4.4039, -4.6561, -4.2455, -4.2679, -4.2914, -4.1704,\n",
      "         -4.3013, -4.2013, -4.4755, -4.4598, -4.7450, -4.4134, -4.5304, -4.6941]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 158\n",
      "OUTPUTS tensor([[-4.2421, -4.2435, -4.2872, -4.4589, -4.2690, -4.7169, -4.5808, -4.7036,\n",
      "         -4.2365, -4.2130, -4.7523, -4.4461, -4.2877, -4.1918, -4.1590, -4.4175,\n",
      "         -4.3031, -4.6353, -4.3741, -4.6513, -4.6233, -4.2444, -4.4069, -4.2577,\n",
      "         -4.6197, -4.2843, -4.6444, -4.2666, -4.2826, -4.2910, -4.2649, -4.2226,\n",
      "         -4.4015, -4.2229, -4.5091, -4.6774, -4.2641, -4.2120, -4.4210, -4.7383,\n",
      "         -4.2176, -4.6646, -4.5376, -4.6468, -4.2694, -4.4221, -4.3861, -4.2777,\n",
      "         -4.2659, -4.1928, -4.2129, -4.3130, -4.2445, -4.4196, -4.4980, -4.5238,\n",
      "         -4.1451, -4.2323, -4.3054, -4.6275, -4.3607, -4.2192, -4.2137, -4.3037,\n",
      "         -4.7075, -4.7123, -4.4038, -4.6586, -4.2454, -4.2677, -4.2899, -4.1701,\n",
      "         -4.3010, -4.2011, -4.4755, -4.4598, -4.7479, -4.4133, -4.5324, -4.6969]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS tensor([[-4.2419, -4.2433, -4.2869, -4.4588, -4.2689, -4.7196, -4.5805, -4.7063,\n",
      "         -4.2346, -4.2128, -4.7551, -4.4457, -4.2878, -4.1909, -4.1574, -4.4150,\n",
      "         -4.3013, -4.6374, -4.3740, -4.6540, -4.6254, -4.2445, -4.4064, -4.2575,\n",
      "         -4.6193, -4.2840, -4.6468, -4.2667, -4.2825, -4.2904, -4.2648, -4.2218,\n",
      "         -4.4014, -4.2228, -4.5090, -4.6798, -4.2642, -4.2119, -4.4208, -4.7408,\n",
      "         -4.2175, -4.6672, -4.5374, -4.6492, -4.2690, -4.4221, -4.3859, -4.2773,\n",
      "         -4.2659, -4.1926, -4.2124, -4.3130, -4.2428, -4.4194, -4.4975, -4.5237,\n",
      "         -4.1438, -4.2312, -4.3053, -4.6246, -4.3605, -4.2184, -4.2135, -4.3014,\n",
      "         -4.7103, -4.7150, -4.4037, -4.6611, -4.2453, -4.2676, -4.2886, -4.1699,\n",
      "         -4.3007, -4.2009, -4.4754, -4.4597, -4.7507, -4.4131, -4.5344, -4.6997]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "[1,   160] loss: 0.022\n",
      "BATCH 160\n",
      "OUTPUTS tensor([[-4.2417, -4.2431, -4.2865, -4.4588, -4.2688, -4.7222, -4.5802, -4.7091,\n",
      "         -4.2330, -4.2125, -4.7579, -4.4452, -4.2878, -4.1901, -4.1559, -4.4127,\n",
      "         -4.2997, -4.6394, -4.3740, -4.6568, -4.6276, -4.2446, -4.4059, -4.2574,\n",
      "         -4.6190, -4.2836, -4.6493, -4.2668, -4.2802, -4.2899, -4.2647, -4.2211,\n",
      "         -4.4012, -4.2226, -4.5090, -4.6822, -4.2643, -4.2118, -4.4207, -4.7433,\n",
      "         -4.2174, -4.6697, -4.5373, -4.6516, -4.2687, -4.4220, -4.3857, -4.2769,\n",
      "         -4.2658, -4.1923, -4.2119, -4.3131, -4.2411, -4.4193, -4.4971, -4.5237,\n",
      "         -4.1426, -4.2302, -4.3052, -4.6220, -4.3603, -4.2177, -4.2133, -4.2993,\n",
      "         -4.7131, -4.7178, -4.4035, -4.6635, -4.2452, -4.2674, -4.2874, -4.1697,\n",
      "         -4.3005, -4.2008, -4.4754, -4.4597, -4.7536, -4.4130, -4.5364, -4.7024]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 161\n",
      "OUTPUTS tensor([[-4.2416, -4.2430, -4.2862, -4.4588, -4.2687, -4.7249, -4.5799, -4.7119,\n",
      "         -4.2315, -4.2123, -4.7608, -4.4449, -4.2879, -4.1893, -4.1547, -4.4106,\n",
      "         -4.2983, -4.6416, -4.3739, -4.6596, -4.6297, -4.2447, -4.4055, -4.2573,\n",
      "         -4.6188, -4.2834, -4.6517, -4.2669, -4.2780, -4.2894, -4.2647, -4.2205,\n",
      "         -4.4011, -4.2225, -4.5089, -4.6785, -4.2644, -4.2118, -4.4206, -4.7458,\n",
      "         -4.2173, -4.6723, -4.5372, -4.6540, -4.2684, -4.4221, -4.3855, -4.2765,\n",
      "         -4.2657, -4.1921, -4.2115, -4.3131, -4.2397, -4.4192, -4.4967, -4.5236,\n",
      "         -4.1416, -4.2293, -4.3051, -4.6196, -4.3602, -4.2170, -4.2132, -4.2975,\n",
      "         -4.7160, -4.7205, -4.4034, -4.6660, -4.2452, -4.2673, -4.2863, -4.1695,\n",
      "         -4.3003, -4.2007, -4.4754, -4.4597, -4.7564, -4.4129, -4.5385, -4.7052]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 162\n",
      "OUTPUTS tensor([[-4.2414, -4.2428, -4.2860, -4.4588, -4.2686, -4.7276, -4.5797, -4.7148,\n",
      "         -4.2302, -4.2121, -4.7636, -4.4446, -4.2880, -4.1887, -4.1535, -4.4088,\n",
      "         -4.2969, -4.6437, -4.3739, -4.6624, -4.6319, -4.2448, -4.4051, -4.2572,\n",
      "         -4.6186, -4.2831, -4.6542, -4.2670, -4.2761, -4.2890, -4.2646, -4.2200,\n",
      "         -4.4010, -4.2224, -4.5089, -4.6751, -4.2617, -4.2118, -4.4205, -4.7484,\n",
      "         -4.2172, -4.6748, -4.5371, -4.6564, -4.2681, -4.4221, -4.3853, -4.2762,\n",
      "         -4.2657, -4.1919, -4.2111, -4.3132, -4.2384, -4.4191, -4.4963, -4.5236,\n",
      "         -4.1406, -4.2284, -4.3051, -4.6174, -4.3600, -4.2164, -4.2130, -4.2959,\n",
      "         -4.7188, -4.7233, -4.4033, -4.6686, -4.2452, -4.2672, -4.2853, -4.1693,\n",
      "         -4.3001, -4.2006, -4.4754, -4.4597, -4.7593, -4.4128, -4.5405, -4.7080]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 163\n",
      "OUTPUTS tensor([[-4.2413, -4.2427, -4.2857, -4.4588, -4.2685, -4.7302, -4.5795, -4.7175,\n",
      "         -4.2290, -4.2119, -4.7664, -4.4443, -4.2880, -4.1880, -4.1524, -4.4071,\n",
      "         -4.2958, -4.6458, -4.3740, -4.6652, -4.6340, -4.2449, -4.4047, -4.2571,\n",
      "         -4.6184, -4.2829, -4.6566, -4.2671, -4.2744, -4.2886, -4.2646, -4.2195,\n",
      "         -4.4009, -4.2223, -4.5089, -4.6720, -4.2574, -4.2118, -4.4204, -4.7509,\n",
      "         -4.2171, -4.6774, -4.5370, -4.6588, -4.2679, -4.4221, -4.3851, -4.2759,\n",
      "         -4.2656, -4.1917, -4.2108, -4.3133, -4.2372, -4.4190, -4.4960, -4.5236,\n",
      "         -4.1397, -4.2277, -4.3050, -4.6155, -4.3599, -4.2159, -4.2129, -4.2944,\n",
      "         -4.7217, -4.7260, -4.4033, -4.6711, -4.2452, -4.2671, -4.2844, -4.1691,\n",
      "         -4.2999, -4.2004, -4.4754, -4.4598, -4.7622, -4.4127, -4.5426, -4.7108]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 164\n",
      "OUTPUTS tensor([[-4.2412, -4.2425, -4.2854, -4.4588, -4.2685, -4.7329, -4.5793, -4.7203,\n",
      "         -4.2279, -4.2117, -4.7692, -4.4440, -4.2881, -4.1875, -4.1514, -4.4055,\n",
      "         -4.2947, -4.6479, -4.3740, -4.6679, -4.6362, -4.2450, -4.4044, -4.2570,\n",
      "         -4.6182, -4.2827, -4.6591, -4.2672, -4.2728, -4.2882, -4.2645, -4.2191,\n",
      "         -4.4008, -4.2222, -4.5089, -4.6673, -4.2535, -4.2117, -4.4202, -4.7534,\n",
      "         -4.2170, -4.6799, -4.5369, -4.6612, -4.2677, -4.4221, -4.3849, -4.2756,\n",
      "         -4.2656, -4.1915, -4.2105, -4.3134, -4.2361, -4.4189, -4.4956, -4.5235,\n",
      "         -4.1389, -4.2270, -4.3049, -4.6137, -4.3598, -4.2154, -4.2128, -4.2930,\n",
      "         -4.7245, -4.7287, -4.4032, -4.6736, -4.2451, -4.2670, -4.2835, -4.1689,\n",
      "         -4.2997, -4.2003, -4.4754, -4.4598, -4.7650, -4.4126, -4.5446, -4.7135]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 165\n",
      "OUTPUTS tensor([[-4.2410, -4.2424, -4.2852, -4.4588, -4.2684, -4.7355, -4.5791, -4.7231,\n",
      "         -4.2269, -4.2115, -4.7720, -4.4437, -4.2882, -4.1869, -4.1505, -4.4041,\n",
      "         -4.2936, -4.6500, -4.3740, -4.6707, -4.6383, -4.2451, -4.4041, -4.2569,\n",
      "         -4.6180, -4.2824, -4.6615, -4.2673, -4.2714, -4.2879, -4.2645, -4.2187,\n",
      "         -4.4007, -4.2221, -4.5089, -4.6630, -4.2500, -4.2117, -4.4201, -4.7559,\n",
      "         -4.2169, -4.6825, -4.5369, -4.6636, -4.2674, -4.4222, -4.3828, -4.2754,\n",
      "         -4.2655, -4.1913, -4.2102, -4.3135, -4.2351, -4.4188, -4.4953, -4.5235,\n",
      "         -4.1381, -4.2263, -4.3049, -4.6120, -4.3596, -4.2149, -4.2126, -4.2918,\n",
      "         -4.7273, -4.7314, -4.4031, -4.6760, -4.2451, -4.2669, -4.2827, -4.1687,\n",
      "         -4.2995, -4.2002, -4.4754, -4.4598, -4.7678, -4.4125, -4.5467, -4.7163]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 166\n",
      "OUTPUTS tensor([[-4.2409, -4.2422, -4.2849, -4.4589, -4.2683, -4.7381, -4.5790, -4.7258,\n",
      "         -4.2260, -4.2113, -4.7748, -4.4434, -4.2883, -4.1864, -4.1497, -4.4008,\n",
      "         -4.2927, -4.6521, -4.3740, -4.6734, -4.6405, -4.2452, -4.4037, -4.2568,\n",
      "         -4.6178, -4.2822, -4.6639, -4.2674, -4.2701, -4.2875, -4.2644, -4.2183,\n",
      "         -4.4006, -4.2220, -4.5089, -4.6591, -4.2468, -4.2117, -4.4200, -4.7584,\n",
      "         -4.2168, -4.6850, -4.5368, -4.6659, -4.2672, -4.4222, -4.3809, -4.2751,\n",
      "         -4.2655, -4.1911, -4.2099, -4.3136, -4.2342, -4.4186, -4.4950, -4.5235,\n",
      "         -4.1374, -4.2257, -4.3048, -4.6105, -4.3595, -4.2145, -4.2125, -4.2907,\n",
      "         -4.7300, -4.7341, -4.4030, -4.6785, -4.2451, -4.2668, -4.2820, -4.1685,\n",
      "         -4.2993, -4.2001, -4.4754, -4.4598, -4.7706, -4.4123, -4.5487, -4.7190]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 167\n",
      "OUTPUTS tensor([[-4.2408, -4.2421, -4.2847, -4.4589, -4.2683, -4.7408, -4.5788, -4.7286,\n",
      "         -4.2252, -4.2112, -4.7776, -4.4432, -4.2884, -4.1859, -4.1489, -4.3978,\n",
      "         -4.2919, -4.6542, -4.3740, -4.6762, -4.6426, -4.2454, -4.4035, -4.2568,\n",
      "         -4.6177, -4.2820, -4.6664, -4.2675, -4.2689, -4.2872, -4.2644, -4.2180,\n",
      "         -4.4005, -4.2219, -4.5089, -4.6556, -4.2440, -4.2117, -4.4199, -4.7610,\n",
      "         -4.2167, -4.6875, -4.5368, -4.6683, -4.2670, -4.4222, -4.3791, -4.2750,\n",
      "         -4.2655, -4.1909, -4.2096, -4.3137, -4.2334, -4.4186, -4.4948, -4.5235,\n",
      "         -4.1368, -4.2252, -4.3048, -4.6092, -4.3593, -4.2141, -4.2124, -4.2897,\n",
      "         -4.7328, -4.7369, -4.4029, -4.6810, -4.2451, -4.2667, -4.2814, -4.1683,\n",
      "         -4.2992, -4.2000, -4.4755, -4.4598, -4.7668, -4.4123, -4.5507, -4.7217]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 168\n",
      "OUTPUTS tensor([[-4.2406, -4.2420, -4.2845, -4.4589, -4.2682, -4.7434, -4.5787, -4.7314,\n",
      "         -4.2245, -4.2110, -4.7804, -4.4430, -4.2885, -4.1855, -4.1482, -4.3951,\n",
      "         -4.2911, -4.6564, -4.3740, -4.6789, -4.6448, -4.2455, -4.4033, -4.2567,\n",
      "         -4.6176, -4.2819, -4.6688, -4.2677, -4.2678, -4.2870, -4.2644, -4.2177,\n",
      "         -4.4004, -4.2219, -4.5089, -4.6525, -4.2414, -4.2116, -4.4176, -4.7635,\n",
      "         -4.2166, -4.6901, -4.5367, -4.6707, -4.2669, -4.4223, -4.3775, -4.2748,\n",
      "         -4.2655, -4.1907, -4.2094, -4.3138, -4.2327, -4.4185, -4.4946, -4.5235,\n",
      "         -4.1362, -4.2247, -4.3047, -4.6080, -4.3592, -4.2137, -4.2123, -4.2888,\n",
      "         -4.7356, -4.7396, -4.4028, -4.6835, -4.2451, -4.2666, -4.2808, -4.1682,\n",
      "         -4.2990, -4.1999, -4.4755, -4.4598, -4.7633, -4.4122, -4.5528, -4.7245]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS tensor([[-4.2405, -4.2419, -4.2843, -4.4590, -4.2682, -4.7461, -4.5786, -4.7341,\n",
      "         -4.2239, -4.2108, -4.7831, -4.4428, -4.2886, -4.1851, -4.1476, -4.3927,\n",
      "         -4.2904, -4.6585, -4.3741, -4.6817, -4.6469, -4.2457, -4.4031, -4.2567,\n",
      "         -4.6175, -4.2817, -4.6713, -4.2678, -4.2669, -4.2867, -4.2644, -4.2175,\n",
      "         -4.4004, -4.2218, -4.5089, -4.6496, -4.2391, -4.2117, -4.4154, -4.7660,\n",
      "         -4.2165, -4.6926, -4.5367, -4.6731, -4.2667, -4.4223, -4.3761, -4.2746,\n",
      "         -4.2655, -4.1906, -4.2092, -4.3111, -4.2320, -4.4184, -4.4944, -4.5235,\n",
      "         -4.1356, -4.2243, -4.3047, -4.6069, -4.3591, -4.2133, -4.2122, -4.2880,\n",
      "         -4.7384, -4.7423, -4.4028, -4.6860, -4.2451, -4.2666, -4.2803, -4.1680,\n",
      "         -4.2989, -4.1999, -4.4755, -4.4599, -4.7602, -4.4121, -4.5549, -4.7272]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "[1,   170] loss: 0.022\n",
      "BATCH 170\n",
      "OUTPUTS tensor([[-4.2404, -4.2418, -4.2842, -4.4591, -4.2682, -4.7487, -4.5786, -4.7369,\n",
      "         -4.2233, -4.2107, -4.7859, -4.4427, -4.2887, -4.1847, -4.1471, -4.3905,\n",
      "         -4.2898, -4.6606, -4.3742, -4.6844, -4.6491, -4.2430, -4.4029, -4.2567,\n",
      "         -4.6174, -4.2816, -4.6737, -4.2680, -4.2660, -4.2865, -4.2644, -4.2173,\n",
      "         -4.4003, -4.2217, -4.5090, -4.6470, -4.2370, -4.2117, -4.4135, -4.7685,\n",
      "         -4.2165, -4.6952, -4.5367, -4.6755, -4.2666, -4.4224, -4.3748, -4.2745,\n",
      "         -4.2655, -4.1904, -4.2091, -4.3086, -4.2314, -4.4184, -4.4942, -4.5235,\n",
      "         -4.1352, -4.2239, -4.3047, -4.6059, -4.3590, -4.2130, -4.2121, -4.2873,\n",
      "         -4.7412, -4.7450, -4.4027, -4.6885, -4.2451, -4.2665, -4.2799, -4.1679,\n",
      "         -4.2988, -4.1998, -4.4756, -4.4600, -4.7574, -4.4121, -4.5569, -4.7300]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 171\n",
      "OUTPUTS tensor([[-4.2404, -4.2417, -4.2840, -4.4592, -4.2682, -4.7514, -4.5785, -4.7397,\n",
      "         -4.2229, -4.2106, -4.7887, -4.4425, -4.2889, -4.1844, -4.1466, -4.3885,\n",
      "         -4.2893, -4.6628, -4.3743, -4.6872, -4.6513, -4.2406, -4.4028, -4.2567,\n",
      "         -4.6174, -4.2815, -4.6762, -4.2682, -4.2653, -4.2864, -4.2645, -4.2171,\n",
      "         -4.4003, -4.2217, -4.5091, -4.6447, -4.2351, -4.2117, -4.4118, -4.7711,\n",
      "         -4.2165, -4.6977, -4.5367, -4.6780, -4.2665, -4.4225, -4.3736, -4.2744,\n",
      "         -4.2655, -4.1903, -4.2089, -4.3064, -4.2309, -4.4184, -4.4940, -4.5236,\n",
      "         -4.1347, -4.2235, -4.3047, -4.6050, -4.3590, -4.2128, -4.2121, -4.2867,\n",
      "         -4.7440, -4.7478, -4.4027, -4.6847, -4.2451, -4.2665, -4.2795, -4.1678,\n",
      "         -4.2987, -4.1998, -4.4757, -4.4601, -4.7549, -4.4121, -4.5590, -4.7327]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "BATCH 172\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0753b9c9ca9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OUTPUTS\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1668\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1669\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1671\u001b[0m     )\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        print(\"BATCH\", i)\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        print(\"OUTPUTS\",outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft,\n",
    "                             num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
