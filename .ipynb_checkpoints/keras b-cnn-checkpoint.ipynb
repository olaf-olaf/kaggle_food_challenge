{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 243909337249068849\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_validation(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    train_set, validation_set = train_test_split(data, test_size=0.1)\n",
    "    train_set.reset_index(inplace = True)\n",
    "    validation_set.reset_index(inplace = True)\n",
    "    return train_set, validation_set\n",
    "\n",
    "\n",
    "train_df, validation_df = create_train_validation('food-recognition-challenge/train_labels.csv')\n",
    "train_df['label'] = train_df['label'].astype(str)\n",
    "validation_df['label'] = validation_df['label'].astype(str)\n",
    "# df = pd.read_csv('food-recognition-challenge/train_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olafkroon/anaconda3/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:355: UserWarning: This ImageDataGenerator specifies `samplewise_std_normalization`, which overrides setting of `samplewise_center`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27550 validated image filenames belonging to 80 classes.\n",
      "Found 3062 validated image filenames belonging to 80 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        directory='food-recognition-challenge/train_set/train_set/',\n",
    "        x_col=\"img_name\",\n",
    "        y_col=\"label\",\n",
    "        target_size=(256, 256),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_dataframe(\n",
    "        dataframe=validation_df,\n",
    "        directory='food-recognition-challenge/train_set/train_set/',\n",
    "        x_col=\"img_name\",\n",
    "        y_col=\"label\",\n",
    "        target_size=(256, 256),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def outer_product(x):\n",
    "    #Einstein Notation  [batch,1,1,depth] x [batch,1,1,depth] -> [batch,depth,depth]\n",
    "    phi_I = tf.einsum('ijkm,ijkn->imn',x[0],x[1])\n",
    "    \n",
    "    # Reshape from [batch_size,depth,depth] to [batch_size, depth*depth]\n",
    "    phi_I = tf.reshape(phi_I,[-1,x[0].shape[3]*x[1].shape[3]])\n",
    "    \n",
    "    # Divide by feature map size [sizexsize]\n",
    "    size1 = int(x[1].shape[1])\n",
    "    size2 = int(x[1].shape[2])\n",
    "    phi_I = tf.divide(phi_I, size1*size2)\n",
    "    \n",
    "    # Take signed square root of phi_I\n",
    "    y_ssqrt = tf.multiply(tf.sign(phi_I),tf.sqrt(tf.abs(phi_I)+1e-12))\n",
    "    \n",
    "    # Apply l2 normalization\n",
    "    z_l2 = tf.nn.l2_normalize(y_ssqrt)\n",
    "    return z_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Lambda\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "# load the model , input_shape= (3,256,256)\n",
    "model1 = VGG16(weights='imagenet', include_top=False, input_shape = (256, 256, 3))\n",
    "# Freeze the layers except the last 4 layers\n",
    "for layer in model1.layers:\n",
    "    layer.trainable = False\n",
    " \n",
    "\n",
    "for layer in model1.layers[:-1]:\n",
    "    print(layer, layer.trainable)\n",
    "\n",
    "conv=model1.get_layer('block4_conv3') # block4_conv3\n",
    "d1=Dropout(0.5)(conv.output)\n",
    "d2=Dropout(0.5)(conv.output)\n",
    "\n",
    "x = Lambda(outer_product, name='outer_product')([d1,d2])\n",
    "predictions=Dense(80, activation='softmax', name='predictions')(x)\n",
    "\n",
    "model = Model(inputs=model1.input, outputs=predictions)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
