{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 128, 128]             128\n",
      "              ReLU-3         [-1, 64, 128, 128]               0\n",
      "         MaxPool2d-4           [-1, 64, 64, 64]               0\n",
      "            Conv2d-5           [-1, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 64, 64]             128\n",
      "              ReLU-7           [-1, 64, 64, 64]               0\n",
      "            Conv2d-8           [-1, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 64, 64]             128\n",
      "             ReLU-10           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-11           [-1, 64, 64, 64]               0\n",
      "           Conv2d-12           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 64, 64]             128\n",
      "             ReLU-14           [-1, 64, 64, 64]               0\n",
      "           Conv2d-15           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 64, 64]             128\n",
      "             ReLU-17           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-18           [-1, 64, 64, 64]               0\n",
      "           Conv2d-19           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-20           [-1, 64, 64, 64]             128\n",
      "             ReLU-21           [-1, 64, 64, 64]               0\n",
      "           Conv2d-22           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-23           [-1, 64, 64, 64]             128\n",
      "             ReLU-24           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-25           [-1, 64, 64, 64]               0\n",
      "           Conv2d-26          [-1, 128, 32, 32]          73,728\n",
      "      BatchNorm2d-27          [-1, 128, 32, 32]             256\n",
      "             ReLU-28          [-1, 128, 32, 32]               0\n",
      "           Conv2d-29          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-30          [-1, 128, 32, 32]             256\n",
      "           Conv2d-31          [-1, 128, 32, 32]           8,192\n",
      "      BatchNorm2d-32          [-1, 128, 32, 32]             256\n",
      "             ReLU-33          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-34          [-1, 128, 32, 32]               0\n",
      "           Conv2d-35          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-36          [-1, 128, 32, 32]             256\n",
      "             ReLU-37          [-1, 128, 32, 32]               0\n",
      "           Conv2d-38          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-39          [-1, 128, 32, 32]             256\n",
      "             ReLU-40          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-41          [-1, 128, 32, 32]               0\n",
      "           Conv2d-42          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-43          [-1, 128, 32, 32]             256\n",
      "             ReLU-44          [-1, 128, 32, 32]               0\n",
      "           Conv2d-45          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-46          [-1, 128, 32, 32]             256\n",
      "             ReLU-47          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-48          [-1, 128, 32, 32]               0\n",
      "           Conv2d-49          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-50          [-1, 128, 32, 32]             256\n",
      "             ReLU-51          [-1, 128, 32, 32]               0\n",
      "           Conv2d-52          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 32, 32]             256\n",
      "             ReLU-54          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-55          [-1, 128, 32, 32]               0\n",
      "           Conv2d-56          [-1, 256, 16, 16]         294,912\n",
      "      BatchNorm2d-57          [-1, 256, 16, 16]             512\n",
      "             ReLU-58          [-1, 256, 16, 16]               0\n",
      "           Conv2d-59          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-60          [-1, 256, 16, 16]             512\n",
      "           Conv2d-61          [-1, 256, 16, 16]          32,768\n",
      "      BatchNorm2d-62          [-1, 256, 16, 16]             512\n",
      "             ReLU-63          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-64          [-1, 256, 16, 16]               0\n",
      "           Conv2d-65          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-66          [-1, 256, 16, 16]             512\n",
      "             ReLU-67          [-1, 256, 16, 16]               0\n",
      "           Conv2d-68          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-69          [-1, 256, 16, 16]             512\n",
      "             ReLU-70          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-71          [-1, 256, 16, 16]               0\n",
      "           Conv2d-72          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-73          [-1, 256, 16, 16]             512\n",
      "             ReLU-74          [-1, 256, 16, 16]               0\n",
      "           Conv2d-75          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-76          [-1, 256, 16, 16]             512\n",
      "             ReLU-77          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-78          [-1, 256, 16, 16]               0\n",
      "           Conv2d-79          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-80          [-1, 256, 16, 16]             512\n",
      "             ReLU-81          [-1, 256, 16, 16]               0\n",
      "           Conv2d-82          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 16, 16]             512\n",
      "             ReLU-84          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-85          [-1, 256, 16, 16]               0\n",
      "           Conv2d-86          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-87          [-1, 256, 16, 16]             512\n",
      "             ReLU-88          [-1, 256, 16, 16]               0\n",
      "           Conv2d-89          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-90          [-1, 256, 16, 16]             512\n",
      "             ReLU-91          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-92          [-1, 256, 16, 16]               0\n",
      "           Conv2d-93          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-94          [-1, 256, 16, 16]             512\n",
      "             ReLU-95          [-1, 256, 16, 16]               0\n",
      "           Conv2d-96          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-97          [-1, 256, 16, 16]             512\n",
      "             ReLU-98          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-99          [-1, 256, 16, 16]               0\n",
      "          Conv2d-100            [-1, 512, 8, 8]       1,179,648\n",
      "     BatchNorm2d-101            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-102            [-1, 512, 8, 8]               0\n",
      "          Conv2d-103            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-104            [-1, 512, 8, 8]           1,024\n",
      "          Conv2d-105            [-1, 512, 8, 8]         131,072\n",
      "     BatchNorm2d-106            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-107            [-1, 512, 8, 8]               0\n",
      "      BasicBlock-108            [-1, 512, 8, 8]               0\n",
      "          Conv2d-109            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-110            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-111            [-1, 512, 8, 8]               0\n",
      "          Conv2d-112            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-113            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-114            [-1, 512, 8, 8]               0\n",
      "      BasicBlock-115            [-1, 512, 8, 8]               0\n",
      "          Conv2d-116            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-117            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-118            [-1, 512, 8, 8]               0\n",
      "          Conv2d-119            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-120            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-121            [-1, 512, 8, 8]               0\n",
      "      BasicBlock-122            [-1, 512, 8, 8]               0\n",
      "================================================================\n",
      "Total params: 21,284,672\n",
      "Trainable params: 21,284,672\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 125.75\n",
      "Params size (MB): 81.19\n",
      "Estimated Total Size (MB): 207.69\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load resnet101 freeze all layers, and add one extra output layer\n",
    "model = models.resnet34(pretrained=True)\n",
    "model = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "# for module in model.modules():\n",
    "#     print('layer: ',module._get_name())\n",
    "\n",
    "# unfreeze linear layers\n",
    "# for module in model.modules():\n",
    "#     if module._get_name() != 'Linear':\n",
    "#         print('layer: ',module._get_name())\n",
    "#         for param in module.parameters():\n",
    "#             param.requires_grad_(False)\n",
    "#     elif module._get_name() == 'Linear':\n",
    "#         print('layer: ',module._get_name())\n",
    "#         for param in module.parameters():\n",
    "#             param.requires_grad_(True)\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Added two linear layers with output of 80 classes and softmax activation.\n",
    "# model.fc = nn.Sequential(nn.Linear(2048, 512),\n",
    "#                                  nn.ReLu(),\n",
    "#                                  nn.Dropout(0.2),\n",
    "#                                  nn.Linear(512, NUM_CLASSES),\n",
    "#                                  nn.LogSoftmax(dim=1))\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "# vgg = models.vgg16()\n",
    "summary(model, (3, 256, 256))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCNN(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True):\n",
    "        super(BCNN, self).__init__()\n",
    "        features = torchvision.models.resnet34(pretrained=pretrained)\n",
    "        # Remove the pooling layer and full connection layer\n",
    "        self.conv = nn.Sequential(*list(features.children())[:-2])\n",
    "        self.fc = nn.Linear(512 * 512, num_classes)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        if pretrained:\n",
    "            for parameter in self.conv.parameters():\n",
    "                parameter.requires_grad = False\n",
    "            nn.init.kaiming_normal_(self.fc.weight.data)\n",
    "            nn.init.constant_(self.fc.bias, val=0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        features = self.conv(input)\n",
    "        # Cross product operation\n",
    "        features = features.view(features.size(0), 512, 8 * 8)\n",
    "        features_T = torch.transpose(features, 1, 2)\n",
    "        features = torch.bmm(features, features_T) / (8 * 8)\n",
    "        features = features.view(features.size(0), 512 * 512)\n",
    "        # The signed square root\n",
    "        features = torch.sign(features) * torch.sqrt(torch.abs(features) + 1e-12)\n",
    "        # L2 regularization\n",
    "        features = torch.nn.functional.normalize(features)\n",
    "\n",
    "        out = self.fc(features)\n",
    "        softmax = self.softmax(out)\n",
    "        return out, softmax\n",
    "\n",
    "model = BCNN(80, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, 512, 196]' is invalid for input of size 65536",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-d2dea6861c80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-fb68cbf0a7e5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Cross product operation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mfeatures_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_T\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[2, 512, 196]' is invalid for input of size 65536"
     ]
    }
   ],
   "source": [
    "summary(model, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
